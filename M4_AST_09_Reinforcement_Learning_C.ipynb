{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M4_AST_09_Reinforcement_Learning_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilav/neuralnetwork/blob/main/M4_AST_09_Reinforcement_Learning_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaSXA3KF57Ko"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "\n",
        "##  A program by IISc and TalentSprint\n",
        "\n",
        "### Assignment 9: Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8abyi0q57Kr"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq4Ny4Jm57Ks"
      },
      "source": [
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* understand Reinforcement learning\n",
        "* setup the OpenAI Gym environment\n",
        "* create simple and neural network policies for cart-pole environment\n",
        "* implement policy gradients to solve cart-pole environment\n",
        "* perform Actor-critic algorithm to solve cart-pole environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsjutCK457Kt"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIxXmF-zDM4d"
      },
      "source": [
        "Machine Learning can be classified into:\n",
        "\n",
        "* Supervised learning\n",
        "* Unsupervised learning\n",
        "* Reinforcement learning\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/899/1*9Eu_-DDMZ_bP_t94_MMEYA.png\" width=500px/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPM_q921EO00"
      },
      "source": [
        "### Reinforcement Learning (RL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zDfRUG8EfZQ"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://docs.aws.amazon.com/deepracer/latest/developerguide/images/deepracer-reinforcement-learning-overview.png\" width=500px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "In Reinforcement Learning, a software **agent** makes observations (**states**) and takes **actions** within an **environment**, and in return it receives **rewards**. Its objective is to learn to act in a way that will maximize its expected rewards over time. We can think of positive rewards as pleasure and negative rewards as pain. In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeUxjOROMkUP"
      },
      "source": [
        "### Introduction to OpenAI Gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJk-xVkONpWK"
      },
      "source": [
        "One of the challenges of Reinforcement Learning is that in order to train an agent, we first need to have a working environment. If we want to program an agent that will learn to play an Atari game, we will need an Atari game simulator. \n",
        "\n",
        "**OpenAI Gym** is a toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physical simulations, and so on), so we can train agents, compare them, or develop new RL algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-jP78yVQ9Qh"
      },
      "source": [
        "**Create a CartPole environment**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.oreilly.com/library/view/hands-on-q-learning-with/9781789345803/assets/9170409d-15f1-453b-816a-6f601a89fcf2.png\" width=400px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "It is a 2D simulation in which a pole is attached to a cart placed on a frictionless track. The agent has to apply force to move the cart left or right as shown in the figure above. **It is rewarded for every time step the pole remains upright**. The agent, therefore, must learn to keep the pole from falling over."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M4_AST_09_Reinforcement_Learning_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    ipython.magic(\"sx apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\")\n",
        "    ipython.magic(\"sx pip install -q -U tf-agents pyvirtualdisplay gym[atari,box2d]\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-JfZNo8Stta"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhmlcSPZYaek"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib as mpl\n",
        "mpl.rc('animation', html='jshtml')\n",
        "from pyvirtualdisplay import Display\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPJrfe7bSNPv"
      },
      "source": [
        "We can get the list of all available environments by running the below code cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okdia0OwSrYg"
      },
      "source": [
        "# List all environments\n",
        "gym.envs.registry.all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnXT_bVPTWrD"
      },
      "source": [
        "An environment can be created using `make()` function. After the environment is created, we must initialize it using the `reset()` method. This returns the first observation. Observations depend on the type of environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01b2q65yTzCB"
      },
      "source": [
        "# Create cartpole environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "obs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSF9J4emSrs8"
      },
      "source": [
        "For the CartPole environment, each observation is a 1D NumPy array containing four floats as shown in the figure below.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Cartpole_image.JPG\" width=400px/>\n",
        "</center>\n",
        "\n",
        "These floats represent:\n",
        "\n",
        "* the cart’s horizontal position (0.0 = center), \n",
        "* its velocity (positive means right), \n",
        "* the angle of the pole (0.0 = vertical), and \n",
        "* its angular velocity (positive means clockwise).\n",
        "\n",
        "Now let’s display this environment by calling its `render()` method. If we want `render()` to return the rendered image as a NumPy array, we can set `mode=\"rgb_array\"` and visualize that array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fN1tWnwWYeR"
      },
      "source": [
        "display = Display(visible=0, size=(1400, 900)).start()\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "print(img.shape) # height, width, channels (3 = Red, Green, Blue)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tDFAbmbRKzj"
      },
      "source": [
        "# Create a function to plot environment\n",
        "def plot_environment(env, figsize=(5,4)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    return img\n",
        "\n",
        "plot_environment(env)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFyvf2bUcmHI"
      },
      "source": [
        "Now we are ready to play the game. We use a simple random agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsK_TqVDT4Tb"
      },
      "source": [
        "# Play the game with random agent\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "print(\"Action space: \", env.action_space)\n",
        "frames = []\n",
        "count = 0\n",
        "v = 0\n",
        "while count < 50:\n",
        "    count = count + 1\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    frames.append(img)\n",
        "    # Agent goes here\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action) \n",
        "    if done and v==0:\n",
        "      v += 1\n",
        "      fall_count = count\n",
        "      print(\"Fell after \", fall_count, \" steps\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PYzYYrzol1q"
      },
      "source": [
        "In the above code cell,\n",
        "\n",
        "* **env.render()**: is used to display the environment image\n",
        "\n",
        "* **env.action_space**: gives the possible actions, here Discrete(2) means integers 0 and 1, which represent accelerating left (0) or right (1)\n",
        "\n",
        "* **.sample()**: is used to choose an action either 0 or 1 randomly\n",
        "\n",
        "* **env.step(action)**: executes the given action and returns four values:\n",
        "    \n",
        "    * **observation**: This is the new observation. \n",
        "\n",
        "    * **reward**: In this environment, we get a reward of 1.0 at every step, no matter what we do, so the goal is to keep the episode running as long as possible.\n",
        "    \n",
        "    * **done**: This value will be True when the episode is over. This will happen when the pole tilts too much, or goes off the screen, or after 200 steps (in this last case, we have won). After that, the environment must be reset before it can be used again.\n",
        "\n",
        "    * **info**: This environment-specific dictionary can provide some extra information that we may find useful for debugging or for training. For example, in some games it may indicate how many lives the agent has.\n",
        "\n",
        "Let's visualize the animation of this simple random agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69TwJCPqT6j7"
      },
      "source": [
        "# Create functions to plot animation\n",
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = animation.FuncAnimation(fig, update_scene, fargs=(frames, patch),\n",
        "                                   frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpwGFKt1T9Ls"
      },
      "source": [
        "# Visualize animation\n",
        "print(\"Fell after \", fall_count, \" steps\\n\")\n",
        "plot_animation(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRQv8X15jWmY"
      },
      "source": [
        "From the above animation, we can see that the random agent keeps the pole up for few timesteps then it falls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnq1pgRxrjAo"
      },
      "source": [
        "### Policy Search\n",
        "\n",
        "The algorithm, a software agent uses to determine its actions is called its **policy**. The policy could be a neural network taking observations as inputs and giving an output in terms of the action to take."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vYeu_4Ujb4C"
      },
      "source": [
        "Now we will solve the cart pole environment using different methods including:\n",
        "\n",
        "* **Simple Hard-coded policy:** Take the action of accelerating the cart left when the pole is leaning toward the left and accelerating it right when the pole is leaning toward the right.\n",
        "\n",
        "* **Neural Network Policy:** Use a neural network which takes an observation as input and outputs the probability of action to be executed. Then we select an action randomly according to the estimated probabilities.\n",
        "\n",
        "* **Policy gradients:** In Policy gradient methods, the process of action selection at every step is stochastic. It is based on the probability of\n",
        "selection of a particular action in each state. This can be useful in many applications where determining the accurate value function is complex. In case of cart-pole balancing problem, one such example is the upright state, where the pole is in the upright position but the agent must take either action defined by the objective. In this context, the agent may not prefer a deterministic action as it may limit the exploration across the state space.\n",
        "\n",
        "* **Actor critic Method:** It overcomes the drawback of high variance observed in policy gradient methods. It consists of two networks - action network and critic network. \n",
        "\n",
        " * The action network learns to select actions as a function of the cart-pole system states. It consists of a single neuron having two possible outputs. The probability of generating each action depends on the box in which the system is in. Initial values of weights are zero, making the two actions equally probable. Weights are incrementally updated, and thus, the action probabilities, after receiving non-zero reinforcements which are obtained as feedback upon failure.\n",
        "\n",
        " * The critic network provides an association between the state vectors and the failure signal. It also consists of a single neuron. This network learns the expected value of a discounted sum of future failure signals by means of Temporal difference learning. Through learning, the output of this network will predict how soon a failure can be expected to occur from the current state. This prediction acts like a feedback for the action network, which enables it to learn to select a correct action when it is in a particular state.\n",
        "\n",
        "To know more about RL algorithms applied to the Cart-Pole environment, click [here](http://azadproject.ir/wp-content/uploads/2014/07/2017-Comparison-of-Reinforcement-Learning-Algorithms-applied-to-the-Cart-Pole-Problem.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw67iU36_pCz"
      },
      "source": [
        "**Simple Hard-coded Policy**\n",
        "\n",
        "Let’s hardcode a simple policy that accelerates left when the pole is leaning toward the left and accelerates right when the pole is leaning toward the right. We will run this policy to see the average rewards (time steps for which the pole remains upright) it gets over 500 episodes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku90qWiVhW0g"
      },
      "source": [
        "# Create environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Create a simple policy function\n",
        "def basic_policy(obs):\n",
        "    ''' Returns 0 (move cart left) if angle < 0 (pole leaning left) else 1 '''\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "# Run the simple policy for 500 episodes\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        action = basic_policy(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done:\n",
        "            break\n",
        "    totals.append(episode_rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDfmQNtR_0-S"
      },
      "source": [
        "# Statistics of the total rewards from 500 episodes\n",
        "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IyoqRTlA64z"
      },
      "source": [
        "From the above results, we can see that for simple hardcoded policy the maximum number of timesteps for which the pole remains upright is very less than the total number of tries (500). This environment is considered solved when the agent keeps the poll up for 200 steps.\n",
        "\n",
        "Let's visualize one episode:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT6_1tKSib_o"
      },
      "source": [
        "# Visualize one episode with basic policy\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "frames = []\n",
        "v = 0\n",
        "for step in range(200):\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    frames.append(img)\n",
        "    # action based on simple policy\n",
        "    action = basic_policy(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    if done and v==0:\n",
        "        fell_step = step\n",
        "        v += 1\n",
        "\n",
        "print(\"\\nFell after \", fell_step, \" steps\\n\")        \n",
        "# Visualize animation\n",
        "plot_animation(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "326vGSEqibbL"
      },
      "source": [
        "If we look at the simulation above, we will see that the cart oscillates left and right more and more strongly until the pole tilts too much. \n",
        "\n",
        "Let’s see if we can use neural networks to come up with a better policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3oFewc--5op"
      },
      "source": [
        "**Neural Network Policies**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Neural_network_policy.JPG\" width=600px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "As shown in the above figure, the neural network will take an observation as input, and it will output the action to be executed. More precisely, it will estimate a probability for each action, and then we will select an action randomly, according to the estimated probabilities. \n",
        "\n",
        "In the case of the CartPole environment, there are just two possible actions (left or right), so we only need one output neuron. It will output the probability $p$ of action $0$ (left), and the probability of action $1$ (right) will be $1 – p$.\n",
        "\n",
        "We are picking a random action based on the probabilities given by the neural network, rather than just picking the action with the highest score because this approach lets the agent find the right balance between ***exploring*** new actions and ***exploiting*** the actions that are known to work well.\n",
        "\n",
        "Let's build the neural network policy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGcsn_M7HXf9"
      },
      "source": [
        "# Create model\n",
        "n_inputs = 4 # == env.observation_space.shape[0]\n",
        "model = Sequential([\n",
        "                    Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
        "                    Dense(1, activation=\"sigmoid\")\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHXREglsI8-G"
      },
      "source": [
        "The number of inputs is the size of the observation space, and we have five hidden units. Finally, we want to output a single probability, so we have a single output neuron using the sigmoid activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCpaNRLXl7mu"
      },
      "source": [
        "Let's write a small function that will run the model to play one episode, and return the frames so we can display an animation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeCS3jLwI0X1"
      },
      "source": [
        "# Create function to return frames for animation\n",
        "def render_policy_net(model, n_max_steps=200, seed=42):\n",
        "    frames = []\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    obs = env.reset()\n",
        "    for step in range(n_max_steps):\n",
        "        img = env.render(mode=\"rgb_array\")\n",
        "        frames.append(img)\n",
        "        left_proba = model.predict(obs.reshape(1, -1))\n",
        "        action = int(np.random.rand() > left_proba)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            print(\"Fell after \", step, \" steps\")\n",
        "            break\n",
        "        if step==n_max_steps-1:\n",
        "            print(\"Pole didn't fell till \", n_max_steps, \"steps!\")\n",
        "    env.close()\n",
        "    return frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeqzRzxpm3QM"
      },
      "source": [
        "Now let's look at how well this randomly initialized policy network performs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1qYB15ZlPIz"
      },
      "source": [
        "# Visualize animation\n",
        "frames = render_policy_net(model)\n",
        "plot_animation(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqb-N168m7aa"
      },
      "source": [
        "We can see it performs pretty bad. The neural network will have to learn to do better. First, let's see if it is capable of learning the basic policy we used earlier: go left if the pole is tilting left, and go right if it is tilting right.\n",
        "\n",
        "We can make the same network play in 50 different environments in parallel (this will give us a diverse training batch at each step), and train for 5000 iterations. We also reset environments when they are done. We train the model using a custom training loop so we can easily use the predictions at each training step to advance the environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnG07GbXlQdk"
      },
      "source": [
        "# Play in 50 different environments\n",
        "n_environments = 50\n",
        "n_iterations = 5000\n",
        "\n",
        "envs = [gym.make(\"CartPole-v1\") for _ in range(n_environments)]\n",
        "for index, env in enumerate(envs):\n",
        "    env.seed(index)\n",
        "np.random.seed(42)\n",
        "observations = [env.reset() for env in envs]\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "loss_fn = keras.losses.binary_crossentropy\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    # if angle < 0, we want proba(left) = 1., or else proba(left) = 0.\n",
        "    target_probas = np.array([([1.] if obs[2] < 0 else [0.])\n",
        "                              for obs in observations])\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_probas = model(np.array(observations))\n",
        "        loss = tf.reduce_mean(loss_fn(target_probas, left_probas))\n",
        "    print(\"\\rIteration: {}, Loss: {:.3f}\".format(iteration, loss.numpy()), end=\"\")\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    actions = (np.random.rand(n_environments, 1) > left_probas.numpy()).astype(np.int32)\n",
        "    for env_index, env in enumerate(envs):\n",
        "        obs, reward, done, info = env.step(actions[env_index][0])\n",
        "        observations[env_index] = obs if not done else env.reset()\n",
        "\n",
        "for env in envs:\n",
        "    env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CUwgba0nTmK"
      },
      "source": [
        "# Visualize animation\n",
        "frames = render_policy_net(model)\n",
        "plot_animation(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLDP2-HHnWzC"
      },
      "source": [
        "From above it seems like it learned the policy correctly. Now let's see if it can learn a better policy on its own. One that does not wobble as much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFlDb-XFnrfW"
      },
      "source": [
        "**Policy Gradients with Neural Network**\n",
        "\n",
        "To train the neural network we will need to define the target probabilities. If an action is good we should increase its probability, and conversely if it is bad we should reduce it. To know whether an action is good or bad, the problem is most actions have delayed effects, so when we win or lose points in an episode, it is not clear which actions contributed to this result. This is called the ***credit assignment problem***.\n",
        "\n",
        "The _Policy Gradients_ algorithm tackles this problem by first playing multiple episodes, then making the actions in good episodes slightly more likely, while actions in bad episodes are made slightly less likely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk9g52ldjb4M"
      },
      "source": [
        "In other words, PG algorithms optimize the parameters of a policy by following the gradients toward higher rewards. One popular class of PG algorithms, called REINFORCE algorithms, was introduced in 1992 by Ronald Williams. Here is one common variant:\n",
        "\n",
        "1. First, let the neural network policy play the game several times, and at each step, compute the gradients that would make the chosen action even more likely-but don’t apply these gradients yet.\n",
        "\n",
        "2. Once we have run several episodes, compute each action’s advantage.\n",
        "\n",
        "3. If an action’s advantage is positive, it means that the action was probably good, and we want to apply the gradients computed earlier to make the action even more likely to be chosen in the future. However, if the action’s advantage is negative, it means the action was probably bad, and we want to apply the opposite gradients to make this action slightly less likely in the future. The solution is simply to multiply each gradient vector by the corresponding action’s advantage.\n",
        "\n",
        "4. Finally, compute the mean of all the resulting gradient vectors, and use it to perform a Gradient Descent step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LyhOnIRjb4M"
      },
      "source": [
        "Let’s implement this algorithm. We will train the neural network policy we built earlier so that it learns to balance the pole on the cart. \n",
        "\n",
        "First, we need a function that will play one step. We will pretend for now that whatever action it takes is the right one so that we can compute the loss and its gradients (these gradients will just be saved for a while, and we will modify them later depending on how good or bad the action turned out to be):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms4TUwsenvIZ"
      },
      "source": [
        "# Function explanation is given below\n",
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_proba = model(obs[np.newaxis])\n",
        "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
        "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
        "    return obs, reward, done, grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R8PBCe3jb4M"
      },
      "source": [
        "Let’s walk through this function:\n",
        "\n",
        "* Within the GradientTape block, we start by calling the model,\n",
        "giving it a single observation (we reshape the observation so it becomes a batch\n",
        "containing a single instance, as the model expects a batch). This outputs the\n",
        "probability of going left.\n",
        "\n",
        "* Next, we sample a random float between 0 and 1, and we check whether it is\n",
        "greater than left_proba. The action will be False with probability left_proba,\n",
        "or True with probability 1 - left_proba. Once we cast this Boolean to a number, the action will be 0 (left) or 1 (right) with the appropriate probabilities.\n",
        "\n",
        "* Next, we define the target probability of going left: it is 1 minus the action (cast\n",
        "to a float). If the action is 0 (left), then the target probability of going left will be 1. If the action is 1 (right), then the target probability will be 0.\n",
        "\n",
        "* Then we compute the loss using the given loss function, and we use the tape to\n",
        "compute the gradient of the loss with regard to the model’s trainable variables.\n",
        "Again, these gradients will be tweaked later, before we apply them, depending on\n",
        "how good or bad the action turned out to be.\n",
        "\n",
        "* Finally, we play the selected action, and we return the new observation, the\n",
        "reward, whether the episode is ended or not, and the gradients that we\n",
        "just computed.\n",
        "\n",
        "Now let’s create another function that will rely on the `play_one_step()` function to play multiple episodes, returning all the rewards and gradients for each episode and each step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2MHQzLojb4M"
      },
      "source": [
        "# Create play multiple episodes function\n",
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "    all_rewards = []\n",
        "    all_grads = []\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = []\n",
        "        current_grads = []\n",
        "        obs = env.reset()\n",
        "        for step in range(n_max_steps):\n",
        "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads)\n",
        "            if done:\n",
        "                break\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads)\n",
        "    return all_rewards, all_grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI-FtBZhjb4N"
      },
      "source": [
        "This code returns a list of reward lists (one reward list per episode, containing one reward per step), as well as a list of gradient lists (one gradient list per episode, each containing one tuple of gradients per step and each tuple containing one gradient tensor per trainable variable).\n",
        "\n",
        "The algorithm will use the `play_multiple_episodes()` function to play the game\n",
        "several times (e.g., 10 times), then it will go back and look at all the rewards, discount them, and normalize them. To do that, we need a couple more functions: \n",
        "\n",
        "* the first will compute the sum of future discounted rewards at each step, and \n",
        "\n",
        "* the second will normalize all these discounted rewards (returns) across many episodes\n",
        "\n",
        "The **discount factor** essentially determines how much the reinforcement learning agents cares about rewards in the distant future relative to those in the immediate future. If $\\gamma = 0$, the agent will be completely myopic and only learn about actions that produce an immediate reward. If $\\gamma = 1$, the agent will evaluate each of its actions based on the sum total of all of its future rewards.\n",
        "\n",
        "To know more about the discount factor in RL, click [here](https://towardsdatascience.com/penalizing-the-discount-factor-in-reinforcement-learning-d672e3a38ffe)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naTc5izxjb4N"
      },
      "source": [
        "# Create functions to sum and normalize rewards\n",
        "def discount_rewards(rewards, discount_rate):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_rate\n",
        "    return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean) / reward_std\n",
        "            for discounted_rewards in all_discounted_rewards]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uR4GFKTjb4N"
      },
      "source": [
        "Let’s check that this works:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/actions_return_.JPG\" width=450px/>\n",
        "</center>\n",
        "\n",
        "Say there were 3 actions, and after each action there was a reward: first 10, then 0, then -50 as shown in the figure above. If we use a discount factor of 80%, then the 3rd action will get -50 (full credit for the last reward), but the 2nd action will only get -40 (80% credit for the last reward), and the 1st action will get 80% of -40 (-32) plus full credit for the first reward (+10), which leads to a discounted reward of -22:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrCH51u1jb4N"
      },
      "source": [
        "# Compute discounted rewards\n",
        "discount_rewards(rewards = [10, 0, -50], discount_rate = 0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7qQ5UHbjb4O"
      },
      "source": [
        "# Normalize discounted all rewards\n",
        "discount_and_normalize_rewards(all_rewards = [[10, 0, -50], [10, 20]], discount_rate = 0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uetms1fkjb4O"
      },
      "source": [
        "The call to `discount_rewards()` returns exactly what we expect.\n",
        "We can verify that the function `discount_and_normalize_rewards()` does indeed\n",
        "return the normalized action advantages for each action in both episodes. Notice that the first episode was much worse than the second, so its normalized advantages are all negative; all actions from the first episode would be considered bad, and conversely all actions from the second episode would be considered good.\n",
        "\n",
        "Now let’s define the hyperparameters to run the algorithm. We will run 150 training iterations, playing 10 episodes per iteration, and each episode will last at most 200 steps. We will use a discount factor of 0.95:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_O2Irhyjb4O"
      },
      "source": [
        "n_iterations = 150\n",
        "n_episodes_per_update = 10\n",
        "n_max_steps = 200\n",
        "discount_rate = 0.95"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vADQINYIjb4O"
      },
      "source": [
        "We also need an optimizer and the loss function. We will use the binary cross-entropy loss function because we are training a binary classifier (there are two possible actions: left or right):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PbbTyxUjb4P"
      },
      "source": [
        "# Create optimizer and loss function\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.binary_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSTq9sFQjb4P"
      },
      "source": [
        "Let's build and run the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vfISct9jb4P"
      },
      "source": [
        "# Run training loop\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env.seed(42);\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    # Play multiple episodes\n",
        "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
        "    total_rewards = sum(map(sum, all_rewards))                     \n",
        "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(iteration, total_rewards / n_episodes_per_update), end=\"\")\n",
        "    # Normalize discounted rewards \n",
        "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
        "    all_mean_grads = []\n",
        "    # Compute weighted mean of gradients for every trainable variable\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "        mean_grads = tf.reduce_mean([final_reward * all_grads[episode_index][step][var_index]\n",
        "                                     for episode_index, final_rewards in enumerate(all_final_rewards)\n",
        "                                     for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "    # Apply gradients\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsY31uwqjb4Q"
      },
      "source": [
        "Let’s walk through this code:\n",
        "\n",
        "* At each training iteration, this loop calls the `play_multiple_episodes()` function, which plays the game 10 times and returns all the rewards and gradients for every episode and step.\n",
        "\n",
        "* Then we call the `discount_and_normalize_rewards()` to compute each action’s\n",
        "normalized advantage. This provides a measure of how good or bad each action actually was, in hindsight.\n",
        "\n",
        "* Next, we go through each trainable variable, and for each of them we compute\n",
        "the weighted mean of the gradients for that variable over all episodes and all\n",
        "steps, weighted by the `final_reward`.\n",
        "\n",
        "* Finally, we apply these mean gradients using the optimizer: the model’s trainable variables will be tweaked, and hopefully the policy will be a bit better. \n",
        "\n",
        "This code will train the neural network policy, and it will successfully learn to balance the pole on the cart. The mean reward per episode will get very close to 200 which represents success."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdT58imXjb4Q"
      },
      "source": [
        "# Visualize PG algorithm animation\n",
        "frames = render_policy_net(model)\n",
        "plot_animation(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JW9FDKsjb4Q"
      },
      "source": [
        "Let's see some more powerful algorithms, such as Actor-Critic algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcJxQn23jb4Q"
      },
      "source": [
        "### Actor-Critic Method (OPTIONAL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOj_6fdsjb4R"
      },
      "source": [
        "As an agent takes actions and moves through an environment, it learns to map the observed state of the environment to two possible outputs:\n",
        "\n",
        "* Recommended action: A probability value for each action in the action space. The part of the agent responsible for this output is called the **actor**.\n",
        "\n",
        "* Estimated rewards in the future: Sum of all rewards it expects to receive in the future. The part of the agent responsible for this output is the **critic**.\n",
        "\n",
        "Agent and Critic learn to perform their tasks, such that the recommended actions from the actor maximize the rewards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpDr09Lajb4R"
      },
      "source": [
        "# Configure parameters\n",
        "seed = 42\n",
        "# Discount factor for past rewards\n",
        "gamma = 0.99  \n",
        "max_steps_per_episode = 10000\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"CartPole-v1\")  \n",
        "env.seed(seed)\n",
        "# Small number to prevent divide by 0\n",
        "eps = 1e-07"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCqVPB7mjb4R"
      },
      "source": [
        "**Implement Actor-Critic network**\n",
        "\n",
        "This network learns two functions:\n",
        "\n",
        "* **Actor:** This takes as input the state of our environment and returns a probability value for each action in its action space.\n",
        "\n",
        "* **Critic:** This takes as input the state of our environment and returns an estimate of total rewards in the future.\n",
        "\n",
        "In this implementation, they share the initial layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaJXaSypjb4S"
      },
      "source": [
        "# Create action and critic models\n",
        "num_inputs = 4\n",
        "num_actions = 2\n",
        "\n",
        "inputs = Input(shape=(num_inputs,))\n",
        "common = Dense(128, activation=\"relu\")(inputs)\n",
        "action = Dense(num_actions, activation=\"softmax\")(common)\n",
        "critic = Dense(1)(common)\n",
        "\n",
        "model1 = Model(inputs=inputs, outputs=[action, critic])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsDTtwupjb4S"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knHMuWj-jb4S"
      },
      "source": [
        "# Training\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "huber_loss = keras.losses.Huber()\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "# Run until solved\n",
        "while True:  \n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        for timestep in range(1, max_steps_per_episode):\n",
        "            state = tf.convert_to_tensor(state)\n",
        "            state = tf.expand_dims(state, 0)\n",
        "\n",
        "            # Predict action probabilities and estimated future rewards from environment state\n",
        "            action_probs, critic_value = model1(state)\n",
        "            critic_value_history.append(critic_value[0, 0])\n",
        "\n",
        "            # Sample action from action probability distribution\n",
        "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
        "\n",
        "            # Apply the sampled action in our environment\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards_history.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update running reward to check condition for solving\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # Calculate expected value from rewards\n",
        "        # - At each timestep what was the total reward received after that timestep\n",
        "        # - Rewards in the past are discounted by multiplying them with gamma\n",
        "        # - These are the labels for our critic\n",
        "        returns = []\n",
        "        discounted_sum = 0\n",
        "        for r in rewards_history[::-1]:\n",
        "            discounted_sum = r + gamma * discounted_sum\n",
        "            returns.insert(0, discounted_sum)\n",
        "\n",
        "        # Normalize\n",
        "        returns = np.array(returns)\n",
        "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "        returns = returns.tolist()\n",
        "\n",
        "        # Calculating loss values to update our network\n",
        "        history = zip(action_probs_history, critic_value_history, returns)\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        for log_prob, value, ret in history:\n",
        "            # At this point in history, the critic estimated that we would get a\n",
        "            # total reward = `value` in the future. We took an action with log probability\n",
        "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
        "            # The actor must be updated so that it predicts an action that leads to\n",
        "            # high rewards (compared to critic's estimate) with high probability.\n",
        "            diff = ret - value\n",
        "            actor_losses.append(-log_prob * diff)  # actor loss\n",
        "\n",
        "            # The critic must be updated so that it predicts a better estimate of the future rewards\n",
        "            critic_losses.append(\n",
        "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
        "            )\n",
        "\n",
        "        # Backpropagation\n",
        "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
        "        grads = tape.gradient(loss_value, model1.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model1.trainable_variables))\n",
        "\n",
        "        # Clear the loss and reward history\n",
        "        action_probs_history.clear()\n",
        "        critic_value_history.clear()\n",
        "        rewards_history.clear()\n",
        "\n",
        "    # Log details\n",
        "    episode_count += 1\n",
        "    if episode_count % 10 == 0:\n",
        "        template = \"running reward: {:.2f} at episode {}\"\n",
        "        print(template.format(running_reward, episode_count))\n",
        "    \n",
        "    # Condition to consider the task solved\n",
        "    if running_reward > 195:  \n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA-dBp-njb4T"
      },
      "source": [
        "**Visualizations**\n",
        "\n",
        "Let's create a function that will run the model to play one episode, and return the frames so we can display an animation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxDUCJiLjb4T"
      },
      "source": [
        "# Create function to return frames for animation\n",
        "def render_policy_network(model, n_max_steps=200, seed=42):\n",
        "    frames = []\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    obs = env.reset()\n",
        "    for step in range(n_max_steps):\n",
        "        img = env.render(mode=\"rgb_array\")\n",
        "        frames.append(img)\n",
        "        left_proba = model.predict(obs.reshape(1, -1))\n",
        "        action = int(np.random.rand() > left_proba[0][0][0])\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            print(\"Fell after \", step, \" steps\")\n",
        "            break\n",
        "        if step==n_max_steps - 1:\n",
        "            print(\"Pole didn't fell till \", n_max_steps, \" steps!\")\n",
        "    env.close()\n",
        "    return frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtwXrAZrjb4T"
      },
      "source": [
        "# Visualize Actor-critic method animation\n",
        "frames = render_policy_network(model1)\n",
        "plot_animation(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title Which of the following is correct about reinforcement learning? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"The agent makes an action in an environment and is given back a new observation and a reward for that action\", \"The agent makes an observation in an environment and is given back a new action and a reward for that observation\", \"The agent is given a new environment whenever it receives a reward for its action\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oUH8xaH8PbjN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}