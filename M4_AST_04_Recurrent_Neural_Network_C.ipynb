{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M4_AST_04_Recurrent_Neural_Network_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilav/neuralnetwork/blob/main/M4_AST_04_Recurrent_Neural_Network_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xWC2ghs70AQ"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 4:  Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTWWSGya8kw1"
      },
      "source": [
        "## Learning Objectives\n",
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* know sequence modeling\n",
        "* understand cell structure of basic RNN, LSTM, and GRU\n",
        "* implement RNNs in sequence modeling problem\n",
        "* apply LSTMs and GRUs to solve real world problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZvdbM2R-4-G"
      },
      "source": [
        "### Introduction\n",
        "### Sequence Modelling:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5_Lq-IpjM_e"
      },
      "source": [
        "Say, we are going to develop a language model having a vocabulary of 1000 words. The ith training example being-\" A quick brown fox jumps over the lazy dog\".\n",
        "\n",
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/Temporal_Sequence.PNG)\n",
        "\n",
        "\n",
        "Here, each word is a sequence one after another and we, call it temporal sequences (**time steps**).\n",
        "\n",
        "**Notation:**\n",
        "\n",
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/Notation1.PNG)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY9SowdHtyoV"
      },
      "source": [
        "* Since the vocabulary is 1000 words, each sequence of each training example can be one-hot encoded into a 1D vector of size 1000 i.e. number of units in a single training example also known as **feature**. \n",
        "* For one training example with 9 temporal sequences the shape of the input:(1000,9).\n",
        "* For say, 10 such examples (**batch size** =10), we have to create a 3D tensor having the shape of (10,1000,9).\n",
        "\n",
        "**Notation:**\n",
        "\n",
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/Tensor_shape_3D.PNG)\n",
        "\n",
        "\n",
        "We can take a 2D slice of the above 3D tensor for each time step and pass them as **input** having shape of (m,nx) for the corresponding time sequence/step in RNN model.\n",
        "\n",
        "* Note: The 3D tensor can also be arranged like:**[batch, timesteps, feature]**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmSZSODMeIEv"
      },
      "source": [
        "### Recurrent Neural Networks (RNN)\n",
        "RNNs are the state of the art algorithm, perfectly suited for machine learning problems that involve sequential data.They have \"memory\" and can read inputs $x_{t}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time step to the next.\n",
        "### Cell of a basic RNN (One Neuron of RNN)\n",
        "Given below is a self explanatory schematic diagram of a cell used in basic RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlPt1yJL0E-1"
      },
      "source": [
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/Basic_Rnn_Cell.PNG)\n",
        "\n",
        "$\\text{Figure: Basic RNN Cell}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shU2_EPuDRNH"
      },
      "source": [
        "Basic RNN cell takes  $x_{(t)}$ (current input) and $y_{(t-1)}$ (cell output containing information from the past) as input, and outputs $y_{(t)}$ which is given to the next RNN cell and also used to predict the output.\n",
        "$W_{y}, W_{x},b $ are weights and bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cBnj0I0ahzP"
      },
      "source": [
        "**Equations for  Implementing the RNN-cell are described in Figure above**.\n",
        "\n",
        "\n",
        "1. Compute the hidden state with tanh activation: $y_{t} = \\phi(x_{t}W_{x}+y_{(t-1)}W_{y} + b)$.\n",
        "2.Activation function, $\\phi=\\tanh$, is preferred.\n",
        "**Dimensions** \n",
        "\n",
        "While implementing these equations dimensions of the matrices should be taken care of. \n",
        "\n",
        "- Interesting to note, each of these weights matrics (parameters) can be thought of as an internal one-layer neural network with output size as defined in parameter units(neurons). See the diagram of RNN cell above and think about it.\n",
        "\n",
        "- $Y_{(t)}$ is an $ m\\times n_{neurons} $ matrix containing the layer’s outputs at time step t for each\n",
        "instance(training example) in the mini-batch ($m$ is the number of instances in the mini-batch and $n_{neurons}$ is the number of neurons).\n",
        "\n",
        "- $X_{(t)}$ is an $ m\\times n_{inputs} $ matrix containing the inputs for all instances ( $ n_{inputs}$ is the number of input features).\n",
        "- $W_x$ is an $n_{inputs}\\times n_{neurons}$ matrix containing the connection weights for the inputs\n",
        "of the current time step.\n",
        "- $W_{(y)}$  is an $n_{neurons}\\times n_{neurons}$ matrix containing the connection weights for the out‐\n",
        "puts of the previous time step.\n",
        "- b is a vector of size $n_{neurons}$ containing each neuron’s bias term.\n",
        "\n",
        "\n",
        "- See the diagram below with equations and  shape of weights and bias according to the given input shape and output shape for any one-time step. We can see how the shape of weights influences the shape of the output.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cteW9r6R0f59"
      },
      "source": [
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/Shapes_of_different_Weights.PNG)\n",
        "\n",
        "$\\text{Figure: Dimensions of weights, bias, given input and output}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M4_AST_04_Recurrent_Neural_Network_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    \n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy4AfLl4WoK6"
      },
      "source": [
        "#### Importing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KoOhXs2Y2fT"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWG_IXTQ1Yuj"
      },
      "source": [
        "###Implementing Basic RNN Cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO2IJxmBcWl0"
      },
      "source": [
        "# Initializing Parameter\n",
        "tf.random.set_seed(0)\n",
        "n_inputs = 3     # Number of Feature in each time sequence\n",
        "n_neurons = 5    # Number of Neurons\n",
        "m =4             # Batch size\n",
        "\n",
        "Xt = tf.Variable(tf.random.normal(shape=[m,n_inputs],dtype=tf.float32)) # Input Sequence X(t), taken random values\n",
        "Yt_m =tf.Variable(tf.zeros([m, n_neurons], dtype=tf.float32))           # Output from previous cell y(t-1), Initially taken zero       \n",
        "# Weights are initialized randomly\n",
        "Wx = tf.Variable(tf.random.normal(shape=[n_inputs, n_neurons],dtype=tf.float32)) \n",
        "Wy = tf.Variable(tf.random.normal(shape=[n_neurons,n_neurons],dtype=tf.float32)) #\n",
        "b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))\n",
        "# Implementing the Basic Rnn Cell Equation:\n",
        "Yt = tf.tanh(tf.matmul(Xt, Wx) +tf.matmul(Yt_m, Wy)+ b)  # y(t)\n",
        "print(Yt)\n",
        "# This Yt is taken as output and  fed into next cell as well"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUzgctapiM7x"
      },
      "source": [
        "#### Defining a function: **Rnn_Cell**\n",
        "* Using the above calculation we are going to define a function that returns the next activation state (a_next) and prediction of the current cell (yt_pred) taking xt, a_prev, and parameters (a dictionary) as input arguments. While implementing, take care of the dimensions of each matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRJswSLQjOAS"
      },
      "source": [
        "def Rnn_Cell(Xt,n_neurons):\n",
        "  tf.random.set_seed(0)\n",
        "  m1,n1=tf.shape(Xt).numpy()  # Getting the shape of input Xt, gives: batch size and number of input features\n",
        "  m = m1                      # Batch size    \n",
        "  n_inputs = n1               # Number of Feature in each time sequence\n",
        "  n_neurons=n_neurons         # Number of Neurons\n",
        "\n",
        "  Yt_m =tf.Variable(tf.zeros([m, n_neurons], dtype=tf.float32)) # Output from previous cell y(t-1), Initially taken zero       \n",
        "  #Weights are initialized randomly\n",
        "  Wx = tf.Variable(tf.random.normal(shape=[n_inputs, n_neurons],dtype=tf.float32)) \n",
        "  Wy = tf.Variable(tf.random.normal(shape=[n_neurons,n_neurons],dtype=tf.float32)) #\n",
        "  b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))\n",
        "  # Implementing the Basic Rnn Cell Equation:\n",
        "  Yt = tf.tanh(tf.matmul(Xt, Wx) +tf.matmul(Yt_m, Wy)+ b)  # y(t)\n",
        "  # This Yt is taken as output and  fed into next cell as well\n",
        "  return Yt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5BjeuNLlLSf"
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "Xt = tf.Variable([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]],dtype=tf.float32)\n",
        "n_neurons=5\n",
        "# Calling the above function by passing the arguments\n",
        "Rnn_Cell(Xt, n_neurons)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-wKePeYnjx9"
      },
      "source": [
        "#### Implementing forward pass of RNN (A layer of recurrent neurons)\n",
        "- We have just built one RNN cell above, and a recurrent neural network (RNN) is a repetition of such cells exactly equal to the number of time steps as given in the figure below. \n",
        "- Each cell takes two inputs at each time step:\n",
        "    - $y_{(t-1)}$: From the previous cell.\n",
        "    - $x_{(t)}$: The current time-step's input data.\n",
        "- The outputs at each time step:\n",
        "    \n",
        "    - A prediction $y_{(t)}$\n",
        "    - And same above output is fed back to next cell as well\n",
        "- The weights and biases $W_{x}, W_{y}, b $ are re-used in each time step. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L3nKA_307zn"
      },
      "source": [
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/Layer_of_Rnn.PNG)\n",
        "\n",
        "$\\text{Figure : An Rnn Layer: A recurrent neuron (left) unrolled through time (right)}$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDONWxYYBHdd"
      },
      "source": [
        "* Now we can implement forward propagation of a recurrent neural network, but it suffers from the vanishing gradient problems.The RNN works best when each output $y_{(t)}$ can be estimated using \"local\" context i.e. information that is close to the prediction's time step t.\n",
        "* We will see, a more complex LSTM/GRU model, which is better at addressing vanishing gradients. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMcKkZlOanT2"
      },
      "source": [
        "<font color='blue'>**Discussion 1:** From the below image, presume that it is a basic Recurrent Neural Network (RNN) network. Find the values of (a, b)? (make reasonable asumptions if any) </font>\n",
        "\n",
        "<font color='blue'>(A) (a,b)=(16, 22)</font>\n",
        "\n",
        "<font color='blue'>(B) (a,b)=(40, 62)</font>\n",
        "\n",
        "<font color='blue'>(C) (a,b)=(76, 82)</font>\n",
        "\n",
        "<font color='blue'>(D) (a,b)=(0, 0)</font>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Discussion1_RNN.png\" width=500px/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGCPEzdjy-SR"
      },
      "source": [
        "#### Memory Cells\n",
        "Since the output of a recurrent neuron at time step $t$ is a function of all the inputs from previous time steps, you could say it has a form of memory. A part of a neural network that preserves some state across time steps is called a memory cell (or simply a cell).\n",
        "\n",
        "In general, a cell’s state at time step $t$, denoted $h_{(t)}$ (the “h” stands for “hidden”), is a function of some inputs at that time step and its state at the previous time step: $ h_{(t)} = f(h_{(t–1)},x_{(t)})$. Its output at time step $t$, denoted $y_{(t)}$, is also a function of the previous\n",
        "state and the current inputs. In the case of the basic cells we have discussed so far, the output is simply equal to the state, but in more complex cells this is not always the case, as shown below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiRRJiSJ1SIA"
      },
      "source": [
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/Memory_Cell.PNG)\n",
        "\n",
        "$\\text{Figure : A cell’s hidden state and its output may be different}$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8k_sppcI9AB"
      },
      "source": [
        "### Basic RNN Implementation for Time Series Prediction\n",
        "For simplicity and demonstration of steps involved, we are going to use hypothetically generated time series data by the function as defined below.\n",
        "We will build:\n",
        "- Linear Model\n",
        "- Single Layered Simple RNN Model\n",
        "- Deep RNNs Model with Multiple Simple RNN Layer\n",
        "- Model for Forecasting Several Steps Ahead\n",
        "- Model with time Distributed layer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqvHfJcGK9ow"
      },
      "source": [
        "##### Generate the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VWzAqKXK89U"
      },
      "source": [
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX5KiROGKq6h"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFul3hbfLNoI"
      },
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QfPBCepLzkO"
      },
      "source": [
        "##### Defining a function for plotting time series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeU1f7qHLRR1"
      },
      "source": [
        "def plot_series(series, y=None, y_pred=None, x_label=\"$t$\", y_label=\"$x(t)$\"):\n",
        "    plt.plot(series, \".-\")\n",
        "    if y is not None:\n",
        "        plt.plot(n_steps, y, \"bx\", markersize=10)\n",
        "    if y_pred is not None:\n",
        "        plt.plot(n_steps, y_pred, \"ro\")\n",
        "    plt.grid(True)\n",
        "    if x_label:\n",
        "        plt.xlabel(x_label, fontsize=16)\n",
        "    if y_label:\n",
        "        plt.ylabel(y_label, fontsize=16, rotation=0)\n",
        "    plt.hlines(0, 0, 100, linewidth=1)\n",
        "    plt.axis([0, n_steps + 1, -1, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK4hjS8-MB9J"
      },
      "source": [
        "##### Visualizing the time series generated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKqNZKBBL9g2"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(12, 4))\n",
        "for col in range(3):\n",
        "    plt.sca(axes[col])\n",
        "    plot_series(X_valid[col, :, 0], y_valid[col, 0],\n",
        "                y_label=(\"$x(t)$\" if col==0 else None))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEvaoVgOMVx-"
      },
      "source": [
        "##### Computing Some Baselines\n",
        "Naive predictions (just predict the last observed value):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltK3CJ71MdnR"
      },
      "source": [
        "y_pred = X_valid[:, -1]\n",
        "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mcz7q-IUMebd"
      },
      "source": [
        "plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TNpLnuhMr9f"
      },
      "source": [
        "**Linear Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djo4CTQbMp1X"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[50, 1]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf6_z7DqM1Vg"
      },
      "source": [
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4Wf7kLrNRt2"
      },
      "source": [
        "##### Making function for plotting learning curves and plotting the same"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GfeGdM2M3XD"
      },
      "source": [
        "def plot_learning_curves(loss, val_loss):\n",
        "    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.-\", label=\"Training loss\")\n",
        "    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r.-\", label=\"Validation loss\")\n",
        "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
        "    plt.axis([1, 20, 0, 0.05])\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "\n",
        "plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJa1l10LNfrf"
      },
      "source": [
        "y_pred = model.predict(X_valid)\n",
        "plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrDHRgPUNx94"
      },
      "source": [
        "**Single Layered Simple RNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry7WjNgMN2Ad"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.005)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0p7y6OaN9wX"
      },
      "source": [
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taUEQyIfOCCF"
      },
      "source": [
        "plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MctNAVvHOJA7"
      },
      "source": [
        "y_pred = model.predict(X_valid)\n",
        "plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vL9FwPwONhf"
      },
      "source": [
        "**Deep RNN Model with Multiple Simple RNN Layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm4epj_5ONH6"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.SimpleRNN(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbFK8hr3OYt7"
      },
      "source": [
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQdOGEVvOcUf"
      },
      "source": [
        "plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1jRC7CnOs7K"
      },
      "source": [
        "y_pred = model.predict(X_valid)\n",
        "plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l57wCThWjiv"
      },
      "source": [
        "Make the second SimpleRNN layer return only the last output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySc_7FnSWbQ8"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0VkS-PCW-GC"
      },
      "source": [
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aztMvyhZXD7j"
      },
      "source": [
        "plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OcBt5UKXJLb"
      },
      "source": [
        "y_pred = model.predict(X_valid)\n",
        "plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQC_9_5lPeNs"
      },
      "source": [
        "##### Using Same above **Model for Forecasting Several Steps** Ahead\n",
        "\n",
        "To predict the next 10 values,  use the model we already trained, make it predict the next value, then add that value to the inputs (acting as if this predicted value had occurred), and use the model again to predict the following value, and so on, as in the following code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuSao-11Phnw"
      },
      "source": [
        "np.random.seed(43) # not 42, as it would give the first series in the train set\n",
        "\n",
        "series = generate_time_series(1, n_steps + 10)\n",
        "X_new, Y_new = series[:, :n_steps], series[:, n_steps:]\n",
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "Y_pred = X[:, n_steps:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6mklsRrPlEi"
      },
      "source": [
        "Y_pred.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LDtA-0qZgHW"
      },
      "source": [
        "X_new.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl4w39iAZsqQ"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_5Qx_vhPpf7"
      },
      "source": [
        "def plot_multiple_forecasts(X, Y, Y_pred):\n",
        "    n_steps = X.shape[1]\n",
        "    ahead = Y.shape[1]\n",
        "    plot_series(X[0, :, 0])\n",
        "    plt.plot(np.arange(n_steps, n_steps + ahead), Y[0, :, 0], \"ro-\", label=\"Actual\")\n",
        "    plt.plot(np.arange(n_steps, n_steps + ahead), Y_pred[0, :, 0], \"bx-\", label=\"Forecast\", markersize=10)\n",
        "    plt.axis([0, n_steps + ahead, -1, 1])\n",
        "    plt.legend(fontsize=14)\n",
        "\n",
        "plot_multiple_forecasts(X_new, Y_new, Y_pred)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YFVMSZDrwjM"
      },
      "source": [
        "##### Now let's use this model to predict the next 10 values. We first need to regenerate the sequences with 9 more time steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfS0pqpQr3UI"
      },
      "source": [
        "np.random.seed(42)\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
        "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
        "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3UO89dyP89N"
      },
      "source": [
        "##### Now let's predict the next 10 values one by one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6MwcR2pP_HA"
      },
      "source": [
        "X = X_valid\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = model.predict(X)[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "Y_pred = X[:, n_steps:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgZGTIP4QIXj"
      },
      "source": [
        "Y_pred.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-mIjn-bQNU3"
      },
      "source": [
        "np.mean(keras.metrics.mean_squared_error(Y_valid, Y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_X12gaEQnLF"
      },
      "source": [
        "#### Now let's create an RNN that predicts all 10 next values at once\n",
        "The second way for predicting multiple time-step ahead is to train an RNN to predict all 10 next values at once. We can still use a sequence-to-vector model, but it will output 10 values instead of 1. However, we first need to change the targets to be vectors containing the next 10 values:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTLSLKx5P2fE"
      },
      "source": [
        "np.random.seed(42)\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
        "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
        "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF1lbr5pQtP6"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20),\n",
        "    keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history = model.fit(X_train, Y_train, epochs=20,\n",
        "                    validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMYnJd-YQ3Qv"
      },
      "source": [
        "np.random.seed(43)\n",
        "\n",
        "series = generate_time_series(1, 50 + 10)\n",
        "X_new, Y_new = series[:, :50, :], series[:, -10:, :]\n",
        "Y_pred = model.predict(X_new)[..., np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw-tlzyjQ0Uy"
      },
      "source": [
        "plot_multiple_forecasts(X_new, Y_new, Y_pred)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rZIisKORmTS"
      },
      "source": [
        "**Model with time Distributed layer**\n",
        "\n",
        "Now let's create an RNN that predicts the next 10 steps at each time step. That is, instead of just forecasting time steps 50 to 59 based on time steps 0 to 49, it will forecast time steps 1 to 10 at time step 0, then time steps 2 to 11 at time step 1, and so on, and finally it will forecast time steps 50 to 59 at the last time step. \n",
        "\n",
        "We have to turn this sequence-to-vector RNN into a sequence-to-sequence RNN by setting return_sequences=True in all recurrent layers (even the last one), and we must apply the output Dense layer at every time step. Keras offers a TimeDistributed layer for this very purpose: it wraps any layer (e.g., a Dense layer) and applies it at every time step of its input sequence. It does this efficiently, by reshaping the inputs so that each time step is treated as a separate instance (i.e., it reshapes the inputs from [batch size, time steps, input dimensions] to [batch size × time steps, input dimensions]; in this example, the number of input dimensions is 20 because the previous SimpleRNN layer has 20 units), then it runs the Dense layer, and finally it reshapes the outputs back to sequences (i.e., it reshapes the outputs from [batch size × time steps, output dimensions] to [batch size, time steps, output dimensions]; in this example the number of output dimensions is 10, since the Dense layer has 10 units)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFdazttFRxdk"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train = series[:7000, :n_steps]\n",
        "X_valid = series[7000:9000, :n_steps]\n",
        "X_test = series[9000:, :n_steps]\n",
        "Y = np.empty((10000, n_steps, 10))\n",
        "for step_ahead in range(1, 10 + 1):\n",
        "    Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0]\n",
        "Y_train = Y[:7000]\n",
        "Y_valid = Y[7000:9000]\n",
        "Y_test = Y[9000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frmSj1WQQmmo"
      },
      "source": [
        "X_train.shape, Y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1TOqZmZSEQz"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n",
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(lr=0.01), metrics=[last_time_step_mse])\n",
        "history = model.fit(X_train, Y_train, epochs=20,\n",
        "                    validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dkNtghFku1Y"
      },
      "source": [
        "<font color='blue'>**Discussion 2:** Why is the `TimeDistributed()` layer used in the above code cell? </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhmLVk25SBqQ"
      },
      "source": [
        "np.random.seed(43)\n",
        "\n",
        "series = generate_time_series(1, 50 + 10)\n",
        "X_new, Y_new = series[:, :50, :], series[:, 50:, :]\n",
        "Y_pred = model.predict(X_new)[:, -1][..., np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0F6oH4oSM7w"
      },
      "source": [
        "plot_multiple_forecasts(X_new, Y_new, Y_pred)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp4ch_va43O7"
      },
      "source": [
        "### Vanishing/exploding gradient \n",
        "* The vanishing and exploding gradient phenomena are often encountered in the context of RNNs. The reason why they happen is that it is difficult to capture long term dependencies because of multiplicative gradient that can be exponentially decreasing/increasing with respect to the number of layers\n",
        "\n",
        "### Gradient clipping \n",
        "* It is a technique used to cope with the exploding gradient problem sometimes encountered when performing backpropagation. By capping the maximum value for the gradient, this phenomenon is controlled in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jqekLXI5ewk"
      },
      "source": [
        "### GRU/LSTM\n",
        "\n",
        "\n",
        "*  Gated Recurrent Unit (GRU) and Long Short-Term Memory units (LSTM) deal with the vanishing gradient problem encountered by traditional RNNs and able to remember a piece of information and keep it saved for many timesteps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6DRqEIY6A3f"
      },
      "source": [
        "### Long Short-Term Memory units (LSTM Cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpQP6UCK1mzS"
      },
      "source": [
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/LSTM_Cell.PNG)\n",
        "\n",
        "$\\text{Figure:  LSTM-Cell }$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsImcgfA0wed"
      },
      "source": [
        "### Understanding Gates and States\n",
        "\n",
        "**Forget gate** (Controlled by $f_{(t)}$) controls which parts of the long-term state should be erased.\n",
        "\n",
        "**Input gate** (controlled by :  $i_{(t)}$) controls which parts of a  $g_{(t)}$ should be added to the long term cell state  $c_{(t)}$.\n",
        "\n",
        "**Output gate** (controlled by $o_{(t)}$) controls which parts of the long term cell state should be read and output at this time step, both to prediction $y_{(t)}$  and $h_{(t)}$.\n",
        "\n",
        "**Long term Cell state**  $c_{(t)}$ is the \"memory\" that gets passed onto future time steps.\n",
        "\n",
        " **Short term Cell state** $h_{(t)}$\n",
        "\n",
        "* The Short term state gets passed to the LSTM cell's next time step.\n",
        "* It is used to determine the three gates (Controlled by $f_{(t)}$, $i_{(t)}$, $o_{(t)}$) of the next time step.\n",
        "* It is also used for the prediction $y_{(t)}$.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzRyecOaVO_J"
      },
      "source": [
        "**Note:** Similar to the basic RNN example, we  can implement the LSTM cell from scratch for a single time-step. After that it can be called from inside a \"for-loop\" to have a forward pass for multiple time step as required by the problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgIjdcjUvakk"
      },
      "source": [
        "#### [An example with dataset where  LSTM is implemented using Keras.](https://drive.google.com/file/d/1qwxJBMGhyQ4N46SLZ7363MEoUgE-B0Dl/view?usp=sharing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3SCUAGb5kAN"
      },
      "source": [
        "### Gated Recurrent Unit (GRU)\n",
        "The GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. Both state vectors are merged into a single vector $h_{(t)}$. It also  has only two gates, a forget gate and input gate, see the figure below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tsbBPVd2CGs"
      },
      "source": [
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/GRU_Cell.PNG)\n",
        "\n",
        "$\\text{Figure: GRU-Cell}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZcXRv3fsFnb"
      },
      "source": [
        "Both state vectors are merged into a single vector $h_{(t)}$\n",
        "- A single gate controller $z_{(t)}$ controls both the forget gate and the input gate. If the gate controller outputs a 1, the forget gate is open (= 1) and the input gate is closed (1 – 1 = 0). If it outputs a 0, the opposite happens. In other words, whenever a memory must be stored, the location where it will be stored is erased first. This is actually a frequent variant to the LSTM cell in and of itself.\n",
        "-There is no output gate; the full state vector is output at every time step. However, there is a new gate controller $r_{(t)}$ that controls which part of the previous state will be shown to the main layer ($g_{(t)}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n79WkJXkeQPd"
      },
      "source": [
        "#### [An example with dataset where  GRU is implemented using Keras.](https://drive.google.com/file/d/1ZOhYzhPiBh5BiAMseMiAyrrOF_AKi6lC/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GocE6MtE0Qi"
      },
      "source": [
        "<font color='blue'>**Discussion 3:** After seeing the GRU example, discuss what a Dropout vs a RecurrentDropout? </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5kzr3C69UuI"
      },
      "source": [
        "#### References\n",
        "`1.` Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
        "by Aurélien Géron, Second edition, O’ Reilly Media Publication.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3EIRfY8gCfE"
      },
      "source": [
        "### Theory Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWzkogw2gOg7"
      },
      "source": [
        "1. What is Sequence-to-Sequence RNN?\n",
        "\n",
        " In Sequence to Sequence Learning, RNN is trained to map an input sequence to an output sequence which is not necessarily of the same length.\n",
        "Applications are speech recognition, machine translation, image captioning and question answering.\n",
        "The Encoder RNN reads the input sequence and generates the ﬁxed-size context vector which represents a semantic summary of the input sequence.\n",
        "\n",
        " The fixed-size context vector is given as input to the decoder RNN.\n",
        "The fixed-size context can be provided as the initial state of the Decoder RNN, or it can be connected to the hidden units at each time step. These two ways can also be combined.\n",
        "The number of time steps in the Encoder and Decoder need not to be equal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHtQk15ugTM0"
      },
      "source": [
        "2. Compare between CNN and RNN ?\n",
        " \n",
        "\n",
        "  CNN                                           |  RNN\n",
        "------------------------------------------------|--------------------------\n",
        "It is suitable for spatial data such as images. | RNN is suitable for temporal data, also called sequential data.\n",
        "CNN is considered to be more powerful than RNN. | RNN includes less feature compatibility when compared to CNN.\n",
        "This network takes fixed size inputs and generates fixed size outputs.   | RNN can handle arbitrary input/output lengths.\n",
        "CNN is a type of feed-forward artificial neural network with variations of multilayer perceptrons designed to use minimal amounts of preprocessing. | RNN unlike feed forward neural networks - can use their internal memory to process arbitrary sequences of inputs.\n",
        "CNNs are ideal for images and video processing.\t| RNNs are ideal for text and speech analysis.\n",
        "\n",
        "                  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CYFbzvlgaFf"
      },
      "source": [
        "3. Suppose you have a daily univariate time series, and you want to forecast it for upcoming days. Which RNN architecture should you use?\n",
        "\n",
        " The answer is given by using a real life based example. \n",
        " \n",
        " Given the rise of smart electricity meters and the wide adoption of electricity generation technology like solar panels, there is a wealth of electricity usage data available.\n",
        "\n",
        " This data represents a multivariate time series of power-related variables that in turn could be used to model and even forecast future electricity consumption.\n",
        "\n",
        " Unlike other machine learning algorithms, long short-term memory (LSTM) recurrent neural networks are capable of automatically learning features from sequence data, support multiple-variate data, and can output a variable length sequences that can be used for multi-step forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title Select the FALSE statement: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"RNNs are incapable of handling long-term dependencies\", \"RNNs suffer from vanishing and exploding gradients problems\", \"In GRUs, the update gate decides how much information needs to be forgotten and the reset gate selects the piece of information to be carried forward\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}