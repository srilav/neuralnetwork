{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "M4_AST_10_Deep_Q_Learning_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilav/neuralnetwork/blob/main/M4_AST_10_Deep_Q_Learning_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvs43b0vHAOG"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "\n",
        "##  A program by IISc and TalentSprint\n",
        "\n",
        "### Assignment 10: Deep Q Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZbtiZbbHAOQ"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL6Uy59UHAOR"
      },
      "source": [
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* understand Q-learning\n",
        "* differentiate between Q-learning and Deep Q-learning\n",
        "* implement Deep Q-learning to solve Atari Breakout environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXBOfzUYHAOR"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJrTyoGDHAOS"
      },
      "source": [
        "**Q-Learning**\n",
        "\n",
        "The agent will perform the sequence of actions that will eventually generate the maximum total reward. This total reward is also called the **Q-value** and we will formalize our strategy as:\n",
        "\n",
        "$$Q(s, a) = r(s, a) + \\gamma\\ maxQ(s', a)$$\n",
        "\n",
        "The above equation states that the Q-value yielded from being at state $s$ and performing action $a$ is the immediate reward $r(s,a)$ plus the highest Q-value possible from the next state $s’$. Gamma here is the discount factor which controls the contribution of rewards further in the future.\n",
        "\n",
        "$Q(s’,a)$ depends on $Q(s”,a)$ which will then have a coefficient of gamma squared. So, the Q-value depends on Q-values of future states as shown here:\n",
        "\n",
        "$$Q(s, a) \\rightarrow \\gamma\\ Q(s', a) + \\gamma^2\\ Q(s'', a)\\ ...\\ ...\\ ...\\ \\gamma^n\\ Q(s''^{...n}, a)$$\n",
        "\n",
        "Adjusting the value of gamma will diminish or increase the contribution of future rewards.\n",
        "\n",
        "Since this is a recursive equation, we can start with making arbitrary assumptions for all q-values. With experience, it will converge to the optimal policy.\n",
        "\n",
        "To know more about Q-Learning, click [here](https://github.com/rishal-hurbans/Grokking-Artificial-Intelligence-Algorithms/tree/master/ch10-reinforcement_learning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-TzAn8XHAOT"
      },
      "source": [
        "**Approximate Q-Learning and Deep Q-Learning**\n",
        "\n",
        "The main problem with Q-Learning is that it does not scale well to large (or even medium) Markov Decision Processes with many states and actions, and it is hard to keep track of an estimate for every single Q-Value.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/04/Screenshot-2019-04-16-at-5.46.01-PM-670x440.png\" width=650px />\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "The solution is to find a function $Q_θ(s, a)$ that approximates the Q-Value of any state-action pair (s, a) using a manageable number of parameters (given by the parameter vector θ). This is called **Approximate Q-Learning**. \n",
        "\n",
        "$$Q_{target}(s, a) = r + \\gamma\\ maxQ_{\\theta}(s', a')$$\n",
        "\n",
        "For years it was recommended to use linear combinations of handcrafted features extracted from the state to estimate Q-Values, but in 2013, DeepMind showed that deep neural networks can work much better, especially for complex problems, and it does not require any feature engineering. A DNN used to estimate Q-Values is called a Deep Q-Network (DQN), and using a DQN for Approximate Q-Learning is called **Deep Q-Learning**.\n",
        "\n",
        "This method is considered an \"Off-Policy\" method, meaning its Q values are updated assuming that the best action was chosen, even if the best action was not chosen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr2ZG0TQHAOT"
      },
      "source": [
        "### Implementing Deep Q-Learning for Atari Breakout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4fqv6BuHAOU"
      },
      "source": [
        "Let's implement Deep Q-Learning on the Atari Breakout (`BreakoutNoFrameskip-v4`) environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aze2K918HAOU"
      },
      "source": [
        "**Atari Breakout**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.gannett-cdn.com/media/USATODAY/USATODAY/2013/05/14/atari-breakout-16_9.jpg?width=1023&height=578&fit=crop&format=pjpg&auto=webp\" width=400px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "In this environment, a board moves along the bottom of the screen returning a ball that will destroy blocks at the top of the screen. The aim of the game is to remove all blocks and breakout of the level. The agent must learn to control the board by moving left and right, returning the ball and removing all the blocks without the ball passing the board."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M4_AST_10_Deep_Q_Learning_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    ipython.magic(\"sx pip install gym pyvirtualdisplay > /dev/null 2>&1\")\n",
        "    ipython.magic(\"sx apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baUfsWTmHAOV"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA3knP9nHAOW"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibJWLFYuHAOX"
      },
      "source": [
        "# Dependencies\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip -qq install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS   rars\n",
        "!mv ROMS  rars\n",
        "!python -m atari_py.import_roms rars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_IiZIFaHAOX"
      },
      "source": [
        "# List of available environments\n",
        "gym.envs.registry.all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wJkENOhHAOY"
      },
      "source": [
        "**Configure parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YArr9GLVHAOZ"
      },
      "source": [
        "seed = 42\n",
        "# Discount factor for past rewards\n",
        "gamma = 0.99  \n",
        "# Epsilon greedy parameter\n",
        "epsilon = 1.0  \n",
        "# Minimum epsilon greedy parameter\n",
        "epsilon_min = 0.1  \n",
        "# Maximum epsilon greedy parameter\n",
        "epsilon_max = 1.0  \n",
        "# Rate at which to reduce chance of random action being taken\n",
        "epsilon_interval = (epsilon_max - epsilon_min)  \n",
        "# Size of batch taken from replay buffer\n",
        "batch_size = 32  \n",
        "max_steps_per_episode = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ECYMkVVHAOZ"
      },
      "source": [
        "Next, we define functions used to show the video by adding it to the CoLab notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1ddymg9HAOZ"
      },
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\" Utility functions to enable video recording of gym environment and displaying it.\n",
        "To enable video, we just do \"env = wrap_env(env) \"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3pDVsJiHAOa"
      },
      "source": [
        "**Create environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlrslBIxHAOa"
      },
      "source": [
        "# Create Breakout environment\n",
        "env = wrap_env(gym.make(\"BreakoutNoFrameskip-v4\"))\n",
        "env.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laKtW9JAHAOa"
      },
      "source": [
        "**Create Model**\n",
        "\n",
        "Deep Q network learns an approximation of the Q-table, which is a mapping between the states and actions that an agent will take. For every state in Breakout environment, we'll have four actions that can be taken:\n",
        "\n",
        "* **0:** do nothing\n",
        "* **1:** fire ball to start game\n",
        "* **2:** move right\n",
        "* **3:** move left\n",
        "\n",
        "The environment provides the state, and the action is chosen by selecting the larger of the four Q-values predicted in the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-aZkypvHAOa"
      },
      "source": [
        "# Shape of observation or state\n",
        "env.reset().shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgATLM--HAOb"
      },
      "source": [
        "# Write a funtion for model creation\n",
        "num_actions = 4\n",
        "def create_q_model():\n",
        "    # Network defined by the Deepmind paper\n",
        "    inputs = layers.Input(shape=(210, 160, 3))\n",
        "\n",
        "    # Convolutions on the frames on the screen\n",
        "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
        "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
        "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
        "\n",
        "    layer4 = layers.Flatten()(layer3)\n",
        "\n",
        "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
        "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t4n9Me9HAOb"
      },
      "source": [
        "The first model makes the predictions for Q-values which are used to make an action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fos3rgMLHAOb"
      },
      "source": [
        "# Create model\n",
        "model = create_q_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99cNxGbnHAOb"
      },
      "source": [
        "Now build a target model for the prediction of future rewards. The weights of a target model get updated every 10000 steps thus when the loss between the Q-values is calculated the target Q-value is stable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2Er6a13HAOc"
      },
      "source": [
        "# Create target model\n",
        "model_target = create_q_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IO5H_7DHAOc"
      },
      "source": [
        "**Train**\n",
        "\n",
        "The below code cell might take some time to run, suggesting to use GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvZAXnP96Vmt"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
        "\n",
        "# Experience replay buffers\n",
        "action_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0\n",
        "# Number of frames to take random action and observe output\n",
        "epsilon_random_frames = 50000\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 1000000.0\n",
        "# Maximum replay length\n",
        "max_memory_length = 100000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "# How often to update the target network\n",
        "update_target_network = 10000\n",
        "# Using huber loss for stability\n",
        "loss_function = keras.losses.Huber()\n",
        "\n",
        "while True:  # Run until solved\n",
        "    state = np.array(env.reset())\n",
        "    episode_reward = 0\n",
        "\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        frame_count += 1\n",
        "\n",
        "        # Use epsilon-greedy for exploration\n",
        "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
        "            # Take random action\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            # Predict action Q-values\n",
        "            # From environment state\n",
        "            state_tensor = tf.convert_to_tensor(state)\n",
        "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "            action_probs = model(state_tensor, training=False)\n",
        "            # Take best action\n",
        "            action = tf.argmax(action_probs[0]).numpy()\n",
        "\n",
        "        # Decay probability of taking random action\n",
        "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "        epsilon = max(epsilon, epsilon_min)\n",
        "        \n",
        "        env.render()\n",
        "        # Apply the sampled action in our environment\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        state_next = np.array(state_next)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Save actions and states in replay buffer\n",
        "        action_history.append(action)\n",
        "        state_history.append(state)\n",
        "        state_next_history.append(state_next)\n",
        "        done_history.append(done)\n",
        "        rewards_history.append(reward)\n",
        "        state = state_next\n",
        "\n",
        "        # Update every fourth frame and once batch size is over 32\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "\n",
        "            # Get indices of samples for replay buffers\n",
        "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "\n",
        "            # Using list comprehension to sample from replay buffer\n",
        "            state_sample = np.array([state_history[i] for i in indices])\n",
        "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
        "            rewards_sample = [rewards_history[i] for i in indices]\n",
        "            action_sample = [action_history[i] for i in indices]\n",
        "            done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
        "\n",
        "            # Build the updated Q-values for the sampled future states\n",
        "            # Use the target model for stability\n",
        "            future_rewards = model_target.predict(state_next_sample)\n",
        "            # Q value = reward + discount factor * expected future reward\n",
        "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
        "\n",
        "            # If final frame set the last value to -1\n",
        "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
        "\n",
        "            # Create a mask so we only calculate loss on the updated Q-values\n",
        "            masks = tf.one_hot(action_sample, num_actions)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Train the model on the states and updated Q-values\n",
        "                q_values = model(state_sample)\n",
        "\n",
        "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "                # Calculate loss between new Q-value and old Q-value\n",
        "                loss = loss_function(updated_q_values, q_action)\n",
        "\n",
        "            # Backpropagation\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if frame_count % update_target_network == 0:\n",
        "            # update the the target network with new weights\n",
        "            model_target.set_weights(model.get_weights())\n",
        "            # Log details\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
        "            print(template.format(running_reward, episode_count, frame_count))\n",
        "\n",
        "        # Limit the state and reward history\n",
        "        if len(rewards_history) > max_memory_length:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del done_history[:1]\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Update running reward to check condition for solving\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if len(episode_reward_history) > 100:\n",
        "        del episode_reward_history[:1]\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "    episode_count += 1\n",
        "    \n",
        "    # Condition to consider the task solved\n",
        "    # Note that this execution may take more than 30 minutes\n",
        "    if running_reward > 2.4 or episode_count >100:  \n",
        "        print(\"Stopped at episode {}!\".format(episode_count))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K58TLcolHAOe"
      },
      "source": [
        "Here, one of the stopping criteria is if `running_reward > 2.4`, by increasing this value, learning can be improved.\n",
        "\n",
        "**Visualizations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlniR4lFHAOg"
      },
      "source": [
        "# Visualize training\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbb0FmW1HAOg"
      },
      "source": [
        "We can also get a gist of how the learning processes through the figures given below.\n",
        "\n",
        "Before any training: \n",
        "\n",
        "<img src=\"https://i.imgur.com/rRxXF4H.gif\" />\n",
        "\n",
        "In early stages of training: \n",
        "\n",
        "<img src=\"https://i.imgur.com/X8ghdpL.gif\" />\n",
        "\n",
        "In later stages of training: \n",
        "\n",
        "<img src=\"https://i.imgur.com/Z1K6qBQ.gif\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title Select the False statement: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"A Q-table maps between the states and actions that an agent will take\", \"A Q-table is the data structure used to calculate the minimum expected future rewards for the action at each state\", \"In Deep Q-Learning, the regular Q-table is replaced with a neural network\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}