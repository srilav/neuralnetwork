{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M4_AST_02_MLP_Regression_and_MLP_Tuning_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilav/neuralnetwork/blob/main/M4_AST_02_MLP_Regression_and_MLP_Tuning_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HklD4MlOgCTy"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 2: MLP Regression and MLP Tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doENGez0k03C"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbFv4sGHk7VW"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "- understand the concept of MLPs for regression\n",
        "- know the hyperparameters of neural network and their tuning \n",
        "- understand batch normalization using Keras\n",
        "- understand the concept of optimizers\n",
        "- understand the time-based learning rate method through an example\n",
        "- understand the different regularization methods to avoid the overfitting of neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-9AxpS_WMr9"
      },
      "source": [
        "### Introduction to Regression MLPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btCQw9drWTH_"
      },
      "source": [
        "First, MLPs can be used for regression tasks. If we want to predict a single value (e.g., the  price  of  a  house  given  many  of  its  features), then  we  just  need  a  single  output neuron:  its  output  is  the  predicted  value.  For  multivariate  regression  (i.e.,  to  predict multiple  values  at  once),  we  need  one  output  neuron  per  output  dimension.  For example, to locate the center of an object on an image, we need to predict 2D coordinates,  so  we  need  two  output  neurons.  If  we  also  want  to  place  a  bounding  box around the object, then we need two more numbers: the width and the height of the object. So we end up with 4 output neurons.\n",
        "\n",
        "In general, when building an MLP for regression, we do not want to use any activation  function  for  the  output  neurons,  so  they  are  free  to  output  any  range  of  values. However,  if  we  want  to  guarantee  that  the  output  will  always  be  positive,  then  we can use the ReLU activation function or the softplus activation function in the output layer.  Finally,  if  we  want  to  guarantee  that  the  predictions  will  fall  within  a  given range of values, then we can use the logistic function or the hyperbolic tangent and scale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for the hyperbolic tangent.\n",
        "\n",
        "The loss function to use during training is typically the mean squared error, but if we have  a  lot  of  outliers  in  the  training  set,  we  may  prefer  to  use  the  mean  absolute error  instead.  Alternatively,  we  can  use  the  Huber  loss,  which  is  a  combination  of both.\n",
        "\n",
        "**Note:** The Huber loss is quadratic when the error is smaller than a threshold $δ$ (typically 1), but linear when the error is larger than $δ$. This makes it less sensitive to outliers than the mean squared error, and\n",
        "it  is  often  more  precise  and  converges  faster  than  the  mean  absolute error.\n",
        "\n",
        "To know more about Multi Layer Perceptron (MLP), click [here](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L05%20Multilayer%20Perceptrons.pdf).\n",
        "\n",
        "**Implementation Using Keras**\n",
        "\n",
        "Keras is a high-level Deep Learning API that allows us to easily build train, evaluate and execute all sorts of neural networks. To know more about the documentation of Keras, click [here](https://keras.io/).\n",
        "\n",
        "**Implementation of MLP regression Using sklearn**\n",
        "\n",
        "The very popular machine learning library Scikit-Learn is also capable of basic deep learning modeling.\n",
        "\n",
        "Salient points of Multilayer Perceptron (MLP) in Scikit-learn:\n",
        "\n",
        "* There is no activation function in the output layer.\n",
        "* For regression scenarios, the square error is the loss function, and cross-entropy is the loss function for the classification\n",
        "* It can work with single as well as multiple target values regression.\n",
        "* Unlike other popular packages, likes Keras the implementation of MLP in Scikit doesn’t support GPU.\n",
        "\n",
        "To know more about Scikit-Learn MLP regressor, click [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html).\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx6BJLwYZBxw"
      },
      "source": [
        "### Typical MLP Regressor Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH2a3AvwZNlT"
      },
      "source": [
        "\n",
        "Hyperparameter             | Typical Value \n",
        "---------------------------|------------------\n",
        "# input neurons            | One per input feature (e.g., 28 x 28 = 784 for MNIST)\n",
        "# hidden layers            | Depends on the problem. Typically 1 to 5. \n",
        "# neurons per hidden layer | Depends on the problem. Typically 10 to 100.\n",
        "# output neurons           | 1 per prediction dimension\n",
        "Hidden activation          | ReLU\n",
        "Output activation          | None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\n",
        "Loss function              | MSE or MAE/Huber (if outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2200183\" #@param {type:\"string\"}"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"7671846954\" #@param {type:\"string\"}"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3d1866c8-426a-4d37-cf6b-6deaa81d2b9d"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M4_AST_02_MLP_Regression_and_MLP_Tuning_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    \n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2200183&recordId=5166\"></script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAu6v-CnA8zj"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SRN62EfayXM"
      },
      "source": [
        "# install livelossplot package to visualize epoch by epoch loss and accuracy curve\n",
        "!pip -qq install livelossplot"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw83tjrgdqNO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error             \n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler              # scaling functions from sklearn\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split                  # search on hyperparameters\n",
        "from functools import partial                                                             # partial functions\n",
        "import tensorflow as tf                                                                   # importing tensorflow library\n",
        "from tensorflow import keras                                                              # importing keras package\n",
        "from scipy.stats import reciprocal \n",
        "from sklearn.neural_network import MLPRegressor                                           # importing MLP regressor            \n",
        "from tensorflow.keras.optimizers import SGD                                               # stochastic Gradient Descent\n",
        "from tensorflow.keras.utils import to_categorical                                         # converting a class to categorical data type\n",
        "from keras.datasets import mnist                                                          # load mnist dataset\n",
        "import livelossplot                                                                       # visualize loss and accuracy \n",
        "from keras.models import Sequential                                                       # using keras importing Sequential Model\n",
        "from keras.layers import Activation, Dense, Input, Flatten, Dropout, BatchNormalization   # using keras importing layers                                    \n",
        "from keras.callbacks import EarlyStopping                                                 # to stop the training process"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLVYkKdrdqNV"
      },
      "source": [
        "### Building a Regression MLP "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5YpbtWMdqNX"
      },
      "source": [
        "Here, in this implementation, we will be using California housing problem and tackle it using a regression neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE9ZZHrjdqNY"
      },
      "source": [
        "#### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2NZQqhkdqNZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "d0928577-060e-4489-ac3f-c6e144df7abe"
      },
      "source": [
        "# train dataset\n",
        "df_train = pd.read_csv('/content/sample_data/california_housing_train.csv')\n",
        "df_train.head()"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-114.31</td>\n",
              "      <td>34.19</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5612.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>1.4936</td>\n",
              "      <td>66900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-114.47</td>\n",
              "      <td>34.40</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7650.0</td>\n",
              "      <td>1901.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>463.0</td>\n",
              "      <td>1.8200</td>\n",
              "      <td>80100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-114.56</td>\n",
              "      <td>33.69</td>\n",
              "      <td>17.0</td>\n",
              "      <td>720.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.6509</td>\n",
              "      <td>85700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.64</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1501.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>3.1917</td>\n",
              "      <td>73400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.57</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1454.0</td>\n",
              "      <td>326.0</td>\n",
              "      <td>624.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>1.9250</td>\n",
              "      <td>65500.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_income  median_house_value\n",
              "0    -114.31     34.19  ...         1.4936             66900.0\n",
              "1    -114.47     34.40  ...         1.8200             80100.0\n",
              "2    -114.56     33.69  ...         1.6509             85700.0\n",
              "3    -114.57     33.64  ...         3.1917             73400.0\n",
              "4    -114.57     33.57  ...         1.9250             65500.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aToiAvNdqNb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "b629c55d-dcec-47e6-e4b3-86a4aacd4e7f"
      },
      "source": [
        "# test dataset\n",
        "df_test = pd.read_csv('/content/sample_data/california_housing_test.csv')\n",
        "df_test.head()"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.05</td>\n",
              "      <td>37.37</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3885.0</td>\n",
              "      <td>661.0</td>\n",
              "      <td>1537.0</td>\n",
              "      <td>606.0</td>\n",
              "      <td>6.6085</td>\n",
              "      <td>344700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-118.30</td>\n",
              "      <td>34.26</td>\n",
              "      <td>43.0</td>\n",
              "      <td>1510.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>809.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>3.5990</td>\n",
              "      <td>176500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-117.81</td>\n",
              "      <td>33.78</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3589.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>1484.0</td>\n",
              "      <td>495.0</td>\n",
              "      <td>5.7934</td>\n",
              "      <td>270500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-118.36</td>\n",
              "      <td>33.82</td>\n",
              "      <td>28.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.1359</td>\n",
              "      <td>330000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-119.67</td>\n",
              "      <td>36.33</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>2.9375</td>\n",
              "      <td>81700.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_income  median_house_value\n",
              "0    -122.05     37.37  ...         6.6085            344700.0\n",
              "1    -118.30     34.26  ...         3.5990            176500.0\n",
              "2    -117.81     33.78  ...         5.7934            270500.0\n",
              "3    -118.36     33.82  ...         6.1359            330000.0\n",
              "4    -119.67     36.33  ...         2.9375             81700.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyyGRVgVdqNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "566db1cc-73db-4d4f-a2ce-dac70ea23e16"
      },
      "source": [
        "# printing train dataset information\n",
        "df_train.info()"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17000 entries, 0 to 16999\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           17000 non-null  float64\n",
            " 1   latitude            17000 non-null  float64\n",
            " 2   housing_median_age  17000 non-null  float64\n",
            " 3   total_rooms         17000 non-null  float64\n",
            " 4   total_bedrooms      17000 non-null  float64\n",
            " 5   population          17000 non-null  float64\n",
            " 6   households          17000 non-null  float64\n",
            " 7   median_income       17000 non-null  float64\n",
            " 8   median_house_value  17000 non-null  float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 1.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV-lUMrIdqNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef72de23-c226-4188-e11f-e195dcc4741f"
      },
      "source": [
        "# printing test dataset information\n",
        "df_test.info()"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           3000 non-null   float64\n",
            " 1   latitude            3000 non-null   float64\n",
            " 2   housing_median_age  3000 non-null   float64\n",
            " 3   total_rooms         3000 non-null   float64\n",
            " 4   total_bedrooms      3000 non-null   float64\n",
            " 5   population          3000 non-null   float64\n",
            " 6   households          3000 non-null   float64\n",
            " 7   median_income       3000 non-null   float64\n",
            " 8   median_house_value  3000 non-null   float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 211.1 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Vwj6P4dqNe"
      },
      "source": [
        "#### Train and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wISS_1e2dqNf"
      },
      "source": [
        "X_train = df_train.drop('median_house_value',axis=1)\n",
        "y_train = df_train['median_house_value']\n",
        "X_test = df_test.drop('median_house_value',axis=1)\n",
        "y_test = df_test['median_house_value']"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Zhd27edqNg"
      },
      "source": [
        "#### Scaling Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLnjSOd6dqNh"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train= scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDSuABdjdqNi"
      },
      "source": [
        "There are two ways to implement MLP regressor one is using keras and the other way is using Scikit-Learn. In this section, we will discuss both two ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fnY7eWpdqNj"
      },
      "source": [
        "#### 1. Using Keras API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzTaNnmudqNk"
      },
      "source": [
        "Building, training, evaluating, and using a regression MLP using the Sequential API to make  predictions  is  quite  similar  to  what  we  did  for classification.  The  main  differences  are  the  fact  that  the  output  layer  has  a  single  neuron  (since  we  only  want  to predict  a  single  value)  and  uses  no  activation  function,  and  the  loss  function  is  the mean squared error. Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdUBydS0dqNl"
      },
      "source": [
        "# create a model with two layers\n",
        "model = Sequential([\n",
        "                    Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "                    Dense(1)\n",
        "                    ])\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glssNoESfQC_"
      },
      "source": [
        "Keras supports the early stopping of training via a callback called EarlyStopping.\n",
        "\n",
        "This callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process.\n",
        "\n",
        "The EarlyStopping callback is configured when instantiated via arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7MdewIYdqNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "344b35f8-3775-45e7-e146-5278f9812449"
      },
      "source": [
        "# defining early stop \n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "# fitting the model\n",
        "model.fit(x=X_train,y=y_train.values,\n",
        "          validation_data=(X_test,y_test.values),\n",
        "          batch_size=128,epochs=400, callbacks=[early_stop])\n",
        "          "
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "133/133 [==============================] - 1s 3ms/step - loss: 56424488960.0000 - val_loss: 55162970112.0000\n",
            "Epoch 2/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56422047744.0000 - val_loss: 55159586816.0000\n",
            "Epoch 3/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56417644544.0000 - val_loss: 55154221056.0000\n",
            "Epoch 4/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56411238400.0000 - val_loss: 55146864640.0000\n",
            "Epoch 5/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56402833408.0000 - val_loss: 55137599488.0000\n",
            "Epoch 6/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56392544256.0000 - val_loss: 55126499328.0000\n",
            "Epoch 7/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56380440576.0000 - val_loss: 55113617408.0000\n",
            "Epoch 8/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56366608384.0000 - val_loss: 55099076608.0000\n",
            "Epoch 9/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56351096832.0000 - val_loss: 55082942464.0000\n",
            "Epoch 10/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56334000128.0000 - val_loss: 55065243648.0000\n",
            "Epoch 11/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56315396096.0000 - val_loss: 55046066176.0000\n",
            "Epoch 12/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56295329792.0000 - val_loss: 55025467392.0000\n",
            "Epoch 13/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56273817600.0000 - val_loss: 55003492352.0000\n",
            "Epoch 14/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56250941440.0000 - val_loss: 54980177920.0000\n",
            "Epoch 15/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56226762752.0000 - val_loss: 54955610112.0000\n",
            "Epoch 16/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56201285632.0000 - val_loss: 54929752064.0000\n",
            "Epoch 17/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56174620672.0000 - val_loss: 54902702080.0000\n",
            "Epoch 18/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56146706432.0000 - val_loss: 54874509312.0000\n",
            "Epoch 19/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56117620736.0000 - val_loss: 54845161472.0000\n",
            "Epoch 20/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56087437312.0000 - val_loss: 54814720000.0000\n",
            "Epoch 21/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56056086528.0000 - val_loss: 54783111168.0000\n",
            "Epoch 22/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56023650304.0000 - val_loss: 54750474240.0000\n",
            "Epoch 23/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55990165504.0000 - val_loss: 54716760064.0000\n",
            "Epoch 24/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55955595264.0000 - val_loss: 54682054656.0000\n",
            "Epoch 25/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55919996928.0000 - val_loss: 54646317056.0000\n",
            "Epoch 26/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55883395072.0000 - val_loss: 54609580032.0000\n",
            "Epoch 27/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55845806080.0000 - val_loss: 54571819008.0000\n",
            "Epoch 28/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55807209472.0000 - val_loss: 54533156864.0000\n",
            "Epoch 29/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55767658496.0000 - val_loss: 54493499392.0000\n",
            "Epoch 30/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55727149056.0000 - val_loss: 54452953088.0000\n",
            "Epoch 31/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55685656576.0000 - val_loss: 54411403264.0000\n",
            "Epoch 32/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55643275264.0000 - val_loss: 54368944128.0000\n",
            "Epoch 33/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55599939584.0000 - val_loss: 54325633024.0000\n",
            "Epoch 34/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55555670016.0000 - val_loss: 54281334784.0000\n",
            "Epoch 35/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55510523904.0000 - val_loss: 54236188672.0000\n",
            "Epoch 36/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55464456192.0000 - val_loss: 54190149632.0000\n",
            "Epoch 37/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55417487360.0000 - val_loss: 54143213568.0000\n",
            "Epoch 38/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55369658368.0000 - val_loss: 54095474688.0000\n",
            "Epoch 39/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55320940544.0000 - val_loss: 54046744576.0000\n",
            "Epoch 40/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55271346176.0000 - val_loss: 53997211648.0000\n",
            "Epoch 41/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55220891648.0000 - val_loss: 53946802176.0000\n",
            "Epoch 42/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55169531904.0000 - val_loss: 53895598080.0000\n",
            "Epoch 43/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55117299712.0000 - val_loss: 53843521536.0000\n",
            "Epoch 44/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55064252416.0000 - val_loss: 53790474240.0000\n",
            "Epoch 45/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55010353152.0000 - val_loss: 53736706048.0000\n",
            "Epoch 46/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54955606016.0000 - val_loss: 53682053120.0000\n",
            "Epoch 47/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54900035584.0000 - val_loss: 53626638336.0000\n",
            "Epoch 48/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54843584512.0000 - val_loss: 53570334720.0000\n",
            "Epoch 49/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54786306048.0000 - val_loss: 53513273344.0000\n",
            "Epoch 50/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54728220672.0000 - val_loss: 53455319040.0000\n",
            "Epoch 51/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54669320192.0000 - val_loss: 53396643840.0000\n",
            "Epoch 52/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54609592320.0000 - val_loss: 53337100288.0000\n",
            "Epoch 53/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54549037056.0000 - val_loss: 53276680192.0000\n",
            "Epoch 54/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54487674880.0000 - val_loss: 53215555584.0000\n",
            "Epoch 55/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54425489408.0000 - val_loss: 53153619968.0000\n",
            "Epoch 56/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54362509312.0000 - val_loss: 53090922496.0000\n",
            "Epoch 57/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54298726400.0000 - val_loss: 53027270656.0000\n",
            "Epoch 58/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54234140672.0000 - val_loss: 52963074048.0000\n",
            "Epoch 59/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54168735744.0000 - val_loss: 52897918976.0000\n",
            "Epoch 60/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54102548480.0000 - val_loss: 52831907840.0000\n",
            "Epoch 61/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54035578880.0000 - val_loss: 52765306880.0000\n",
            "Epoch 62/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53967822848.0000 - val_loss: 52697812992.0000\n",
            "Epoch 63/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53899309056.0000 - val_loss: 52629622784.0000\n",
            "Epoch 64/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53830017024.0000 - val_loss: 52560629760.0000\n",
            "Epoch 65/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53759889408.0000 - val_loss: 52490809344.0000\n",
            "Epoch 66/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53689032704.0000 - val_loss: 52420370432.0000\n",
            "Epoch 67/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53617414144.0000 - val_loss: 52348936192.0000\n",
            "Epoch 68/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53544976384.0000 - val_loss: 52276903936.0000\n",
            "Epoch 69/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53471821824.0000 - val_loss: 52204171264.0000\n",
            "Epoch 70/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53397848064.0000 - val_loss: 52130619392.0000\n",
            "Epoch 71/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53323177984.0000 - val_loss: 52056305664.0000\n",
            "Epoch 72/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53247733760.0000 - val_loss: 51981172736.0000\n",
            "Epoch 73/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53171519488.0000 - val_loss: 51905396736.0000\n",
            "Epoch 74/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53094563840.0000 - val_loss: 51828813824.0000\n",
            "Epoch 75/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53016854528.0000 - val_loss: 51751559168.0000\n",
            "Epoch 76/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52938432512.0000 - val_loss: 51673554944.0000\n",
            "Epoch 77/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52859244544.0000 - val_loss: 51594780672.0000\n",
            "Epoch 78/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52779323392.0000 - val_loss: 51515441152.0000\n",
            "Epoch 79/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52698681344.0000 - val_loss: 51435204608.0000\n",
            "Epoch 80/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52617302016.0000 - val_loss: 51354185728.0000\n",
            "Epoch 81/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52535214080.0000 - val_loss: 51272671232.0000\n",
            "Epoch 82/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52452380672.0000 - val_loss: 51190349824.0000\n",
            "Epoch 83/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52368834560.0000 - val_loss: 51107254272.0000\n",
            "Epoch 84/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52284571648.0000 - val_loss: 51023507456.0000\n",
            "Epoch 85/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52199616512.0000 - val_loss: 50939023360.0000\n",
            "Epoch 86/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52113973248.0000 - val_loss: 50853744640.0000\n",
            "Epoch 87/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52027580416.0000 - val_loss: 50768179200.0000\n",
            "Epoch 88/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51940474880.0000 - val_loss: 50681647104.0000\n",
            "Epoch 89/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51852701696.0000 - val_loss: 50594283520.0000\n",
            "Epoch 90/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51764191232.0000 - val_loss: 50506416128.0000\n",
            "Epoch 91/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51674996736.0000 - val_loss: 50417733632.0000\n",
            "Epoch 92/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51585118208.0000 - val_loss: 50328461312.0000\n",
            "Epoch 93/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51494559744.0000 - val_loss: 50238550016.0000\n",
            "Epoch 94/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51403333632.0000 - val_loss: 50147913728.0000\n",
            "Epoch 95/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51311419392.0000 - val_loss: 50056634368.0000\n",
            "Epoch 96/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51218808832.0000 - val_loss: 49964490752.0000\n",
            "Epoch 97/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51125526528.0000 - val_loss: 49872048128.0000\n",
            "Epoch 98/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51031580672.0000 - val_loss: 49778601984.0000\n",
            "Epoch 99/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50937004032.0000 - val_loss: 49684561920.0000\n",
            "Epoch 100/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50841702400.0000 - val_loss: 49589932032.0000\n",
            "Epoch 101/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50745782272.0000 - val_loss: 49494740992.0000\n",
            "Epoch 102/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50649206784.0000 - val_loss: 49398894592.0000\n",
            "Epoch 103/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50551988224.0000 - val_loss: 49302388736.0000\n",
            "Epoch 104/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50454081536.0000 - val_loss: 49205141504.0000\n",
            "Epoch 105/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50355597312.0000 - val_loss: 49107308544.0000\n",
            "Epoch 106/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50256453632.0000 - val_loss: 49008820224.0000\n",
            "Epoch 107/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50156642304.0000 - val_loss: 48909819904.0000\n",
            "Epoch 108/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50056237056.0000 - val_loss: 48810053632.0000\n",
            "Epoch 109/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49955180544.0000 - val_loss: 48709742592.0000\n",
            "Epoch 110/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49853505536.0000 - val_loss: 48608886784.0000\n",
            "Epoch 111/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49751179264.0000 - val_loss: 48507121664.0000\n",
            "Epoch 112/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49648250880.0000 - val_loss: 48405049344.0000\n",
            "Epoch 113/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49544704000.0000 - val_loss: 48302448640.0000\n",
            "Epoch 114/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 49440591872.0000 - val_loss: 48199024640.0000\n",
            "Epoch 115/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 49335783424.0000 - val_loss: 48095195136.0000\n",
            "Epoch 116/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49230401536.0000 - val_loss: 47990448128.0000\n",
            "Epoch 117/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 49124405248.0000 - val_loss: 47885324288.0000\n",
            "Epoch 118/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49017851904.0000 - val_loss: 47779495936.0000\n",
            "Epoch 119/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 48910684160.0000 - val_loss: 47673151488.0000\n",
            "Epoch 120/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48802893824.0000 - val_loss: 47566274560.0000\n",
            "Epoch 121/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48694525952.0000 - val_loss: 47458721792.0000\n",
            "Epoch 122/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48585564160.0000 - val_loss: 47350804480.0000\n",
            "Epoch 123/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48476065792.0000 - val_loss: 47242006528.0000\n",
            "Epoch 124/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48365989888.0000 - val_loss: 47132647424.0000\n",
            "Epoch 125/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48255361024.0000 - val_loss: 47023026176.0000\n",
            "Epoch 126/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48144150528.0000 - val_loss: 46912765952.0000\n",
            "Epoch 127/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48032350208.0000 - val_loss: 46801899520.0000\n",
            "Epoch 128/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47920029696.0000 - val_loss: 46690324480.0000\n",
            "Epoch 129/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47807123456.0000 - val_loss: 46578368512.0000\n",
            "Epoch 130/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47693651968.0000 - val_loss: 46465949696.0000\n",
            "Epoch 131/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47579680768.0000 - val_loss: 46352830464.0000\n",
            "Epoch 132/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47465201664.0000 - val_loss: 46239334400.0000\n",
            "Epoch 133/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47350153216.0000 - val_loss: 46125195264.0000\n",
            "Epoch 134/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47234568192.0000 - val_loss: 46010646528.0000\n",
            "Epoch 135/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47118471168.0000 - val_loss: 45895360512.0000\n",
            "Epoch 136/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47001804800.0000 - val_loss: 45779750912.0000\n",
            "Epoch 137/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46884646912.0000 - val_loss: 45663580160.0000\n",
            "Epoch 138/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46767017984.0000 - val_loss: 45546889216.0000\n",
            "Epoch 139/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46648803328.0000 - val_loss: 45429710848.0000\n",
            "Epoch 140/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46530097152.0000 - val_loss: 45311938560.0000\n",
            "Epoch 141/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46410850304.0000 - val_loss: 45193990144.0000\n",
            "Epoch 142/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46291132416.0000 - val_loss: 45075140608.0000\n",
            "Epoch 143/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46170910720.0000 - val_loss: 44955860992.0000\n",
            "Epoch 144/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46050209792.0000 - val_loss: 44836286464.0000\n",
            "Epoch 145/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45929033728.0000 - val_loss: 44716220416.0000\n",
            "Epoch 146/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45807386624.0000 - val_loss: 44595589120.0000\n",
            "Epoch 147/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45685202944.0000 - val_loss: 44474548224.0000\n",
            "Epoch 148/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45562552320.0000 - val_loss: 44352933888.0000\n",
            "Epoch 149/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45439438848.0000 - val_loss: 44230836224.0000\n",
            "Epoch 150/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45315858432.0000 - val_loss: 44108402688.0000\n",
            "Epoch 151/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45191823360.0000 - val_loss: 43985494016.0000\n",
            "Epoch 152/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45067341824.0000 - val_loss: 43862118400.0000\n",
            "Epoch 153/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44942422016.0000 - val_loss: 43738292224.0000\n",
            "Epoch 154/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44817010688.0000 - val_loss: 43614056448.0000\n",
            "Epoch 155/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44691185664.0000 - val_loss: 43489243136.0000\n",
            "Epoch 156/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44564930560.0000 - val_loss: 43364085760.0000\n",
            "Epoch 157/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44438196224.0000 - val_loss: 43238617088.0000\n",
            "Epoch 158/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44311023616.0000 - val_loss: 43112804352.0000\n",
            "Epoch 159/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44183433216.0000 - val_loss: 42986348544.0000\n",
            "Epoch 160/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44055416832.0000 - val_loss: 42859458560.0000\n",
            "Epoch 161/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43926958080.0000 - val_loss: 42732142592.0000\n",
            "Epoch 162/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 43798151168.0000 - val_loss: 42604548096.0000\n",
            "Epoch 163/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 43668959232.0000 - val_loss: 42476474368.0000\n",
            "Epoch 164/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 43539329024.0000 - val_loss: 42348085248.0000\n",
            "Epoch 165/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 43409297408.0000 - val_loss: 42219249664.0000\n",
            "Epoch 166/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 43278913536.0000 - val_loss: 42090246144.0000\n",
            "Epoch 167/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 43148099584.0000 - val_loss: 41960615936.0000\n",
            "Epoch 168/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 43016855552.0000 - val_loss: 41830715392.0000\n",
            "Epoch 169/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 42885246976.0000 - val_loss: 41700503552.0000\n",
            "Epoch 170/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 42753314816.0000 - val_loss: 41569517568.0000\n",
            "Epoch 171/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 42620973056.0000 - val_loss: 41438461952.0000\n",
            "Epoch 172/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42488250368.0000 - val_loss: 41306886144.0000\n",
            "Epoch 173/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 42355183616.0000 - val_loss: 41175310336.0000\n",
            "Epoch 174/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 42221785088.0000 - val_loss: 41043152896.0000\n",
            "Epoch 175/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 42088038400.0000 - val_loss: 40910794752.0000\n",
            "Epoch 176/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41953931264.0000 - val_loss: 40778113024.0000\n",
            "Epoch 177/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41819484160.0000 - val_loss: 40644878336.0000\n",
            "Epoch 178/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41684643840.0000 - val_loss: 40511397888.0000\n",
            "Epoch 179/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41549516800.0000 - val_loss: 40377573376.0000\n",
            "Epoch 180/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 41414074368.0000 - val_loss: 40243363840.0000\n",
            "Epoch 181/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41278300160.0000 - val_loss: 40108961792.0000\n",
            "Epoch 182/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41142190080.0000 - val_loss: 39974264832.0000\n",
            "Epoch 183/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41005752320.0000 - val_loss: 39839227904.0000\n",
            "Epoch 184/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40869076992.0000 - val_loss: 39703945216.0000\n",
            "Epoch 185/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40732057600.0000 - val_loss: 39568146432.0000\n",
            "Epoch 186/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40594706432.0000 - val_loss: 39432400896.0000\n",
            "Epoch 187/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40457048064.0000 - val_loss: 39296249856.0000\n",
            "Epoch 188/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40319094784.0000 - val_loss: 39159611392.0000\n",
            "Epoch 189/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40180932608.0000 - val_loss: 39022567424.0000\n",
            "Epoch 190/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40042496000.0000 - val_loss: 38885736448.0000\n",
            "Epoch 191/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39903731712.0000 - val_loss: 38748598272.0000\n",
            "Epoch 192/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39764811776.0000 - val_loss: 38610685952.0000\n",
            "Epoch 193/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39625543680.0000 - val_loss: 38473109504.0000\n",
            "Epoch 194/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39486042112.0000 - val_loss: 38335246336.0000\n",
            "Epoch 195/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39346229248.0000 - val_loss: 38196916224.0000\n",
            "Epoch 196/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39206240256.0000 - val_loss: 38058086400.0000\n",
            "Epoch 197/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39066001408.0000 - val_loss: 37919342592.0000\n",
            "Epoch 198/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38925537280.0000 - val_loss: 37780357120.0000\n",
            "Epoch 199/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38784815104.0000 - val_loss: 37641400320.0000\n",
            "Epoch 200/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38643900416.0000 - val_loss: 37501812736.0000\n",
            "Epoch 201/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38502735872.0000 - val_loss: 37362061312.0000\n",
            "Epoch 202/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38361358336.0000 - val_loss: 37222305792.0000\n",
            "Epoch 203/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38219776000.0000 - val_loss: 37082189824.0000\n",
            "Epoch 204/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38077956096.0000 - val_loss: 36941950976.0000\n",
            "Epoch 205/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37935915008.0000 - val_loss: 36801548288.0000\n",
            "Epoch 206/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37793742848.0000 - val_loss: 36660621312.0000\n",
            "Epoch 207/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37651369984.0000 - val_loss: 36519809024.0000\n",
            "Epoch 208/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 37508788224.0000 - val_loss: 36379316224.0000\n",
            "Epoch 209/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37366124544.0000 - val_loss: 36237688832.0000\n",
            "Epoch 210/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37223260160.0000 - val_loss: 36096552960.0000\n",
            "Epoch 211/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37080199168.0000 - val_loss: 35955208192.0000\n",
            "Epoch 212/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36936941568.0000 - val_loss: 35813588992.0000\n",
            "Epoch 213/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36793548800.0000 - val_loss: 35671564288.0000\n",
            "Epoch 214/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36650008576.0000 - val_loss: 35529756672.0000\n",
            "Epoch 215/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36506324992.0000 - val_loss: 35387437056.0000\n",
            "Epoch 216/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36362555392.0000 - val_loss: 35245244416.0000\n",
            "Epoch 217/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36218634240.0000 - val_loss: 35102994432.0000\n",
            "Epoch 218/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36074528768.0000 - val_loss: 34960662528.0000\n",
            "Epoch 219/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35930365952.0000 - val_loss: 34818097152.0000\n",
            "Epoch 220/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35786051584.0000 - val_loss: 34675355648.0000\n",
            "Epoch 221/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35641634816.0000 - val_loss: 34532597760.0000\n",
            "Epoch 222/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35497140224.0000 - val_loss: 34389692416.0000\n",
            "Epoch 223/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35352526848.0000 - val_loss: 34246813696.0000\n",
            "Epoch 224/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35207819264.0000 - val_loss: 34103984128.0000\n",
            "Epoch 225/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35063033856.0000 - val_loss: 33960876032.0000\n",
            "Epoch 226/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34918207488.0000 - val_loss: 33817507840.0000\n",
            "Epoch 227/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34773233664.0000 - val_loss: 33674420224.0000\n",
            "Epoch 228/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34628206592.0000 - val_loss: 33530943488.0000\n",
            "Epoch 229/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34483093504.0000 - val_loss: 33387710464.0000\n",
            "Epoch 230/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34337908736.0000 - val_loss: 33244299264.0000\n",
            "Epoch 231/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34192701440.0000 - val_loss: 33100787712.0000\n",
            "Epoch 232/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34047453184.0000 - val_loss: 32957134848.0000\n",
            "Epoch 233/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33902190592.0000 - val_loss: 32813545472.0000\n",
            "Epoch 234/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33756856320.0000 - val_loss: 32670101504.0000\n",
            "Epoch 235/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 33611526144.0000 - val_loss: 32526354432.0000\n",
            "Epoch 236/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33466128384.0000 - val_loss: 32383002624.0000\n",
            "Epoch 237/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33320759296.0000 - val_loss: 32239265792.0000\n",
            "Epoch 238/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33175310336.0000 - val_loss: 32095662080.0000\n",
            "Epoch 239/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33029851136.0000 - val_loss: 31952056320.0000\n",
            "Epoch 240/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32884480000.0000 - val_loss: 31808002048.0000\n",
            "Epoch 241/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32739082240.0000 - val_loss: 31664648192.0000\n",
            "Epoch 242/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32593717248.0000 - val_loss: 31520985088.0000\n",
            "Epoch 243/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32448368640.0000 - val_loss: 31377432576.0000\n",
            "Epoch 244/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32303085568.0000 - val_loss: 31234078720.0000\n",
            "Epoch 245/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32157782016.0000 - val_loss: 31090761728.0000\n",
            "Epoch 246/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32012464128.0000 - val_loss: 30947069952.0000\n",
            "Epoch 247/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31867183104.0000 - val_loss: 30803759104.0000\n",
            "Epoch 248/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31721957376.0000 - val_loss: 30660499456.0000\n",
            "Epoch 249/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31576858624.0000 - val_loss: 30516824064.0000\n",
            "Epoch 250/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31431753728.0000 - val_loss: 30373789696.0000\n",
            "Epoch 251/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31286749184.0000 - val_loss: 30230505472.0000\n",
            "Epoch 252/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31141799936.0000 - val_loss: 30087518208.0000\n",
            "Epoch 253/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30996879360.0000 - val_loss: 29944705024.0000\n",
            "Epoch 254/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30852085760.0000 - val_loss: 29801570304.0000\n",
            "Epoch 255/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30707341312.0000 - val_loss: 29658828800.0000\n",
            "Epoch 256/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30562734080.0000 - val_loss: 29516060672.0000\n",
            "Epoch 257/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30418266112.0000 - val_loss: 29373337600.0000\n",
            "Epoch 258/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30273859584.0000 - val_loss: 29230909440.0000\n",
            "Epoch 259/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 30129639424.0000 - val_loss: 29088479232.0000\n",
            "Epoch 260/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29985503232.0000 - val_loss: 28946354176.0000\n",
            "Epoch 261/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29841498112.0000 - val_loss: 28804114432.0000\n",
            "Epoch 262/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29697638400.0000 - val_loss: 28662237184.0000\n",
            "Epoch 263/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29554016256.0000 - val_loss: 28520587264.0000\n",
            "Epoch 264/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29410445312.0000 - val_loss: 28379107328.0000\n",
            "Epoch 265/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29267005440.0000 - val_loss: 28237502464.0000\n",
            "Epoch 266/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29123829760.0000 - val_loss: 28096028672.0000\n",
            "Epoch 267/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28980770816.0000 - val_loss: 27954946048.0000\n",
            "Epoch 268/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28837849088.0000 - val_loss: 27814350848.0000\n",
            "Epoch 269/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28695158784.0000 - val_loss: 27673579520.0000\n",
            "Epoch 270/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28552652800.0000 - val_loss: 27533033472.0000\n",
            "Epoch 271/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28410339328.0000 - val_loss: 27392479232.0000\n",
            "Epoch 272/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28268142592.0000 - val_loss: 27252752384.0000\n",
            "Epoch 273/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28126253056.0000 - val_loss: 27112476672.0000\n",
            "Epoch 274/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27984562176.0000 - val_loss: 26972628992.0000\n",
            "Epoch 275/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27843127296.0000 - val_loss: 26833018880.0000\n",
            "Epoch 276/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27701897216.0000 - val_loss: 26694082560.0000\n",
            "Epoch 277/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27560871936.0000 - val_loss: 26555277312.0000\n",
            "Epoch 278/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27420102656.0000 - val_loss: 26416558080.0000\n",
            "Epoch 279/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27279601664.0000 - val_loss: 26277642240.0000\n",
            "Epoch 280/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27139299328.0000 - val_loss: 26139729920.0000\n",
            "Epoch 281/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26999322624.0000 - val_loss: 26001645568.0000\n",
            "Epoch 282/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26859622400.0000 - val_loss: 25863813120.0000\n",
            "Epoch 283/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26720118784.0000 - val_loss: 25726507008.0000\n",
            "Epoch 284/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26580969472.0000 - val_loss: 25589127168.0000\n",
            "Epoch 285/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26442100736.0000 - val_loss: 25452232704.0000\n",
            "Epoch 286/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26303447040.0000 - val_loss: 25315938304.0000\n",
            "Epoch 287/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26165157888.0000 - val_loss: 25179844608.0000\n",
            "Epoch 288/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26027173888.0000 - val_loss: 25043992576.0000\n",
            "Epoch 289/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25889513472.0000 - val_loss: 24908113920.0000\n",
            "Epoch 290/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25752127488.0000 - val_loss: 24772720640.0000\n",
            "Epoch 291/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25615089664.0000 - val_loss: 24637984768.0000\n",
            "Epoch 292/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25478381568.0000 - val_loss: 24503199744.0000\n",
            "Epoch 293/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25342017536.0000 - val_loss: 24368867328.0000\n",
            "Epoch 294/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25205948416.0000 - val_loss: 24235155456.0000\n",
            "Epoch 295/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25070313472.0000 - val_loss: 24101482496.0000\n",
            "Epoch 296/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24934979584.0000 - val_loss: 23968114688.0000\n",
            "Epoch 297/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 24800055296.0000 - val_loss: 23835289600.0000\n",
            "Epoch 298/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 24665556992.0000 - val_loss: 23703136256.0000\n",
            "Epoch 299/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24531408896.0000 - val_loss: 23571156992.0000\n",
            "Epoch 300/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24397617152.0000 - val_loss: 23439294464.0000\n",
            "Epoch 301/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24264181760.0000 - val_loss: 23308136448.0000\n",
            "Epoch 302/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24131141632.0000 - val_loss: 23177043968.0000\n",
            "Epoch 303/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23998451712.0000 - val_loss: 23046733824.0000\n",
            "Epoch 304/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23866347520.0000 - val_loss: 22916247552.0000\n",
            "Epoch 305/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23734589440.0000 - val_loss: 22786801664.0000\n",
            "Epoch 306/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23603339264.0000 - val_loss: 22657861632.0000\n",
            "Epoch 307/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23472445440.0000 - val_loss: 22529103872.0000\n",
            "Epoch 308/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23342000128.0000 - val_loss: 22400983040.0000\n",
            "Epoch 309/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23212009472.0000 - val_loss: 22272626688.0000\n",
            "Epoch 310/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23082426368.0000 - val_loss: 22145478656.0000\n",
            "Epoch 311/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22953310208.0000 - val_loss: 22018846720.0000\n",
            "Epoch 312/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22824718336.0000 - val_loss: 21892315136.0000\n",
            "Epoch 313/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22696591360.0000 - val_loss: 21766445056.0000\n",
            "Epoch 314/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22568937472.0000 - val_loss: 21640892416.0000\n",
            "Epoch 315/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22441805824.0000 - val_loss: 21515950080.0000\n",
            "Epoch 316/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22315102208.0000 - val_loss: 21391687680.0000\n",
            "Epoch 317/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22188959744.0000 - val_loss: 21266946048.0000\n",
            "Epoch 318/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22063298560.0000 - val_loss: 21143607296.0000\n",
            "Epoch 319/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21938196480.0000 - val_loss: 21020958720.0000\n",
            "Epoch 320/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21813665792.0000 - val_loss: 20898244608.0000\n",
            "Epoch 321/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21689481216.0000 - val_loss: 20777113600.0000\n",
            "Epoch 322/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21565943808.0000 - val_loss: 20655333376.0000\n",
            "Epoch 323/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21442912256.0000 - val_loss: 20534310912.0000\n",
            "Epoch 324/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21320443904.0000 - val_loss: 20414468096.0000\n",
            "Epoch 325/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21198526464.0000 - val_loss: 20294893568.0000\n",
            "Epoch 326/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21077186560.0000 - val_loss: 20175536128.0000\n",
            "Epoch 327/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20956424192.0000 - val_loss: 20056909824.0000\n",
            "Epoch 328/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20836259840.0000 - val_loss: 19939176448.0000\n",
            "Epoch 329/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20716693504.0000 - val_loss: 19821580288.0000\n",
            "Epoch 330/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20597708800.0000 - val_loss: 19705010176.0000\n",
            "Epoch 331/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20479352832.0000 - val_loss: 19588755456.0000\n",
            "Epoch 332/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20361568256.0000 - val_loss: 19473369088.0000\n",
            "Epoch 333/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20244428800.0000 - val_loss: 19358658560.0000\n",
            "Epoch 334/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20127936512.0000 - val_loss: 19244161024.0000\n",
            "Epoch 335/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20012019712.0000 - val_loss: 19130531840.0000\n",
            "Epoch 336/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19896686592.0000 - val_loss: 19017838592.0000\n",
            "Epoch 337/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19782070272.0000 - val_loss: 18905057280.0000\n",
            "Epoch 338/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19668051968.0000 - val_loss: 18793510912.0000\n",
            "Epoch 339/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19554736128.0000 - val_loss: 18682222592.0000\n",
            "Epoch 340/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19442046976.0000 - val_loss: 18572124160.0000\n",
            "Epoch 341/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19330093056.0000 - val_loss: 18462279680.0000\n",
            "Epoch 342/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19218761728.0000 - val_loss: 18353399808.0000\n",
            "Epoch 343/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 19108177920.0000 - val_loss: 18244704256.0000\n",
            "Epoch 344/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 18998261760.0000 - val_loss: 18137147392.0000\n",
            "Epoch 345/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 18889121792.0000 - val_loss: 18030200832.0000\n",
            "Epoch 346/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 18780573696.0000 - val_loss: 17924382720.0000\n",
            "Epoch 347/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 18672797696.0000 - val_loss: 17818753024.0000\n",
            "Epoch 348/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 18565652480.0000 - val_loss: 17713764352.0000\n",
            "Epoch 349/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 18459328512.0000 - val_loss: 17609469952.0000\n",
            "Epoch 350/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 18353688576.0000 - val_loss: 17506117632.0000\n",
            "Epoch 351/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 18248765440.0000 - val_loss: 17403703296.0000\n",
            "Epoch 352/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 18144606208.0000 - val_loss: 17301798912.0000\n",
            "Epoch 353/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 18041192448.0000 - val_loss: 17200437248.0000\n",
            "Epoch 354/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17938632704.0000 - val_loss: 17100423168.0000\n",
            "Epoch 355/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17836861440.0000 - val_loss: 17000997888.0000\n",
            "Epoch 356/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17735847936.0000 - val_loss: 16902321152.0000\n",
            "Epoch 357/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17635538944.0000 - val_loss: 16804255744.0000\n",
            "Epoch 358/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17536045056.0000 - val_loss: 16707332096.0000\n",
            "Epoch 359/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17437466624.0000 - val_loss: 16610894848.0000\n",
            "Epoch 360/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17339619328.0000 - val_loss: 16515191808.0000\n",
            "Epoch 361/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17242597376.0000 - val_loss: 16420482048.0000\n",
            "Epoch 362/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17146247168.0000 - val_loss: 16326844416.0000\n",
            "Epoch 363/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17050814464.0000 - val_loss: 16233547776.0000\n",
            "Epoch 364/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 16956168192.0000 - val_loss: 16141173760.0000\n",
            "Epoch 365/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 16862406656.0000 - val_loss: 16049556480.0000\n",
            "Epoch 366/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 16769484800.0000 - val_loss: 15959093248.0000\n",
            "Epoch 367/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 16677395456.0000 - val_loss: 15869347840.0000\n",
            "Epoch 368/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 16586099712.0000 - val_loss: 15780613120.0000\n",
            "Epoch 369/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 16495750144.0000 - val_loss: 15692249088.0000\n",
            "Epoch 370/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 16406249472.0000 - val_loss: 15605146624.0000\n",
            "Epoch 371/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 16317645824.0000 - val_loss: 15518816256.0000\n",
            "Epoch 372/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 16229895168.0000 - val_loss: 15433601024.0000\n",
            "Epoch 373/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 16143121408.0000 - val_loss: 15348964352.0000\n",
            "Epoch 374/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 16057219072.0000 - val_loss: 15265287168.0000\n",
            "Epoch 375/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15972127744.0000 - val_loss: 15182768128.0000\n",
            "Epoch 376/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15888045056.0000 - val_loss: 15100631040.0000\n",
            "Epoch 377/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15804822528.0000 - val_loss: 15019909120.0000\n",
            "Epoch 378/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15722526720.0000 - val_loss: 14939867136.0000\n",
            "Epoch 379/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15641131008.0000 - val_loss: 14860931072.0000\n",
            "Epoch 380/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15560646656.0000 - val_loss: 14782540800.0000\n",
            "Epoch 381/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15481079808.0000 - val_loss: 14705157120.0000\n",
            "Epoch 382/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15402483712.0000 - val_loss: 14628956160.0000\n",
            "Epoch 383/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15324841984.0000 - val_loss: 14553902080.0000\n",
            "Epoch 384/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15248175104.0000 - val_loss: 14479180800.0000\n",
            "Epoch 385/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15172414464.0000 - val_loss: 14405696512.0000\n",
            "Epoch 386/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15097628672.0000 - val_loss: 14333386752.0000\n",
            "Epoch 387/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 15023785984.0000 - val_loss: 14262150144.0000\n",
            "Epoch 388/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14951012352.0000 - val_loss: 14191072256.0000\n",
            "Epoch 389/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14879087616.0000 - val_loss: 14121979904.0000\n",
            "Epoch 390/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14808203264.0000 - val_loss: 14053416960.0000\n",
            "Epoch 391/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14738271232.0000 - val_loss: 13985916928.0000\n",
            "Epoch 392/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14669346816.0000 - val_loss: 13918730240.0000\n",
            "Epoch 393/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14601331712.0000 - val_loss: 13853220864.0000\n",
            "Epoch 394/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14534334464.0000 - val_loss: 13788705792.0000\n",
            "Epoch 395/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14468361216.0000 - val_loss: 13724759040.0000\n",
            "Epoch 396/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14403369984.0000 - val_loss: 13662588928.0000\n",
            "Epoch 397/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14339422208.0000 - val_loss: 13600869376.0000\n",
            "Epoch 398/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14276551680.0000 - val_loss: 13539875840.0000\n",
            "Epoch 399/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14214649856.0000 - val_loss: 13480545280.0000\n",
            "Epoch 400/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 14153883648.0000 - val_loss: 13421565952.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd459d77850>"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzYPgOOEdqNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777173f9-6aac-4326-c030-83121eec7f72"
      },
      "source": [
        "# Sequential Model Summary \n",
        "model.summary() "
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_178\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_418 (Dense)           (None, 30)                270       \n",
            "                                                                 \n",
            " dense_419 (Dense)           (None, 1)                 31        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 301\n",
            "Trainable params: 301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0IzyLyTdqNo"
      },
      "source": [
        "##### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_0If9N3dqNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6db88b4-8533-4d29-8c7b-9a725afa0f24"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[176612.89],\n",
              "       [174236.03],\n",
              "       [172838.39],\n",
              "       ...,\n",
              "       [132951.92],\n",
              "       [166734.33],\n",
              "       [186154.3 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osGmwIw7dqNq"
      },
      "source": [
        "##### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eou7_DeldqNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f01a50f-71f7-487b-b687-bfd5525adf70"
      },
      "source": [
        "np.sqrt(mean_squared_error(y_test,y_pred))"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "115851.47817981098"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52QacZPvdqNs"
      },
      "source": [
        "#### 2. Using Sci-kit Learn API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgmVeddndqNs"
      },
      "source": [
        "Using the same dataset, we will implement MLP regressor using sci-kit learn API. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUVZ9KE8dqNt"
      },
      "source": [
        "In the below code, one hidden layer is modeled with 32 neurons. Considering\n",
        "the input and output layer, we have a total of 5 layers in the model. In case any optimizer is not mentioned then “Adam” is the default optimizer and it can manage a pretty large dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkBzCoekdqNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ef8e1a-18f3-44ca-c1a7-70aed7a6fbff"
      },
      "source": [
        "# implementing MLPregressor\n",
        "regr = MLPRegressor(hidden_layer_sizes=(32), activation=\"relu\", random_state=1, max_iter=500).fit(X_train, y_train)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rruum2FKdqNv"
      },
      "source": [
        "##### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yPBpSl0dqN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada863ae-38fe-4899-ac6e-e2c2619455ed"
      },
      "source": [
        "y_pred = regr.predict(X_test)\n",
        "y_pred"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([106943.7296704 , 105433.71377299, 104575.22478819, ...,\n",
              "        80490.94502437, 100875.35533722, 112639.58639599])"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-HUagWGdqN2"
      },
      "source": [
        "##### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKmNxwzodqN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac2fe29-40d1-4f42-ddf3-76fc4340a012"
      },
      "source": [
        "np.sqrt(mean_squared_error(y_test,y_pred))"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "152367.3177159458"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehdUQLWSdqN4"
      },
      "source": [
        "Now, let us look at the tuning of the neural network hyperparameters or hyperparameter regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYgkmzLEdqN5"
      },
      "source": [
        "### Fine-Tuning Neural Network Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEYREMR3dqN6"
      },
      "source": [
        "The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters  to  tweak.  Not  only  can  we  use  any  imaginable  network  architecture, but even in a simple MLP we can change the number of layers, the number of neurons per layer, the type of activation function to use in each layer, the weight initialization  logic,  and  much  more. \n",
        "\n",
        "One  option  is  to  simply  try  many  combinations  of  hyperparameters  and  see  which one works best on the validation set (or using K-fold cross-validation). For this, one approach  is  simply to use  GridSearchCV  or  RandomizedSearchCV  to  explore  the  hyperparameter space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WawLE5TndqN6"
      },
      "source": [
        "The first step is to create a function that will build and compile a Keras model, given a set of hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYsW5K_VdqN7"
      },
      "source": [
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "    model = Sequential()\n",
        "    options = {\"input_shape\": input_shape}\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(Dense(n_neurons, activation=\"relu\", **options))\n",
        "        options = {}\n",
        "    model.add(Dense(1, **options))\n",
        "    optimizer = SGD(learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBH4CYF5dqN8"
      },
      "source": [
        "This function creates a simple Sequential model for univariate regression (only one output  neuron),  with  the  given  input  shape  and  the  given  number  of  hidden  layers and  neurons,  and  it  compiles  it  using  an  SGD  optimizer  configured  with  the  given learning rate.\n",
        "\n",
        "Next, let’s create a KerasRegressor based on this build_model() function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLNf-9oFdqN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b30e7d-0ac1-4131-b6d9-3086e99da2ec"
      },
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUo6pP6JdqN-"
      },
      "source": [
        "We want to train hundreds of variants and see which one performs best on the validation set. Since there are many hyperparameters, it is preferable to use a randomized search rather than grid search. \n",
        "\n",
        "Let’s try to explore the number of hidden layers, the number of neurons, and the learning rate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKMjD2XHdqN_"
      },
      "source": [
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(1, 100),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl4MtAGwdqOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7774291d-aca8-4942-b58a-7c1453aa5090"
      },
      "source": [
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
        "                  validation_data=(X_test,y_test),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])   "
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 412682289152.0000 - val_loss: 12812530688.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15152117760.0000 - val_loss: 12809405440.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151756288.0000 - val_loss: 12834106368.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151925248.0000 - val_loss: 12840471552.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150649344.0000 - val_loss: 12813318144.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15153724416.0000 - val_loss: 12809211904.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151130624.0000 - val_loss: 12863744000.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15147998208.0000 - val_loss: 12797598720.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150273536.0000 - val_loss: 12796205056.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15154423808.0000 - val_loss: 12832917504.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15152203776.0000 - val_loss: 12829323264.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15152759808.0000 - val_loss: 12840397824.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15148503040.0000 - val_loss: 12900489216.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15154078720.0000 - val_loss: 12792467456.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15153704960.0000 - val_loss: 12814295040.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15152898048.0000 - val_loss: 12818743296.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15152337920.0000 - val_loss: 12820867072.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15152422912.0000 - val_loss: 12829209600.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15153422336.0000 - val_loss: 12857352192.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150252032.0000 - val_loss: 12803335168.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151972352.0000 - val_loss: 12832939008.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15152991232.0000 - val_loss: 12814311424.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15154681856.0000 - val_loss: 12810293248.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151334400.0000 - val_loss: 12896252928.0000\n",
            "178/178 [==============================] - 0s 1ms/step - loss: 10246321152.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 44675064402018304.0000 - val_loss: 23888244736.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13070916608.0000 - val_loss: 12834077696.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12388581376.0000 - val_loss: 12801843200.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382529536.0000 - val_loss: 12806595584.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386852864.0000 - val_loss: 12853380096.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12389863424.0000 - val_loss: 12809133056.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384045056.0000 - val_loss: 12801294336.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385851392.0000 - val_loss: 12825484288.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386575360.0000 - val_loss: 12828125184.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386622464.0000 - val_loss: 12798701568.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381458432.0000 - val_loss: 12847290368.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12388692992.0000 - val_loss: 12806579200.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12387664896.0000 - val_loss: 12815886336.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385584128.0000 - val_loss: 12802137088.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386400256.0000 - val_loss: 12791942144.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386710528.0000 - val_loss: 12805263360.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12379020288.0000 - val_loss: 12820670464.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12387588096.0000 - val_loss: 12799993856.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382036992.0000 - val_loss: 12801364992.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12387264512.0000 - val_loss: 12803248128.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386897920.0000 - val_loss: 12814135296.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12387201024.0000 - val_loss: 12793707520.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12387149824.0000 - val_loss: 12792377344.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382390272.0000 - val_loss: 12837787648.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12389780480.0000 - val_loss: 12802043904.0000\n",
            "178/178 [==============================] - 0s 1ms/step - loss: 15601921024.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 106951583029986341158912.0000 - val_loss: 1786656600358912.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 108142763442176.0000 - val_loss: 12957378560.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12792353792.0000 - val_loss: 12830241792.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12788973568.0000 - val_loss: 12792948736.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12787638272.0000 - val_loss: 12799167488.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784731136.0000 - val_loss: 12829839360.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786388992.0000 - val_loss: 12837148672.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12789018624.0000 - val_loss: 12810506240.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12789959680.0000 - val_loss: 12798608384.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785079296.0000 - val_loss: 12816487424.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785733632.0000 - val_loss: 12792022016.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12787545088.0000 - val_loss: 12850117632.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785846272.0000 - val_loss: 12901006336.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12787544064.0000 - val_loss: 12792026112.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12788471808.0000 - val_loss: 12799478784.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12788023296.0000 - val_loss: 12806535168.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786928640.0000 - val_loss: 12834187264.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12789664768.0000 - val_loss: 12813467648.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785085440.0000 - val_loss: 12833636352.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12789982208.0000 - val_loss: 12806851584.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12790202368.0000 - val_loss: 12818944000.0000\n",
            "178/178 [==============================] - 0s 1ms/step - loss: 14920559616.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2195327033540608.0000 - val_loss: 12794152960.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15153301504.0000 - val_loss: 12809176064.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15158388736.0000 - val_loss: 12864386048.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15156768768.0000 - val_loss: 12803506176.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15159522304.0000 - val_loss: 12797276160.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15160820736.0000 - val_loss: 12825059328.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15155313664.0000 - val_loss: 12808440832.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150059520.0000 - val_loss: 12793687040.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15160972288.0000 - val_loss: 12835080192.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15153830912.0000 - val_loss: 12803954688.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15158534144.0000 - val_loss: 12810977280.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15152592896.0000 - val_loss: 12815934464.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150171136.0000 - val_loss: 12935906304.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150489600.0000 - val_loss: 12792481792.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15158774784.0000 - val_loss: 12871981056.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15155788800.0000 - val_loss: 12814723072.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15156632576.0000 - val_loss: 12822204416.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15156828160.0000 - val_loss: 12797304832.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151507456.0000 - val_loss: 12952642560.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15149710336.0000 - val_loss: 12823479296.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15156911104.0000 - val_loss: 12874848256.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15158766592.0000 - val_loss: 12838072320.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15159644160.0000 - val_loss: 12797805568.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15145754624.0000 - val_loss: 12797320192.0000\n",
            "178/178 [==============================] - 0s 1ms/step - loss: 9989571584.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7751667571228672.0000 - val_loss: 12804312064.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382926848.0000 - val_loss: 12851362816.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12392175616.0000 - val_loss: 12792095744.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12388028416.0000 - val_loss: 12965200896.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12394018816.0000 - val_loss: 12792584192.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12388110336.0000 - val_loss: 12795612160.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12395746304.0000 - val_loss: 12791946240.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12390785024.0000 - val_loss: 12806419456.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12390828032.0000 - val_loss: 12801383424.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12392481792.0000 - val_loss: 12826809344.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12391412736.0000 - val_loss: 12792170496.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12388474880.0000 - val_loss: 12800755712.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12388911104.0000 - val_loss: 12853470208.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12394037248.0000 - val_loss: 12820455424.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12390089728.0000 - val_loss: 12839054336.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12392173568.0000 - val_loss: 12845883392.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12392912896.0000 - val_loss: 12807456768.0000\n",
            "178/178 [==============================] - 0s 1ms/step - loss: 15609754624.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3710240948224.0000 - val_loss: 12804753408.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12788507648.0000 - val_loss: 12886184960.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12794398720.0000 - val_loss: 12824819712.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12789491712.0000 - val_loss: 12802377728.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12791723008.0000 - val_loss: 12792445952.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12796606464.0000 - val_loss: 12792731648.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785796096.0000 - val_loss: 12803783680.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12792334336.0000 - val_loss: 12807349248.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12789859328.0000 - val_loss: 12792420352.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12792192000.0000 - val_loss: 12793288704.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12791687168.0000 - val_loss: 12794717184.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12790899712.0000 - val_loss: 12795031552.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12794314752.0000 - val_loss: 12791941120.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12790843392.0000 - val_loss: 12801559552.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12788827136.0000 - val_loss: 12792306688.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12789896192.0000 - val_loss: 12824558592.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12793334784.0000 - val_loss: 12829689856.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12792839168.0000 - val_loss: 12806296576.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12792603648.0000 - val_loss: 12851706880.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12787395584.0000 - val_loss: 12892826624.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12791533568.0000 - val_loss: 12840345600.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12781312000.0000 - val_loss: 12793971712.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12790534144.0000 - val_loss: 12795463680.0000\n",
            "178/178 [==============================] - 0s 1ms/step - loss: 14816433152.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 289662180809244672.0000 - val_loss: 12909287424.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15160707072.0000 - val_loss: 12803066880.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15156461568.0000 - val_loss: 12812955648.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15156658176.0000 - val_loss: 12801972224.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15154792448.0000 - val_loss: 12791835648.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15156320256.0000 - val_loss: 12803254272.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15159645184.0000 - val_loss: 12793868288.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15148932096.0000 - val_loss: 12792138752.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15156529152.0000 - val_loss: 12842393600.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15156222976.0000 - val_loss: 12849177600.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15155565568.0000 - val_loss: 12804631552.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15157113856.0000 - val_loss: 12861200384.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15156093952.0000 - val_loss: 12813384704.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15159083008.0000 - val_loss: 12819028992.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15147335680.0000 - val_loss: 12847960064.0000\n",
            "178/178 [==============================] - 0s 1ms/step - loss: 10163794944.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 22151475200.0000 - val_loss: 12792956928.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12391213056.0000 - val_loss: 12792544256.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12389122048.0000 - val_loss: 12792195072.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12389898240.0000 - val_loss: 12803896320.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12387310592.0000 - val_loss: 12801043456.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12391902208.0000 - val_loss: 12827533312.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12389528576.0000 - val_loss: 12795298816.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12390408192.0000 - val_loss: 12806270976.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12391984128.0000 - val_loss: 12836210688.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12392763392.0000 - val_loss: 12811815936.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12392392704.0000 - val_loss: 12796477440.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12388362240.0000 - val_loss: 12793177088.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381559808.0000 - val_loss: 12856980480.0000\n",
            "178/178 [==============================] - 0s 1ms/step - loss: 15672415232.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 153488937019506688.0000 - val_loss: 20080909877248.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15813901811712.0000 - val_loss: 12091077427200.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9530439106560.0000 - val_loss: 7280932159488.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5746016124928.0000 - val_loss: 4384819773440.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3466385424384.0000 - val_loss: 2641966923776.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2093643988992.0000 - val_loss: 1593351471104.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1267014303744.0000 - val_loss: 962484895744.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 769192361984.0000 - val_loss: 582991675392.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 469315944448.0000 - val_loss: 354805088256.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 288664125440.0000 - val_loss: 217725911040.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 179914522624.0000 - val_loss: 135406518272.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 114397511680.0000 - val_loss: 86009585664.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 74938368000.0000 - val_loss: 56381857792.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 51148615680.0000 - val_loss: 38647508992.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 36820836352.0000 - val_loss: 28079194112.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 28209643520.0000 - val_loss: 21773633536.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 23015542784.0000 - val_loss: 18032793600.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 19888769024.0000 - val_loss: 15819689984.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 18004987904.0000 - val_loss: 14515425280.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16868126720.0000 - val_loss: 13754113024.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16182611968.0000 - val_loss: 13316372480.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15771432960.0000 - val_loss: 13067016192.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15522435072.0000 - val_loss: 12928104448.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15372573696.0000 - val_loss: 12853525504.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15282338816.0000 - val_loss: 12816430080.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15228800000.0000 - val_loss: 12798957568.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15196009472.0000 - val_loss: 12792520704.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15175534592.0000 - val_loss: 12792111104.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15163629568.0000 - val_loss: 12794391552.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15156429824.0000 - val_loss: 12797751296.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15152317440.0000 - val_loss: 12801623040.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15149440000.0000 - val_loss: 12804892672.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15148021760.0000 - val_loss: 12807885824.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15147095040.0000 - val_loss: 12810338304.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146490880.0000 - val_loss: 12812382208.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146252288.0000 - val_loss: 12813825024.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146010624.0000 - val_loss: 12815550464.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15145864192.0000 - val_loss: 12816406528.0000\n",
            "178/178 [==============================] - 0s 1ms/step - loss: 10100440064.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1413738307099033600.0000 - val_loss: 167806704287744.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 131857098211328.0000 - val_loss: 101055555698688.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 79417443352576.0000 - val_loss: 60856335335424.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 47834896269312.0000 - val_loss: 36648528117760.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 28814140768256.0000 - val_loss: 22071291674624.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 17358938701824.0000 - val_loss: 13293315948544.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10459800403968.0000 - val_loss: 8007314309120.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6304391233536.0000 - val_loss: 4824496078848.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3801727893504.0000 - val_loss: 2908521234432.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2294659153920.0000 - val_loss: 1755104018432.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1386972446720.0000 - val_loss: 1060786601984.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 840275329024.0000 - val_loss: 642952855552.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 511021187072.0000 - val_loss: 391488471040.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 312700633088.0000 - val_loss: 240262086656.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 193282146304.0000 - val_loss: 149328969728.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 121353756672.0000 - val_loss: 94633787392.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 78011342848.0000 - val_loss: 61789396992.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 51912601600.0000 - val_loss: 42095108096.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 36200239104.0000 - val_loss: 30257227776.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 26720972800.0000 - val_loss: 23169284096.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 21015091200.0000 - val_loss: 18932774912.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 17576693760.0000 - val_loss: 16408558592.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15508577280.0000 - val_loss: 14902557696.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14261185536.0000 - val_loss: 14015798272.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13512323072.0000 - val_loss: 13494162432.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13063886848.0000 - val_loss: 13188903936.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12792956928.0000 - val_loss: 13011707904.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12629961728.0000 - val_loss: 12909517824.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12530985984.0000 - val_loss: 12851990528.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12471390208.0000 - val_loss: 12820767744.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12435895296.0000 - val_loss: 12804391936.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12414428160.0000 - val_loss: 12796420096.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12401495040.0000 - val_loss: 12792771584.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12393342976.0000 - val_loss: 12791804928.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12388234240.0000 - val_loss: 12792133632.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385414144.0000 - val_loss: 12792993792.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12383694848.0000 - val_loss: 12794064896.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382594048.0000 - val_loss: 12795214848.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381938688.0000 - val_loss: 12795978752.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381553664.0000 - val_loss: 12797115392.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381240320.0000 - val_loss: 12797938688.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381120512.0000 - val_loss: 12798776320.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12380972032.0000 - val_loss: 12799027200.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12380961792.0000 - val_loss: 12799454208.0000\n",
            "178/178 [==============================] - 0s 1ms/step - loss: 15597943808.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3602889072842386244452220928.0000 - val_loss: 510093942783437929709568.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 400614426232213745958912.0000 - val_loss: 307210418478896000270336.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 241275053803212535496704.0000 - val_loss: 185021167318479973384192.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 145310947820118267133952.0000 - val_loss: 111431430672922085163008.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 87515172938162123046912.0000 - val_loss: 67110934782816357974016.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 52707139648591711174656.0000 - val_loss: 40418324971372796706816.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 31743509383530256793600.0000 - val_loss: 24342415353099522670592.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 19117911022576949264384.0000 - val_loss: 14660545348981038776320.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11513996257027678011392.0000 - val_loss: 8829495657694253547520.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6934448488491159388160.0000 - val_loss: 5317669733064033435648.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4176362730421198258176.0000 - val_loss: 3202635514337749368832.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2515261799261305569280.0000 - val_loss: 1928821351658605772800.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1514846392523547475968.0000 - val_loss: 1161658136040224849920.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 912336257025459617792.0000 - val_loss: 699623154219170660352.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 549465507633895047168.0000 - val_loss: 421356745952411516928.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 330922459925602893824.0000 - val_loss: 253767512389119377408.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 199302297909653929984.0000 - val_loss: 152834702312212529152.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 120032373786445611008.0000 - val_loss: 92046645062151438336.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 72291051142830358528.0000 - val_loss: 55436316343256743936.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 43538184359743848448.0000 - val_loss: 33387204940069339136.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 26221392992736903168.0000 - val_loss: 20107866449744232448.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15792164563467632640.0000 - val_loss: 12110221179441119232.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9511029567448416256.0000 - val_loss: 7293530668259082240.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5728132324294393856.0000 - val_loss: 4392624819267436544.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3449840877449510912.0000 - val_loss: 2645518984673230848.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2077712350953078784.0000 - val_loss: 1593300238225899520.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1251329719438147584.0000 - val_loss: 959588190342086656.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 753629321494528000.0000 - val_loss: 577925164352667648.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 453882557474275328.0000 - val_loss: 348063668631502848.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 273356509268672512.0000 - val_loss: 209626581939781632.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 164632367107932160.0000 - val_loss: 126250623795462144.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 99151785339387904.0000 - val_loss: 76036537640288256.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 59715460052025344.0000 - val_loss: 45794375829028864.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 35964391836876800.0000 - val_loss: 27580564135477248.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 21660016142450688.0000 - val_loss: 16610953521004544.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13045007531376640.0000 - val_loss: 10004327452114944.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7856527050276864.0000 - val_loss: 6025368280498176.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4731684939366400.0000 - val_loss: 3628959333875712.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2849721269878784.0000 - val_loss: 2185672618147840.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1716286245568512.0000 - val_loss: 1316419958472704.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1033660316778496.0000 - val_loss: 792886060777472.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 622540477169664.0000 - val_loss: 477568989921280.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 374937961365504.0000 - val_loss: 287658253221888.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 225817116278784.0000 - val_loss: 173273996328960.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 136006380552192.0000 - val_loss: 104380934127616.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 81917642801152.0000 - val_loss: 62883874471936.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 49341108584448.0000 - val_loss: 37888905445376.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 29721498746880.0000 - val_loss: 22832478158848.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 17905332781056.0000 - val_loss: 13763147202560.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10788975673344.0000 - val_loss: 8298967859200.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6502666993664.0000 - val_loss: 5007119220736.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3921406066688.0000 - val_loss: 3023741911040.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2366751375360.0000 - val_loss: 1828490838016.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1430440247296.0000 - val_loss: 1108084129792.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 866537439232.0000 - val_loss: 673834139648.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 526921924608.0000 - val_loss: 412043902976.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 322456780800.0000 - val_loss: 254149525504.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 199315079168.0000 - val_loss: 158847909888.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 125142622208.0000 - val_loss: 101277319168.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 80453492736.0000 - val_loss: 66507624448.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 53552345088.0000 - val_loss: 45469376512.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 37338578944.0000 - val_loss: 32713748480.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 27567040512.0000 - val_loss: 24994179072.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 21696616448.0000 - val_loss: 20287858688.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 18145470464.0000 - val_loss: 17425772544.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16011403264.0000 - val_loss: 15675174912.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14726471680.0000 - val_loss: 14603537408.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13953853440.0000 - val_loss: 13939894272.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13485552640.0000 - val_loss: 13528110080.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13204964352.0000 - val_loss: 13272957952.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13036453888.0000 - val_loss: 13112741888.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12935596032.0000 - val_loss: 13008741376.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12873755648.0000 - val_loss: 12943892480.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12837738496.0000 - val_loss: 12900489216.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12815298560.0000 - val_loss: 12872663040.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12802400256.0000 - val_loss: 12853830656.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12794450944.0000 - val_loss: 12839662592.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12789229568.0000 - val_loss: 12830954496.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786507776.0000 - val_loss: 12824537088.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784783360.0000 - val_loss: 12820058112.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12783822848.0000 - val_loss: 12817214464.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12783179776.0000 - val_loss: 12814498816.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782817280.0000 - val_loss: 12812530688.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782590976.0000 - val_loss: 12811421696.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782488576.0000 - val_loss: 12810816512.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782427136.0000 - val_loss: 12810012672.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782336000.0000 - val_loss: 12809374720.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782291968.0000 - val_loss: 12808848384.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782325760.0000 - val_loss: 12807877632.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782302208.0000 - val_loss: 12807182336.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782282752.0000 - val_loss: 12807482368.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782321664.0000 - val_loss: 12807093248.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782272512.0000 - val_loss: 12807006208.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782272512.0000 - val_loss: 12807047168.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782287872.0000 - val_loss: 12806917120.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782277632.0000 - val_loss: 12806579200.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782280704.0000 - val_loss: 12806457344.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782281728.0000 - val_loss: 12806736896.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782296064.0000 - val_loss: 12806652928.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782320640.0000 - val_loss: 12806841344.0000\n",
            "178/178 [==============================] - 0s 1ms/step - loss: 14875874304.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2641936345168406644785152.0000 - val_loss: 686391508145211965440.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 188200942420371701760.0000 - val_loss: 19328345690999881728.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5299622406380322816.0000 - val_loss: 544269525743304704.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 149234050358312960.0000 - val_loss: 15325832734048256.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4202359211163648.0000 - val_loss: 431455469568000.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 118332757901312.0000 - val_loss: 12139528978432.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3341979222016.0000 - val_loss: 350613798912.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 106000867328.0000 - val_loss: 21840498688.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15051843584.0000 - val_loss: 13002779648.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12464385024.0000 - val_loss: 12791806976.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385139712.0000 - val_loss: 12794016768.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382740480.0000 - val_loss: 12799438848.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382194688.0000 - val_loss: 12805559296.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382548992.0000 - val_loss: 12797357056.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382519296.0000 - val_loss: 12798436352.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382402560.0000 - val_loss: 12798838784.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381566976.0000 - val_loss: 12804027392.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382334976.0000 - val_loss: 12799726592.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382043136.0000 - val_loss: 12798033920.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382156800.0000 - val_loss: 12802398208.0000\n",
            "178/178 [==============================] - 0s 2ms/step - loss: 15602461696.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 710644829048524341539940335616.0000 - val_loss: 1430736099655564312707072.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 131297556226791486521344.0000 - val_loss: 21746499414944382976.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 1995657584878551040.0000 - val_loss: 330811635138560.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 30353725063168.0000 - val_loss: 18143744000.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15549113344.0000 - val_loss: 12816258048.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150204928.0000 - val_loss: 12831226880.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150454784.0000 - val_loss: 12831243264.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150732288.0000 - val_loss: 12819307520.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15143860224.0000 - val_loss: 12791849984.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150810112.0000 - val_loss: 12832561152.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151032320.0000 - val_loss: 12811245568.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151635456.0000 - val_loss: 12823446528.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151004672.0000 - val_loss: 12839702528.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15147506688.0000 - val_loss: 12833812480.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15148900352.0000 - val_loss: 12798789632.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151586304.0000 - val_loss: 12846311424.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150479360.0000 - val_loss: 12813295616.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150946304.0000 - val_loss: 12827666432.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151833088.0000 - val_loss: 12831211520.0000\n",
            "178/178 [==============================] - 0s 2ms/step - loss: 10131788800.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 35261427712.0000 - val_loss: 18775500800.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 17757859840.0000 - val_loss: 13148124160.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14854737920.0000 - val_loss: 12233349120.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14252366848.0000 - val_loss: 12028693504.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14021123072.0000 - val_loss: 11906742272.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13846623232.0000 - val_loss: 11789129728.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13684707328.0000 - val_loss: 11675458560.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13527541760.0000 - val_loss: 11561656320.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13375248384.0000 - val_loss: 11453258752.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13226517504.0000 - val_loss: 11338920960.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13082511360.0000 - val_loss: 11236372480.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12942118912.0000 - val_loss: 11134812160.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12804168704.0000 - val_loss: 11036867584.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12670654464.0000 - val_loss: 10935119872.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12540393472.0000 - val_loss: 10844039168.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12412744704.0000 - val_loss: 10750141440.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12288849920.0000 - val_loss: 10657533952.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12167872512.0000 - val_loss: 10570625024.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12050130944.0000 - val_loss: 10486248448.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11935139840.0000 - val_loss: 10401850368.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11822416896.0000 - val_loss: 10317487104.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11712209920.0000 - val_loss: 10242284544.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11604189184.0000 - val_loss: 10159038464.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11499225088.0000 - val_loss: 10079885312.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11397045248.0000 - val_loss: 10005320704.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11296906240.0000 - val_loss: 9930140672.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11199024128.0000 - val_loss: 9861260288.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11103554560.0000 - val_loss: 9792028672.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11009859584.0000 - val_loss: 9718775808.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10918141952.0000 - val_loss: 9648934912.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10828877824.0000 - val_loss: 9583166464.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10741465088.0000 - val_loss: 9522588672.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10655863808.0000 - val_loss: 9460017152.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10572026880.0000 - val_loss: 9404239872.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10489832448.0000 - val_loss: 9344315392.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10409488384.0000 - val_loss: 9288761344.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10330851328.0000 - val_loss: 9228701696.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10253956096.0000 - val_loss: 9172764672.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10178549760.0000 - val_loss: 9112795136.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10104938496.0000 - val_loss: 9057010688.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10032526336.0000 - val_loss: 9011183616.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9961459712.0000 - val_loss: 8958401536.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9892025344.0000 - val_loss: 8897908736.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9824291840.0000 - val_loss: 8850422784.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9757533184.0000 - val_loss: 8801254400.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9691973632.0000 - val_loss: 8755612672.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9628130304.0000 - val_loss: 8710259712.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9565302784.0000 - val_loss: 8660488192.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9503800320.0000 - val_loss: 8619613184.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9443344384.0000 - val_loss: 8578245120.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9384450048.0000 - val_loss: 8523831808.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9326457856.0000 - val_loss: 8478258688.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9269782528.0000 - val_loss: 8439117312.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9214295040.0000 - val_loss: 8397658112.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9159622656.0000 - val_loss: 8349198336.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9106011136.0000 - val_loss: 8308883456.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9053408256.0000 - val_loss: 8274536960.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9002087424.0000 - val_loss: 8228530176.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8951604224.0000 - val_loss: 8202227712.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8902133760.0000 - val_loss: 8159669760.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8853564416.0000 - val_loss: 8125960192.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8806063104.0000 - val_loss: 8085787648.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8759461888.0000 - val_loss: 8046156288.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8713585664.0000 - val_loss: 8008593408.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8668565504.0000 - val_loss: 7970593280.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8624460800.0000 - val_loss: 7944067584.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8581104128.0000 - val_loss: 7919615488.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8538862080.0000 - val_loss: 7877892608.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8497020928.0000 - val_loss: 7843150848.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8456087552.0000 - val_loss: 7804144128.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8416162304.0000 - val_loss: 7777996288.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8376723456.0000 - val_loss: 7745162752.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8337998848.0000 - val_loss: 7720465920.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8300420096.0000 - val_loss: 7689011200.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8263265280.0000 - val_loss: 7656418304.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8226624000.0000 - val_loss: 7622189056.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8191124480.0000 - val_loss: 7595627520.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8156066816.0000 - val_loss: 7568411136.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8121563136.0000 - val_loss: 7537963520.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8087787520.0000 - val_loss: 7514870784.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8054379520.0000 - val_loss: 7490042880.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8021802496.0000 - val_loss: 7460958720.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7989730304.0000 - val_loss: 7434664448.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7958422528.0000 - val_loss: 7407525888.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7927872000.0000 - val_loss: 7389592576.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7897397248.0000 - val_loss: 7366952448.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7867692032.0000 - val_loss: 7341059584.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7838437888.0000 - val_loss: 7324210176.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7810024448.0000 - val_loss: 7300670464.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7781879296.0000 - val_loss: 7272881664.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7754296832.0000 - val_loss: 7251226112.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7727350784.0000 - val_loss: 7230351872.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7700753408.0000 - val_loss: 7208160768.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7674589696.0000 - val_loss: 7184683520.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7648936960.0000 - val_loss: 7155538432.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7624067072.0000 - val_loss: 7140926976.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7599147520.0000 - val_loss: 7123296768.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7575075328.0000 - val_loss: 7102849536.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7551202304.0000 - val_loss: 7085322240.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7527961088.0000 - val_loss: 7062641664.0000\n",
            "178/178 [==============================] - 0s 2ms/step - loss: 6496598528.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 32362172416.0000 - val_loss: 18958415872.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15215563776.0000 - val_loss: 13222983680.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12326013952.0000 - val_loss: 12267936768.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11755703296.0000 - val_loss: 12053873664.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11567033344.0000 - val_loss: 11941157888.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11440789504.0000 - val_loss: 11840792576.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11328559104.0000 - val_loss: 11741027328.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11219250176.0000 - val_loss: 11642264576.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11112212480.0000 - val_loss: 11544045568.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11008377856.0000 - val_loss: 11447155712.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10905082880.0000 - val_loss: 11348348928.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10804470784.0000 - val_loss: 11254316032.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10705812480.0000 - val_loss: 11161060352.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10609215488.0000 - val_loss: 11070615552.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10514363392.0000 - val_loss: 10981098496.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10420793344.0000 - val_loss: 10896115712.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10329618432.0000 - val_loss: 10809788416.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10239671296.0000 - val_loss: 10725032960.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10151407616.0000 - val_loss: 10642543616.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10064907264.0000 - val_loss: 10561281024.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9979938816.0000 - val_loss: 10481603584.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9896291328.0000 - val_loss: 10405138432.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9814435840.0000 - val_loss: 10326556672.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9733788672.0000 - val_loss: 10251367424.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9654401024.0000 - val_loss: 10176269312.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9576675328.0000 - val_loss: 10103508992.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9500304384.0000 - val_loss: 10031633408.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9425373184.0000 - val_loss: 9961474048.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9351277568.0000 - val_loss: 9891747840.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9278862336.0000 - val_loss: 9823950848.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9207696384.0000 - val_loss: 9756995584.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9137761280.0000 - val_loss: 9691339776.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9069040640.0000 - val_loss: 9626785792.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9001944064.0000 - val_loss: 9563590656.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8935464960.0000 - val_loss: 9501254656.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8870253568.0000 - val_loss: 9439886336.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8806512640.0000 - val_loss: 9380000768.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8743673856.0000 - val_loss: 9320994816.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8682101760.0000 - val_loss: 9263027200.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8621447168.0000 - val_loss: 9206094848.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8561872384.0000 - val_loss: 9150142464.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8503296512.0000 - val_loss: 9095050240.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8445546496.0000 - val_loss: 9040942080.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8389065216.0000 - val_loss: 8987751424.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8333604352.0000 - val_loss: 8935374848.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8278811648.0000 - val_loss: 8883607552.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8225063936.0000 - val_loss: 8833231872.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8172294656.0000 - val_loss: 8783639552.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8120406528.0000 - val_loss: 8734844928.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8069572096.0000 - val_loss: 8687101952.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8019414016.0000 - val_loss: 8639538176.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7970006528.0000 - val_loss: 8593110016.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7921483776.0000 - val_loss: 8547886592.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7874115584.0000 - val_loss: 8503538688.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7827495936.0000 - val_loss: 8459270656.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7781490688.0000 - val_loss: 8416220672.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7736314368.0000 - val_loss: 8373252608.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7691542528.0000 - val_loss: 8331048960.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7647874048.0000 - val_loss: 8290388480.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7605174272.0000 - val_loss: 8249586176.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7562786304.0000 - val_loss: 8209852928.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7521305088.0000 - val_loss: 8171235328.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7480528896.0000 - val_loss: 8132377088.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7440366080.0000 - val_loss: 8094839808.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7400858624.0000 - val_loss: 8057365504.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7362180608.0000 - val_loss: 8020873216.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7324179456.0000 - val_loss: 7985161216.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7286685696.0000 - val_loss: 7950168064.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7249800192.0000 - val_loss: 7915282432.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7213517312.0000 - val_loss: 7881835520.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7178099712.0000 - val_loss: 7847837696.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7142842880.0000 - val_loss: 7815039488.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7108486144.0000 - val_loss: 7782616576.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7074646016.0000 - val_loss: 7751312384.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7041383936.0000 - val_loss: 7718801408.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7008616448.0000 - val_loss: 7688350720.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6976455680.0000 - val_loss: 7658947072.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6944913920.0000 - val_loss: 7628438016.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6913818624.0000 - val_loss: 7599471616.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6883064320.0000 - val_loss: 7570192384.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6853039616.0000 - val_loss: 7541928448.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6823373312.0000 - val_loss: 7514401280.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6794375168.0000 - val_loss: 7487373824.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6765539328.0000 - val_loss: 7461839360.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6737666560.0000 - val_loss: 7434434048.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6709953024.0000 - val_loss: 7407418880.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6682625024.0000 - val_loss: 7381194752.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6655995392.0000 - val_loss: 7355395072.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6629662720.0000 - val_loss: 7330648576.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6603680768.0000 - val_loss: 7307297792.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6578043904.0000 - val_loss: 7282565120.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6553279488.0000 - val_loss: 7259209216.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6528539136.0000 - val_loss: 7236233216.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6504169472.0000 - val_loss: 7214255104.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6480353792.0000 - val_loss: 7191211008.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6456768000.0000 - val_loss: 7169558016.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6433732096.0000 - val_loss: 7147178496.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6411031040.0000 - val_loss: 7125678080.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6388832256.0000 - val_loss: 7104799232.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6366843904.0000 - val_loss: 7083795456.0000\n",
            "178/178 [==============================] - 0s 1ms/step - loss: 9021513728.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 30913992704.0000 - val_loss: 19765334016.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14952724480.0000 - val_loss: 13806354432.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12412933120.0000 - val_loss: 12628738048.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11928957952.0000 - val_loss: 12286053376.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11759508480.0000 - val_loss: 12121553920.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11643302912.0000 - val_loss: 12006925312.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11536990208.0000 - val_loss: 11913469952.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11433847808.0000 - val_loss: 11826312192.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11333294080.0000 - val_loss: 11735957504.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11234840576.0000 - val_loss: 11660325888.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11138632704.0000 - val_loss: 11579901952.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11044303872.0000 - val_loss: 11498399744.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10952289280.0000 - val_loss: 11425122304.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10861913088.0000 - val_loss: 11353245696.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10773740544.0000 - val_loss: 11280327680.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10686891008.0000 - val_loss: 11212446720.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10602267648.0000 - val_loss: 11143856128.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10519234560.0000 - val_loss: 11077231616.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10437878784.0000 - val_loss: 11005224960.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10358228992.0000 - val_loss: 10944369664.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10280141824.0000 - val_loss: 10876402688.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10203632640.0000 - val_loss: 10807049216.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10128954368.0000 - val_loss: 10748912640.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10055520256.0000 - val_loss: 10694991872.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9983613952.0000 - val_loss: 10639554560.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9913109504.0000 - val_loss: 10586544128.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9844126720.0000 - val_loss: 10523488256.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9776580608.0000 - val_loss: 10465708032.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9710045184.0000 - val_loss: 10406241280.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9645230080.0000 - val_loss: 10357931008.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9581324288.0000 - val_loss: 10306126848.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9518540800.0000 - val_loss: 10261773312.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9457403904.0000 - val_loss: 10215197696.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9397412864.0000 - val_loss: 10167624704.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9338608640.0000 - val_loss: 10115176448.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9280574464.0000 - val_loss: 10075842560.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9224159232.0000 - val_loss: 10020329472.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9168332800.0000 - val_loss: 9980250112.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9113924608.0000 - val_loss: 9925384192.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9060658176.0000 - val_loss: 9884728320.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9007993856.0000 - val_loss: 9842094080.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8956521472.0000 - val_loss: 9804481536.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8906063872.0000 - val_loss: 9758458880.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8856443904.0000 - val_loss: 9720720384.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8807750656.0000 - val_loss: 9683885056.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8759971840.0000 - val_loss: 9641414656.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8713069568.0000 - val_loss: 9594636288.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8667124736.0000 - val_loss: 9553916928.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8622133248.0000 - val_loss: 9522861056.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8577734144.0000 - val_loss: 9494844416.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8534490112.0000 - val_loss: 9453454336.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8491732992.0000 - val_loss: 9413041152.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8449561088.0000 - val_loss: 9366151168.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8408585728.0000 - val_loss: 9341908992.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8368120320.0000 - val_loss: 9301691392.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8328651776.0000 - val_loss: 9263873024.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8289672192.0000 - val_loss: 9232507904.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8251325952.0000 - val_loss: 9215596544.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8213910528.0000 - val_loss: 9171538944.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8177234944.0000 - val_loss: 9136679936.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8140731904.0000 - val_loss: 9111852032.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8105184768.0000 - val_loss: 9076242432.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8070214656.0000 - val_loss: 9051201536.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8035922944.0000 - val_loss: 9015731200.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8002326528.0000 - val_loss: 8980343808.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7969167872.0000 - val_loss: 8961124352.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7936741376.0000 - val_loss: 8931861504.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7904848896.0000 - val_loss: 8897187840.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7873181696.0000 - val_loss: 8857025536.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7842292224.0000 - val_loss: 8830918656.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7811903488.0000 - val_loss: 8801822720.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7782311424.0000 - val_loss: 8779821056.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7752967680.0000 - val_loss: 8749027328.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7724162560.0000 - val_loss: 8732984320.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7696037376.0000 - val_loss: 8708242432.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7668280832.0000 - val_loss: 8682510336.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7641063936.0000 - val_loss: 8651533312.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7614052352.0000 - val_loss: 8613515264.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7587859456.0000 - val_loss: 8590714880.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7561944576.0000 - val_loss: 8565751808.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7536581120.0000 - val_loss: 8546551296.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7511356928.0000 - val_loss: 8513632768.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7486854656.0000 - val_loss: 8491730432.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7462512640.0000 - val_loss: 8474262528.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7438633472.0000 - val_loss: 8451777536.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7415356928.0000 - val_loss: 8424213504.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7392345088.0000 - val_loss: 8406973440.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7369693184.0000 - val_loss: 8382541824.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7347496960.0000 - val_loss: 8360092160.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7325593088.0000 - val_loss: 8337262592.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7304017920.0000 - val_loss: 8311734272.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7282680832.0000 - val_loss: 8298853376.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7262237184.0000 - val_loss: 8273242112.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7241719296.0000 - val_loss: 8254566912.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7221731328.0000 - val_loss: 8229727232.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7201985536.0000 - val_loss: 8209218560.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7182483968.0000 - val_loss: 8181964288.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7163119104.0000 - val_loss: 8157608960.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7144442880.0000 - val_loss: 8142387200.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7126034432.0000 - val_loss: 8119284224.0000\n",
            "178/178 [==============================] - 0s 2ms/step - loss: 10462810112.0000\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [-1.35896006e+10 -1.34719198e+10             nan -1.35247527e+10\n",
            "             nan             nan             nan             nan\n",
            "             nan -8.66030746e+09]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "532/532 [==============================] - 1s 2ms/step - loss: 27837587456.0000 - val_loss: 14939103232.0000\n",
            "Epoch 2/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13731283968.0000 - val_loss: 12284511232.0000\n",
            "Epoch 3/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 12658148352.0000 - val_loss: 11982483456.0000\n",
            "Epoch 4/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 12422228992.0000 - val_loss: 11813485568.0000\n",
            "Epoch 5/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 12243009536.0000 - val_loss: 11653492736.0000\n",
            "Epoch 6/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 12073314304.0000 - val_loss: 11498523648.0000\n",
            "Epoch 7/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 11908450304.0000 - val_loss: 11347643392.0000\n",
            "Epoch 8/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 11748525056.0000 - val_loss: 11200348160.0000\n",
            "Epoch 9/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 11592928256.0000 - val_loss: 11058572288.0000\n",
            "Epoch 10/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 11442014208.0000 - val_loss: 10919966720.0000\n",
            "Epoch 11/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 11295085568.0000 - val_loss: 10784838656.0000\n",
            "Epoch 12/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 11152557056.0000 - val_loss: 10653927424.0000\n",
            "Epoch 13/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 11013962752.0000 - val_loss: 10525742080.0000\n",
            "Epoch 14/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 10878776320.0000 - val_loss: 10403377152.0000\n",
            "Epoch 15/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 10747858944.0000 - val_loss: 10281315328.0000\n",
            "Epoch 16/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 10620164096.0000 - val_loss: 10164755456.0000\n",
            "Epoch 17/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 10495873024.0000 - val_loss: 10049936384.0000\n",
            "Epoch 18/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 10375818240.0000 - val_loss: 9939792896.0000\n",
            "Epoch 19/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 10258208768.0000 - val_loss: 9832455168.0000\n",
            "Epoch 20/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 10143774720.0000 - val_loss: 9728111616.0000\n",
            "Epoch 21/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 10032897024.0000 - val_loss: 9625704448.0000\n",
            "Epoch 22/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 9924665344.0000 - val_loss: 9526283264.0000\n",
            "Epoch 23/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 9819235328.0000 - val_loss: 9429404672.0000\n",
            "Epoch 24/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 9716896768.0000 - val_loss: 9335406592.0000\n",
            "Epoch 25/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 9617143808.0000 - val_loss: 9244193792.0000\n",
            "Epoch 26/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 9519688704.0000 - val_loss: 9154939904.0000\n",
            "Epoch 27/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 9425226752.0000 - val_loss: 9067964416.0000\n",
            "Epoch 28/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 9333176320.0000 - val_loss: 8983970816.0000\n",
            "Epoch 29/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 9243082752.0000 - val_loss: 8901465088.0000\n",
            "Epoch 30/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 9155718144.0000 - val_loss: 8821491712.0000\n",
            "Epoch 31/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 9070503936.0000 - val_loss: 8744124416.0000\n",
            "Epoch 32/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8987956224.0000 - val_loss: 8668145664.0000\n",
            "Epoch 33/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8907290624.0000 - val_loss: 8593931264.0000\n",
            "Epoch 34/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8828440576.0000 - val_loss: 8521913344.0000\n",
            "Epoch 35/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8752088064.0000 - val_loss: 8452171264.0000\n",
            "Epoch 36/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8677444608.0000 - val_loss: 8383676416.0000\n",
            "Epoch 37/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8605020160.0000 - val_loss: 8317362688.0000\n",
            "Epoch 38/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8534116864.0000 - val_loss: 8252647424.0000\n",
            "Epoch 39/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8465283072.0000 - val_loss: 8189667328.0000\n",
            "Epoch 40/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8398205440.0000 - val_loss: 8128876544.0000\n",
            "Epoch 41/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8333052928.0000 - val_loss: 8068945920.0000\n",
            "Epoch 42/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8269054464.0000 - val_loss: 8010457088.0000\n",
            "Epoch 43/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8207280640.0000 - val_loss: 7953869312.0000\n",
            "Epoch 44/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 8146793472.0000 - val_loss: 7898913280.0000\n",
            "Epoch 45/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 8088044032.0000 - val_loss: 7845090304.0000\n",
            "Epoch 46/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8030630400.0000 - val_loss: 7792949760.0000\n",
            "Epoch 47/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7974943744.0000 - val_loss: 7741732864.0000\n",
            "Epoch 48/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7920338432.0000 - val_loss: 7691951104.0000\n",
            "Epoch 49/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 7867292160.0000 - val_loss: 7643578880.0000\n",
            "Epoch 50/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7815633408.0000 - val_loss: 7596403200.0000\n",
            "Epoch 51/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7765032960.0000 - val_loss: 7550646272.0000\n",
            "Epoch 52/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7716210688.0000 - val_loss: 7505605632.0000\n",
            "Epoch 53/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7668250624.0000 - val_loss: 7461847040.0000\n",
            "Epoch 54/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7621530624.0000 - val_loss: 7419369984.0000\n",
            "Epoch 55/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7575979008.0000 - val_loss: 7377940992.0000\n",
            "Epoch 56/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7531589120.0000 - val_loss: 7337536512.0000\n",
            "Epoch 57/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7488578560.0000 - val_loss: 7298206208.0000\n",
            "Epoch 58/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7446400000.0000 - val_loss: 7259984384.0000\n",
            "Epoch 59/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7405514240.0000 - val_loss: 7222561280.0000\n",
            "Epoch 60/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7365282816.0000 - val_loss: 7186016256.0000\n",
            "Epoch 61/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7326382080.0000 - val_loss: 7150488576.0000\n",
            "Epoch 62/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7288397824.0000 - val_loss: 7115916800.0000\n",
            "Epoch 63/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7251351040.0000 - val_loss: 7082281472.0000\n",
            "Epoch 64/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7215214080.0000 - val_loss: 7049302528.0000\n",
            "Epoch 65/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7179888640.0000 - val_loss: 7017211904.0000\n",
            "Epoch 66/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7145628672.0000 - val_loss: 6986064896.0000\n",
            "Epoch 67/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 7112236544.0000 - val_loss: 6955527680.0000\n",
            "Epoch 68/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 7079445504.0000 - val_loss: 6925787648.0000\n",
            "Epoch 69/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 7047667200.0000 - val_loss: 6896798720.0000\n",
            "Epoch 70/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 7016536576.0000 - val_loss: 6868561920.0000\n",
            "Epoch 71/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6986336256.0000 - val_loss: 6840941056.0000\n",
            "Epoch 72/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6956647936.0000 - val_loss: 6814161408.0000\n",
            "Epoch 73/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6927832576.0000 - val_loss: 6788064256.0000\n",
            "Epoch 74/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 6899745792.0000 - val_loss: 6762558976.0000\n",
            "Epoch 75/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6872346624.0000 - val_loss: 6737594368.0000\n",
            "Epoch 76/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 6845440512.0000 - val_loss: 6713169408.0000\n",
            "Epoch 77/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6819235840.0000 - val_loss: 6689763328.0000\n",
            "Epoch 78/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6794074112.0000 - val_loss: 6666422784.0000\n",
            "Epoch 79/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6769164288.0000 - val_loss: 6643747328.0000\n",
            "Epoch 80/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6744855040.0000 - val_loss: 6621883904.0000\n",
            "Epoch 81/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6721372160.0000 - val_loss: 6600460800.0000\n",
            "Epoch 82/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6698384384.0000 - val_loss: 6579425792.0000\n",
            "Epoch 83/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6675845120.0000 - val_loss: 6559107072.0000\n",
            "Epoch 84/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6653919232.0000 - val_loss: 6539316224.0000\n",
            "Epoch 85/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6632468992.0000 - val_loss: 6520107008.0000\n",
            "Epoch 86/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6611592704.0000 - val_loss: 6501024256.0000\n",
            "Epoch 87/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6591230976.0000 - val_loss: 6482403328.0000\n",
            "Epoch 88/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6571323904.0000 - val_loss: 6464403968.0000\n",
            "Epoch 89/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6551879168.0000 - val_loss: 6447046656.0000\n",
            "Epoch 90/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6533020160.0000 - val_loss: 6429619200.0000\n",
            "Epoch 91/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6514402816.0000 - val_loss: 6413231104.0000\n",
            "Epoch 92/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6496479744.0000 - val_loss: 6396858368.0000\n",
            "Epoch 93/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6478866432.0000 - val_loss: 6380454912.0000\n",
            "Epoch 94/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6461628416.0000 - val_loss: 6364852224.0000\n",
            "Epoch 95/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6444760064.0000 - val_loss: 6349526016.0000\n",
            "Epoch 96/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6428374528.0000 - val_loss: 6334657536.0000\n",
            "Epoch 97/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6412175872.0000 - val_loss: 6320588288.0000\n",
            "Epoch 98/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 6396518912.0000 - val_loss: 6306800128.0000\n",
            "Epoch 99/100\n",
            "532/532 [==============================] - 1s 1ms/step - loss: 6381466624.0000 - val_loss: 6292225024.0000\n",
            "Epoch 100/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6366523392.0000 - val_loss: 6278643200.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3,\n",
              "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7fd45dfcea10>,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fd45a257ed0>,\n",
              "                                        'n_hidden': [0, 1, 2, 3],\n",
              "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
              "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
              "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
              "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
              "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDyLtOT8dqOB"
      },
      "source": [
        "The above  exploration  may  last  many  hours  depending  on  the  hardware,  the  size  of  the dataset, the complexity of the model and the value of n_iter and cv. When it is over, we can access the best parameters found, the best score, and the trained Keras model like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgsAM812dqOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b324809c-1812-4358-fab8-5e47b5be2abf"
      },
      "source": [
        "# finding the best parameters\n",
        "rnd_search_cv.best_params_"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.0007499431826469648, 'n_hidden': 0, 'n_neurons': 62}"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeOHn8WVdqOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb81db83-99a0-477e-a96c-bd209b660aff"
      },
      "source": [
        "# best score\n",
        "rnd_search_cv.best_score_"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-8660307456.0"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNY1vgDAdqOD"
      },
      "source": [
        "# applying best parameters to the model for predictions\n",
        "model = rnd_search_cv.best_estimator_.model"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqut1OR3dqOE"
      },
      "source": [
        "Refer to the guidelines below for choosing the  number  of  hidden  layers  and  neurons  in  an  MLP,  and  selecting  appropriate  values  for some of the main hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9t_szGwdqOF"
      },
      "source": [
        "#### Number of Hidden Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqiYlS6AdqOG"
      },
      "source": [
        "- For simple problems, we can start with just one or two hidden layers and get the accurate results.\n",
        "- For more complex problems, we can gradually rampup the number of hidden layers, until we start overfitting the training set. Very complex  tasks,  such  as  large  image  classification  or  speech  recognition,  typically  require networks  with  dozens  of  layers  (or  even  hundreds,  but  not  fully  connected  ones),  and  they  need  a  huge  amount  of  training  data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZu_Fc1ydqOH"
      },
      "source": [
        "#### Number of Neurons per Hidden Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-1AYlmBdqOI"
      },
      "source": [
        "- We can try increasing the number of neurons gradually  until  the  network  starts  overfitting. \n",
        "- In general, it may be more advantageous to increase  the  number  of  layers  than  the  number  of  neurons  per  layer.\n",
        "- A  simpler  approach  is  to  pick  a  model  with  more  layers  and  neurons  than  we actually need, then use early stopping to prevent it from overfitting (and other regularization  techniques,  such  as  dropout, which we will discuss further in this notebook)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQJXWhRIdqOJ"
      },
      "source": [
        "#### Learning Rate, Batch Size, and Other Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMuxqHl1dqOJ"
      },
      "source": [
        "Here are some of the important hyperparameters other than hidden layers and neurons, and some tips on how to set them:\n",
        "\n",
        "- The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges). So a  simple  approach  for  tuning  the  learning  rate  is  to  start  with  a  large  value  that makes  the  training  algorithm  diverge,  then  divide  this  value  by  3  and  try  again, and repeat until the training algorithm stops diverging.\n",
        "- Choosing  a  better  optimizer  than  plain  old  Mini-batch  Gradient  Descent  (and tuning its hyperparameters) is also quite important. We will discuss this in further sections.\n",
        "- The  batch  size  can  also  have  a  significant  impact  on  our  model’s  performance and the training time. In general the optimal batch size will be lower than 32. We will study batch normalization further in this notebook.\n",
        "- We discussed the choice of the activation function in previous assignment notebook, the $ReLU$ activation function will be a good default for all hidden layers. For the output layer, it really depends on our task.\n",
        "- In  most  cases,  the  number  of  training  iterations  does  not actually  need  to  be tweaked: just use early stopping instead.\n",
        "\n",
        "Let us also take a look at techniques such as Batch normalization, overfitting, drop out, optimizers and learning rate to  train deep neural networks.\n",
        "\n",
        "To know more about hyperparameter tuning of deep neural networks, click [here](https://towardsdatascience.com/the-art-of-hyperparameter-tuning-in-deep-neural-nets-by-example-685cb5429a38)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc_IEVbFdqOK"
      },
      "source": [
        "### Accelerate Learning of Deep Neural Networks With Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRK5Px-qdqOM"
      },
      "source": [
        "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n",
        "\n",
        "![Image](https://lh3.googleusercontent.com/-9hMD_jyPLuE/YNQoKrc-_4I/AAAAAAAACTs/9nZ-BEdtI-QoAe6R5HXlG6X3AcprX8NaQCJEEGAsYHg/s512/2021-06-23.png)\n",
        "\n",
        "$\\text{Figure: Batch Normalization Algorithm}$\n",
        "\n",
        "So  during  training,  BN  just  standardizes  its  inputs  then  rescales  and  offsets  them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_el24cRdqON"
      },
      "source": [
        "#### Implementing Batch Normalization with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmjykNm-dqOO"
      },
      "source": [
        "Implementing  Batch  Normalization  is  quite  simple, just  add  a  BatchNormalization  layer  before  or  after  each  hidden  layer’s  activation function,  and  optionally  add  a  BN  layer  as  well  as  the  first  layer  in  our  model.  For example,  this  model  applies  BN  after  every  hidden  layer  and  as  the  first  layer  in  the\n",
        "model (after flattening the input images):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUc-PjdHdqOP"
      },
      "source": [
        "# create model with Batch Normalization\n",
        "model = Sequential([\n",
        "                    Flatten(input_shape=[28, 28]),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(10, activation=\"softmax\")\n",
        "                    ])"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3u4zONAdqOQ"
      },
      "source": [
        "If we display the model summary, we can see that each BN layer adds 4 parameters per input: γ, β, μ and σ (for example, the first BN layer adds 3136 parameters, which is 4 times 784). The last two parameters, μ and σ, are the moving averages, they are not affected by backpropagation, so Keras calls them “Nontrainable” (if we count the total number of BN parameters, 3136 + 1200 + 400, and divide  by  two,  we get  2,368,  which  is  the  total  number  of  non-trainable  params  in this model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpyQU5LLdqOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6927f102-0c59-429f-9f25-83fbb906eb5f"
      },
      "source": [
        "# summary of model\n",
        "model.summary()"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_210\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_16 (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 784)              3136      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_502 (Dense)           (None, 300)               235500    \n",
            "                                                                 \n",
            " batch_normalization_33 (Bat  (None, 300)              1200      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_503 (Dense)           (None, 100)               30100     \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_504 (Dense)           (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaE1qvfhdqOS"
      },
      "source": [
        "Let’s look at the parameters of the first BN layer. Two are trainable (by backprop), and two are not:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTpS00QzdqOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9623406-cb3e-4e84-8f58-a38d4a208a0c"
      },
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('batch_normalization_32/gamma:0', True),\n",
              " ('batch_normalization_32/beta:0', True),\n",
              " ('batch_normalization_32/moving_mean:0', False),\n",
              " ('batch_normalization_32/moving_variance:0', False)]"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6zEJrikdqOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de08a55b-b5ea-43a4-c078-dda46bdcee53"
      },
      "source": [
        "model.layers[1].updates"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ku_jrTpdqOU"
      },
      "source": [
        "Moreover,  since  a  Batch  Normalization layer includes one offset parameter per input, we can remove the bias term from the previous layer (just pass `use_bias=False` when creating it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWeHUnCvdqOV"
      },
      "source": [
        "# create model\n",
        "model = Sequential([\n",
        "                    Flatten(input_shape=[28, 28]),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "                    BatchNormalization(),\n",
        "                    Activation(\"relu\"),\n",
        "                    Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "                    Activation(\"relu\"),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(10, activation=\"softmax\")\n",
        "                    ])"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nQq1BuIdqOW"
      },
      "source": [
        "The  BatchNormalization class has regularizable hyperparameters. Tweaking the “momentum” argument allows us to control how much of the statistics from the previous mini batch to include when the update is calculated.\n",
        "\n",
        "\n",
        "A good momentum value is typically close to 1, for example, 0.9, 0.99, or 0.999 \n",
        "\n",
        "To know more about batch normalization, click [here](https://towardsdatascience.com/batch-normalization-in-practice-an-example-with-keras-and-tensorflow-2-0-b1ec28bde96f)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvBrjDXmdqOX"
      },
      "source": [
        "###  Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tv15JM0dqOX"
      },
      "source": [
        "Some popular optimizers used for boosting the speed in training large deep neural networks are: Momentum optimization, RMSProp, and Adam optimization. Refer [here](https://mlfromscratch.com/optimizers-explained/#/) for a detailed understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA4iM1aQdqOY"
      },
      "source": [
        "#### Momentum Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz-Y0M-SdqOZ"
      },
      "source": [
        "Momentum  optimization subtracts  the  local  gradient  from  the  momentum  vector  m  (multiplied  by  the  learning  rate  η),  and  it  updates  the  weights  by  simply  adding  this momentum vector, thus accelerating the speed. The momentum hyperparameter $β$ is introduced to prevent  the momentum from growing too large (set between 0 and 1, typically 0.9).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz1B-VDedqOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d12dfa00-8ee4-4a30-b424-c601ccde8169"
      },
      "source": [
        "#Implementing the momentum optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFp9_IkfdqOb"
      },
      "source": [
        "#### RMSProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cWLQD2hdqOc"
      },
      "source": [
        "The RMSProp algorithm fixes only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step. \n",
        "\n",
        "The decay rate $β$ is typically set to 0.9. Yes, it is once again a new hyperparameter, but this default value often works well, so we may not need to tune it at all.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ta7gsJFdqOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78cb70e4-08a2-49bd-e496-14dc3863014d"
      },
      "source": [
        "#Implementing the RMSProp optimizer\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1vPNNs7dqOe"
      },
      "source": [
        "#### Adam Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMvwuAridqOf"
      },
      "source": [
        "Adam combines the ideas of Momentum  optimization  and  RMSProp:  it keeps track of both, an  exponentially  decaying  average  of  past  gradients,  and  an  exponentially  decaying  average  of  past  squared  gradients.\n",
        "\n",
        "The momentum decay hyperparameter $β_1$ is typically initialized to 0.9, while the scaling  decay  hyperparameter  $β_2$  is  often  initialized  to  0.999."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EfFHwAFdqOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbdf6d4-e826-4fc6-8a22-37c668392fd9"
      },
      "source": [
        "#Implementing the Adam optimizer\n",
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYIwICgtdqOh"
      },
      "source": [
        "To know more about optimizers, click [here](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6#:~:text=Optimizers%20are%20algorithms%20or%20methods,help%20to%20get%20results%20faster)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt5i_U0udqOh"
      },
      "source": [
        "#### Learning Rate Schedule For Training Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3ln8QNTdqOi"
      },
      "source": [
        "The simplest adaptation of learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdmHxgkMdqOj"
      },
      "source": [
        "##### Time-Based Learning Rate Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI2I4w5ZdqOk"
      },
      "source": [
        "Keras has an in-built time-based learning rate schedule function.\n",
        "\n",
        "The decay argument in the stochastic gradient descent optimization algorithm  is used in the time-based learning rate decay schedule equation as follows:\n",
        "\n",
        "- LearningRate = LearningRate * $\\frac{1}{(1 + decay * epoch)}$\n",
        "\n",
        "When the decay argument is zero (the default), this has no effect on the learning rate.\n",
        "\n",
        "- LearningRate = 0.1 * $\\frac{1}{(1 + 0.0 * 1)} \\implies $LearningRate = 0.1\n",
        "\n",
        "When the decay argument is specified, it will decrease the learning rate from the previous epoch by the given fixed amount.\n",
        "\n",
        "*See the implementation of time-based learning rate scheduling with the MNIST dataset Example at the end of this notebook.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ltSUwe4dqOu"
      },
      "source": [
        "### Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTtpElDodqOv"
      },
      "source": [
        "Deep neural networks may have millions of parameters. The network, therefore,   has vast freedom and can fit a huge variety of complex datasets. This flexibility however also makes it prone to overfitting the training set. Thus we need regularization.\n",
        "\n",
        "Let us now see some popular regularization techniques for neural networks: $ℓ1$ and $ℓ2$ regularization and dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD98NJQ6dqOw"
      },
      "source": [
        "#### $ℓ1$ and $ℓ2$ Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7-pZLbsdqOw"
      },
      "source": [
        "We can use $ℓ1$ and $ℓ2$ regularization  to  constrain  a  neural  network’s  connection  weights  (but  typically  not  its  biases).  Here  is  how  to  apply  $ℓ2$  regularization  to  a  Keras  layer’s  connection  weights, using a regularization factor of 0.01:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E369mam-dqOx"
      },
      "source": [
        "layer = Dense(100, activation=\"relu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD9lTIltdqOy"
      },
      "source": [
        "Applying the  same  regularizer, activation function and initialization strategy repeatedly to  all  layers  in  our  network may make it error-prone. To avoid this, we can try refactoring our code to use loops. Another option is to use Python’s `functools.partial()` function: it lets us  create  a  thin  wrapper  for  any  callable,  with  some  default  argument  values.  For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IUZfTosdqOz"
      },
      "source": [
        "# creating regularized dense layer for model\n",
        "RegularizedDense = partial(keras.layers.Dense,\n",
        "                           activation=\"relu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwH4qCwhdqO0"
      },
      "source": [
        "# defining model with regularization\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=[28, 28]),\n",
        "    RegularizedDense(300),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(10, activation=\"softmax\",\n",
        "                     kernel_initializer=\"glorot_uniform\")\n",
        "])"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ9wGk2FdqO1"
      },
      "source": [
        "#### Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23QlGnLbdqO2"
      },
      "source": [
        "Dropout  is  one  of  the  most  popular  regularization  techniques  for  deep  neural  networks. At each training stage, individual nodes are either dropped out of the net with probability 1-p or kept with probability p, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.\n",
        "\n",
        "![Image](https://i.ibb.co/HnfSTyX/M5-2.jpg)\n",
        "\n",
        "$\\text{Figure: Dropout Regularization}$\n",
        "\n",
        "To  implement  dropout  using  Keras,  we  can  use  the  keras.layers.Dropout  layer. During  training,  it  randomly  drops  some  inputs  (setting  them  to  0)  and  divides  the remaining inputs by the keep probability. After training, it just passes  the  inputs  to  the  next  layer.  For  example,  the  following  code  applies  dropout regularization before every Dense layer, using a dropout rate of 0.2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IErGvcSdqO3"
      },
      "source": [
        "model = Sequential([\n",
        "                    Flatten(input_shape=[28, 28]),\n",
        "                    Dropout(rate=0.2),\n",
        "                    Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "                    Dropout(rate=0.2),\n",
        "                    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "                    Dropout(rate=0.2),\n",
        "                    Dense(10, activation=\"softmax\")\n",
        "                    ])"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpTgUd2EdqO4"
      },
      "source": [
        "If we observe that the model is overfitting, we can increase the dropout rate. Conversely, we should try decreasing the dropout rate if the model underfits the training set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iQC5OcP8cTx"
      },
      "source": [
        "Based on the learnings above, let us now explore hyperparameter tuning during the neural network training phase.\n",
        "\n",
        "Here, we implement the sequential model and use the **MNIST dataset**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVb26VZQMLXn"
      },
      "source": [
        "#### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYcT8C3cO7Nc"
      },
      "source": [
        "We load the MNIST dataset, using Keras' dataset utilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJsnHKAIMsW4"
      },
      "source": [
        "# data resizing variables\n",
        "NUM_ROWS = 28\n",
        "NUM_COLS = 28\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWHAgEhhVSJE"
      },
      "source": [
        "To feed MNIST instances into a neural network, they need to be reshaped, from a 2D image representation to a single dimension sequence. We also convert the class vector to a binary matrix (using to_categorical). This is accomplished below after which the same function defined above is called again in order to show the effects of our data reshaping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4gQx33YR1ca"
      },
      "source": [
        "# Reshape data\n",
        "X_train = X_train.reshape((X_train.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.reshape((X_test.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# Categorically encode labels\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = to_categorical(y_test, NUM_CLASSES)"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBfeJhORQCyB"
      },
      "source": [
        "# create the sequential model with BN and dropout layers\n",
        "model = Sequential([\n",
        "    Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    # dropout layer to drop neurons with rate less than 0.2\n",
        "    Dropout(rate=0.2),\n",
        "    # BN layer to rescale the inputs\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    Dropout(rate=0.2),\n",
        "    Activation(\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgVUeM5yWSTS"
      },
      "source": [
        "**Note:** You can also try to define Regularized dense layer and can create a sequential model as we see in the $l1$ and $l2$ regularization section discussed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsKGOEeHQg_k"
      },
      "source": [
        "# time based learning-rate scheduling\n",
        "epochs = 10\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs\n",
        "# define optimizer\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, decay=decay_rate)\n",
        " \n",
        "# Compile model\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ3bT-_DUpLq"
      },
      "source": [
        "**Note:** In the above code cell, you can also try compiling the with other optimizers like RMS prop, momentum optimization, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47V1U9LhQkIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "outputId": "839982d6-cd05-4c84-e906-fe58ff78ad93"
      },
      "source": [
        "# outputs epoch-by-epoch loss functions and accuracies at the end of each epoch of training\n",
        "plot_losses = livelossplot.PlotLossesKeras()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=epochs,\n",
        "          callbacks=[plot_losses],\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "\ttraining         \t (min:    0.901, max:    0.981, cur:    0.981)\n",
            "\tvalidation       \t (min:    0.951, max:    0.981, cur:    0.980)\n",
            "Loss\n",
            "\ttraining         \t (min:    0.062, max:    0.323, cur:    0.062)\n",
            "\tvalidation       \t (min:    0.064, max:    0.154, cur:    0.064)\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r469/469 [==============================] - 5s 11ms/step - loss: 0.0618 - accuracy: 0.9807 - val_loss: 0.0638 - val_accuracy: 0.9802\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd4411ede10>"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPTUDJNjVd_n"
      },
      "source": [
        "##### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQhlgALoVgzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16cffa8-1b0b-49b1-9528-73e72223e89d"
      },
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.06382469832897186\n",
            "Test accuracy: 0.9801999926567078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2rywYq6bjG4"
      },
      "source": [
        "##### Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhEfjsTKbny1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289b74bd-356f-4c4d-a6da-451ac9d3c1a4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_214\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_515 (Dense)           (None, 300)               235200    \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 300)               0         \n",
            "                                                                 \n",
            " batch_normalization_38 (Bat  (None, 300)              1200      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 300)               0         \n",
            "                                                                 \n",
            " dense_516 (Dense)           (None, 100)               30000     \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 100)               0         \n",
            "                                                                 \n",
            " batch_normalization_39 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_517 (Dense)           (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 267,810\n",
            "Trainable params: 267,010\n",
            "Non-trainable params: 800\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v4LSY3AdqO5"
      },
      "source": [
        "### Theory Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z62bkuLOdqO5"
      },
      "source": [
        "**Q1.** How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer?\n",
        "\n",
        "**Answer 1:** Email classification is a binary classification problem, so you would only need one neuron in the output layer. This neuron would indicate the probability that the email is spam or ham. You'd most likely use the sigmoid activation function in the output layer.\n",
        "\n",
        "For the MNIST problem you would need 10 output neurons in the final layer, one for each digit. You would then replace the logistic function with the softmax function which can output one probability per class per digit.\n",
        "\n",
        "**Q2.** Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
        "\n",
        "**Answer 2:** In general, the hyperparameters of a neural network you can adjust are the number of hidden layers, the number of neurons in each hidden layer, and the activation function used by each neuron.\n",
        "\n",
        "For binary classification, use the logistic activation function. For a multi-class problem, use softmax. For a linear regression problem, don't use an activation function.\n",
        "\n",
        "Some simple ways to try and solve overfitting are reducing the number of hidden layers or the number of neurons.\n",
        "\n",
        "**Q3.** What  may  happen  if  you  set  the  momentum  hyperparameter  too  close  to  1  (e.g., 0.99999) when using an SGD optimizer?\n",
        "\n",
        "**Answer 3:** If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value.\n",
        "\n",
        "**Q4.** Does dropout slow down training?\n",
        "\n",
        "**Answer 4:** Yes, dropout does slow down training, in general roughly by a factor of two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title Which of the following is designed to automatically standardize the inputs to a layer in a deep learning neural network? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"Batch Normalization\" #@param [\"\", \"Momentum\", \"Batch Normalization\", \"ReLU\", \"Dropout\"]"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good, But Not Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"Not sure\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0282154-2862-4934-8914-8e77d4fa6c0a"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 5166\n",
            "Date of submission:  19 Nov 2021\n",
            "Time of submission:  23:34:44\n",
            "View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}