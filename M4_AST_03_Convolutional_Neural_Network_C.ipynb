{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M4_AST_03_Convolutional_Neural_Network_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilav/neuralnetwork/blob/main/M4_AST_03_Convolutional_Neural_Network_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZbO1TP3GWLl"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 3: Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssvaS0BoGWLp"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC3yBYSkGWLp"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* distinguish between Feed Forward (fully connected) neural networks and Convolutional neural networks\n",
        "* understand terms like filtering, convolution, pooling\n",
        "* build CNN model to tackle fashion mnist dataset\n",
        "* perform 1D convolution on a time series dataset\n",
        "* know various CNN architectures: AlexNet and ResNet\n",
        "* build AlexNet architecture using Keras Sequential API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wDGpB4AGWLq"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWVZa0E6GWLq"
      },
      "source": [
        "The idea of the CNN was derived from the biological process of how the visual cortex is structured and works. \n",
        "\n",
        "As per the study of David H. and Torsten Wheel,\n",
        " \n",
        "* neurons in the visual cortex have a local **receptive field** that means neurons will respond to stimuli only in the restricted region, and \n",
        "\n",
        "* receptive fields of all neurons combine to make the whole visual image. \n",
        "\n",
        "The above study inspired the paper Neocognitron in 1980 and which later evolved into Convolutional Neural networks (CNN).\n",
        "\n",
        "The most important building block of a CNN is the convolutional layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7Iax1yyGWLq"
      },
      "source": [
        "### Convolutional Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z3LXJ26GWLr"
      },
      "source": [
        "Neurons in the first convolutional layer are not connected to every single pixel in the input image (like they were in the layers of DNN), but only to pixels in their receptive fields as shown in the figure below. In turn, each neuron in the second convolutional layer is connected only to neurons located within a receptive field in the first layer.\n",
        "\n",
        "This architecture allows the network to concentrate on small low-level features in the first hidden layer, then assemble them into larger higher-level features in the next hidden layer, and so on.\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/conv_layers_multiple_fmap.png\" width=600px/>\n",
        "</center>\n",
        "$\\hspace{6cm} \\text {Convolutional layers with multiple feature maps, and images with three color channels}$\n",
        "<br><br>\n",
        "\n",
        "A neuron’s weights can be represented as a small image with the size of the receptive field. This is termed as **filter**. And, a layer full of neurons using the same filter outputs a **feature map**, which highlights the areas in an image that activate the filter the most.\n",
        "\n",
        "A convolutional layer has multiple filters and outputs one feature map per filter, also:\n",
        "\n",
        "* it has one neuron per pixel in each feature map, and all neurons within a given feature map share the same parameters (i.e., the same weights and bias term). \n",
        "\n",
        "* neurons in different feature maps use different parameters. \n",
        "\n",
        "* a neuron’s receptive field extends across all the previous layers’ feature maps i.e, its shape will be $f_h$ x $f_w$ x depth of previous layer, where $f_h$ and $f_w$ are the height and width of the receptive field. \n",
        "\n",
        "In short, a convolutional layer simultaneously applies multiple trainable filters to its inputs, making it capable of detecting multiple features anywhere in its inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFyAL26MGWLr"
      },
      "source": [
        "For instance, if the input image and the filter look like the following:\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://pylessons.com/static/images/CNN-tutorials/CNN-tutorial-introduction/Figure_4.jpg\" width=400px/>\n",
        "</center>\n",
        "\n",
        "The filter (green) slides over the input image (blue) one pixel at a time starting from the top left. The filter multiplies its own values with the overlapping values of the image while sliding over it and adds all of them up to output a single value for each overlap until the entire image is traversed:\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://pylessons.com/static/images/CNN-tutorials/CNN-tutorial-introduction/Figure_5.gif\" width=500px/>\n",
        "</center>\n",
        "\n",
        "In the case of images with multiple channels (e.g. RGB), the Kernel has the same depth as that of the input image. Matrix Multiplication is performed between $K_n$ and $I_n$ stack ([$K_1,I_1$], [$K_2,I_2$], [$K_3,I_3$]) and all the results are summed with the bias to give us a squashed one-depth channel Convoluted Feature Output:\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://pylessons.com/static/images/CNN-tutorials/CNN-tutorial-introduction/Figure_7.gif\" />\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj4ArdG0GWLs"
      },
      "source": [
        "**Note that:** \n",
        "\n",
        "* all neurons located in the same row $i$ and column $j$ but in different feature maps are connected to the outputs of the exact same neurons in the previous layer.\n",
        "\n",
        "* we do not have to define the filters manually: instead, during training, the convolutional layer will automatically learn the most useful filters for its task, and the layers above will learn to combine them into more complex patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M4_AST_03_Convolutional_Neural_Network_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    ipython.magic(\"sx wget https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/FordA_TRAIN.tsv\")\n",
        "    ipython.magic(\"sx wget https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/FordA_TEST.tsv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV9JjuNrGWLs"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL4hjwyJGWLt"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization, InputLayer, Conv1D, ReLU, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feZFBVDmGWLu"
      },
      "source": [
        "#### Using a TensorFlow implementation, we will now understand convolutional operations on images\n",
        "\n",
        "* Applying a convolutional filter\n",
        "* Feature Map generation\n",
        "* Stride and Padding\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysxxzh6RGWLv"
      },
      "source": [
        "In TensorFlow, \n",
        "\n",
        "* each input image is typically represented as a 3D tensor of shape\n",
        "[height, width, channels]. \n",
        "\n",
        "* A mini-batch is represented as a 4D tensor of shape [minibatch size, height, width, channels]. \n",
        "\n",
        "* The weights of a convolutional layer are represented as a 4D tensor of shape [$f_h$, $f_w$, $f_{n′}$, $f_n$]. \n",
        "\n",
        "* The bias terms of a convolutional layer are simply represented as a 1D tensor of shape [$f_n$].\n",
        "\n",
        "where \n",
        "\n",
        "$f_h$ is filter (or receptive field) height, \n",
        "\n",
        "$f_w$ is filter width, \n",
        "\n",
        "$f_{n′}$ is number of feature maps in the previous layer (layer $l – 1$), and \n",
        "\n",
        "$f_n$ is number of feature maps in the current layer (layer $l$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGi23P1BGWLv"
      },
      "source": [
        "Let's creates two filters and apply them to Chinese temple and flower images.\n",
        "\n",
        "In the following code we use Scikit-Learn’s `load_sample_image()` (which loads two color images, one of a Chinese temple, and the other of a flower)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uexJKiHoGWLw"
      },
      "source": [
        "# Load sample images\n",
        "china = datasets.load_sample_image(\"china.jpg\")\n",
        "flower = datasets.load_sample_image(\"flower.jpg\")\n",
        "\n",
        "# Scale image features\n",
        "china = china / 255\n",
        "flower = flower / 255\n",
        "\n",
        "# Visualize images\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "ax[0].imshow(china)\n",
        "ax[1].imshow(flower)\n",
        "plt.show()\n",
        "china.shape, flower.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTNPRQfQGWLw"
      },
      "source": [
        "# Combine images as single 4D array\n",
        "images = np.array([china, flower])\n",
        "batch_size, height, width, channels = images.shape\n",
        "batch_size, height, width, channels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmuVHfc4GWLx"
      },
      "source": [
        "Create two 7 × 7 filters (one with a vertical white line in the middle, and the other with a horizontal white line in the middle).\n",
        "\n",
        "Note that shape should be [$f_h$, $f_w$, $f_{n′}$, $f_n$]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQNcmmIyGWLx"
      },
      "source": [
        "# Create 2 filters\n",
        "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
        "# vertical line\n",
        "filters[:, 3, :, 0] = 1 \n",
        "# horizontal line\n",
        "filters[3, :, :, 1] = 1\n",
        "filters.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGl6HLzBGWLx"
      },
      "source": [
        "Apply the filters to both images using the `tf.nn.conv2d()` function, which is part of TensorFlow’s low-level Deep Learning API. \n",
        "\n",
        "Here, we use zero padding (`padding=\"SAME\"`) and a stride of 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDtI9fdjGWLx"
      },
      "source": [
        "# Convolutional layer\n",
        "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5MgZldBGWLx"
      },
      "source": [
        "# Images' 1st feature map using vertical filter\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "ax[0].imshow(outputs[0, :, :, 0], cmap=\"gray\")\n",
        "ax[1].imshow(outputs[1, :, :, 0], cmap=\"gray\")\n",
        "print(\"1st feature map through verticle filter\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBTXOBz7GWLx"
      },
      "source": [
        "# Images' 2nd feature map using horizontal filter\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "ax[0].imshow(outputs[0, :, :, 1], cmap=\"gray\")\n",
        "ax[1].imshow(outputs[1, :, :, 1], cmap=\"gray\")\n",
        "print(\"2nd feature map through horizontal filter\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsXxF2G2GWLy"
      },
      "source": [
        "In `tf.nn.conv2d()`:\n",
        "\n",
        "* **images** is the input mini-batch (a 4D tensor).\n",
        "\n",
        "* **filters** is the set of filters to apply (also a 4D tensor).\n",
        "\n",
        "* **strides** is equal to 1, but it could also be a 1D array with four elements, where the two central elements are the vertical and horizontal strides ($s_h$ and $s_w$). The first and last elements must currently be equal to 1. They may one day be used to specify a batch stride (to skip some instances) and a channel stride (to skip some of the previous layer’s feature maps or channels).\n",
        "\n",
        "* **padding** must be either \"SAME\" or \"VALID\":\n",
        "\n",
        "  * If set to \"SAME\", the convolutional layer uses zero padding if necessary. The output size is set to the number of input neurons divided by the stride, rounded up. For example, if the input size is 13 and the stride is 5 as shown in the figure below, then the output size is 3 (i.e., 13 / 5 = 2.6, rounded up to 3). Then zeros are added as evenly as possible around the inputs, as needed.\n",
        "\n",
        "  * If set to \"VALID\", the convolutional layer does not use zero padding and may\n",
        "ignore some rows and columns at the bottom and right of the input image,\n",
        "depending on the stride. This means that every neuron’s receptive field lies strictly within valid positions inside the input, hence the name valid.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://wizardforcel.gitbooks.io/scikit-and-tensorflow-workbooks-bjpcjp/content/pics/padding-options.png\" width=450px/>\n",
        "</center>\n",
        "$\\hspace{9.5cm} \\text {Different padding options}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZh1eecQGWLy"
      },
      "source": [
        "In the above example, we manually defined the filters, but in a real CNN we would normally define filters as trainable variables so the neural net can learn which filters work best. Instead of manually creating the variables, use the `keras.layers.Conv2D` layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk5qBb4uGWLy"
      },
      "source": [
        "conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXZD3hEhGWLy"
      },
      "source": [
        "As we can see, convolutional layers have quite a few hyperparameters and we can use cross-validation to find the right values, but this is very time-consuming.\n",
        "\n",
        "Also, CNNs' convolutional layers require a huge amount of RAM. This problem is taken care of by the second common building block of CNNs: the pooling layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcAQ-t5pGWLz"
      },
      "source": [
        "### Pooling Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIp2wjP6GWLz"
      },
      "source": [
        "The goal of the pooling layer is to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters. Each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a receptive field. \n",
        "\n",
        "However, **a pooling neuron has no weights**; all it does is aggregate the inputs using an aggregation function such as the **max** or **mean**. \n",
        "\n",
        "The below figure shows a max pooling layer, with a 2 × 2 pooling kernel, stride of 2 and no padding. Only the max input value in each receptive field makes it to the next layer. Because of the stride of 2, the output image has half the size of the input image.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Pooling_layer.png\" width=600px/>\n",
        "</center>\n",
        "$\\hspace{7cm} \\text {Max pooling layer (2x2 pooling kernel, stride 2, no padding)}$\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5NgBVoNGWLz"
      },
      "source": [
        "#### Using a TensorFlow implementation, we will now understand pooling operation on images\n",
        "\n",
        "* Applying a pooling layer\n",
        "* Image compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hstKAFXtGWLz"
      },
      "source": [
        "The following code creates a max pooling layer using a 2 × 2 kernel and applies it to `images`. The strides default to the kernel size, so this layer will use a stride of 2 (both horizontally and vertically):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNx9eeWMGWLz"
      },
      "source": [
        "# Pooling layer\n",
        "max_pool = keras.layers.MaxPool2D(pool_size=2, padding=\"valid\", dtype='float64')(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rO_s79qGWL0"
      },
      "source": [
        "To create an average pooling layer, just use AvgPool2D instead of MaxPool2D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiO1GfIbGWL0"
      },
      "source": [
        "# Images before and after pooling operation, showing only single channel\n",
        "fig, ax = plt.subplots(2,2, figsize=(10,6))\n",
        "\n",
        "ax[0][0].imshow(images[0, :, :, 1], cmap=\"gray\")\n",
        "ax[0][1].imshow(images[1, :, :, 1], cmap=\"gray\")\n",
        "\n",
        "ax[1][0].imshow(max_pool[0, :, :, 1], cmap=\"gray\")\n",
        "ax[1][1].imshow(max_pool[1, :, :, 1], cmap=\"gray\")\n",
        "\n",
        "print(\"Images before and after pooling operation:\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVFraiWNGWL1"
      },
      "source": [
        "# Compression ratio\n",
        "original_bytes = images.nbytes\n",
        "pooling_bytes = np.array(max_pool).nbytes\n",
        "ratio = pooling_bytes / original_bytes\n",
        "print(\"The compression ratio between the original images size and the total size after pooling is:\", ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAbHLiAzGWL1"
      },
      "source": [
        "From the above results, we can see that the images are shrunk.\n",
        "\n",
        "Now we know all the building blocks to create convolutional neural networks.\n",
        "Let's perform a 1D convolution on a time series dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G28fQGLXevX"
      },
      "source": [
        "### 1D Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKUu52J8Xy8H"
      },
      "source": [
        "**Dataset description**\n",
        "\n",
        "The dataset we are using here is a time series data and is called FordA. It contains 3601 training instances and another 1320 testing instances. Each time series corresponds to a measurement of engine noise captured by a motor sensor. For this task, the goal is to automatically detect the presence of a specific issue with the engine. The problem is a balanced binary classification task.\n",
        "\n",
        "To know more about this dataset, click [here](http://www.j-wichard.de/publications/FordPaper.pdf).\n",
        "\n",
        "We will use the `FordA_TRAIN` file for training and the\n",
        "`FordA_TEST` file for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjbf_dBIZKUv"
      },
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"FordA_TRAIN.tsv\", delimiter=\"\\t\", header=None)\n",
        "df_test = pd.read_csv(\"FordA_TEST.tsv\", delimiter=\"\\t\", header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGIKaBGAaUYE"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoxUdc--bX73"
      },
      "source": [
        "The first column corresponds to the label (-1 or 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a9PTBboa82j"
      },
      "source": [
        "# Train and test data\n",
        "x_train = df_train.iloc[:, 1:].values\n",
        "y_train = df_train.iloc[:, 0].astype(int)\n",
        "x_test =  df_test.iloc[:, 1:].values\n",
        "y_test = df_test.iloc[:, 0].astype(int)\n",
        "x_train.shape, x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XF-JBOjdBqg"
      },
      "source": [
        "Visualize one time series example for each class in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnP1GJGYdEhY"
      },
      "source": [
        "# Visualize one time series per class\n",
        "classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n",
        "\n",
        "for c in classes:\n",
        "    c_x_train = x_train[y_train == c]\n",
        "    plt.plot(c_x_train[0], label=\"class \" + str(c))\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26S1HmqKeGj6"
      },
      "source": [
        "Note that the time series data used here are univariate, meaning we only have one channel per time series example. Therefore, we need to transform it into a multivariate one with one channel. This will allow us to construct a model that is easily applicable to multivariate time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMuH47PGea97"
      },
      "source": [
        "# Reshape training set\n",
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "x_train.shape, x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdaFq6mtgN4x"
      },
      "source": [
        "# Shuffle the training set\n",
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7w8xx0YgeWH"
      },
      "source": [
        "Standardize the labels to positive integers. The expected labels will then be 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsIpZ0K2gYA8"
      },
      "source": [
        "# Change label -1 to 0\n",
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ztH_M96gmBk"
      },
      "source": [
        "# Create model\n",
        "model = Sequential([\n",
        "                    InputLayer(x_train.shape[1:]),\n",
        "                    Conv1D(filters=64, kernel_size=3, padding=\"same\"),\n",
        "                    BatchNormalization(),\n",
        "                    ReLU(),\n",
        "                    Conv1D(filters=64, kernel_size=3, padding=\"same\"),\n",
        "                    BatchNormalization(),\n",
        "                    ReLU(),\n",
        "                    Conv1D(filters=64, kernel_size=3, padding=\"same\"),\n",
        "                    BatchNormalization(),\n",
        "                    ReLU(),\n",
        "                    GlobalAveragePooling1D(),\n",
        "                    Dense(2, activation=\"softmax\")\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHoPO8NzkfHg"
      },
      "source": [
        "# Compile model\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lbKUboog6hv"
      },
      "source": [
        "# Train the model\n",
        "callbacks = [ keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"val_loss\")]\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size = 32, epochs = 10, callbacks=callbacks, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTE3RSV-hAPG"
      },
      "source": [
        "# Evaluate model on test set\n",
        "model = keras.models.load_model(\"best_model.h5\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test loss\", test_loss)\n",
        "print(\"Test accuracy\", test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q_hV_PXhHFh"
      },
      "source": [
        "# Visualize model's training and validation metrics\n",
        "metric = \"sparse_categorical_accuracy\"\n",
        "\n",
        "plt.plot(history.history[metric])\n",
        "plt.plot(history.history[\"val_\" + metric])\n",
        "\n",
        "plt.title(\"model \" + metric)\n",
        "plt.ylabel(metric, fontsize=\"large\")\n",
        "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j6QGWl49WHc"
      },
      "source": [
        "From the above plot, we can see that the network still needs training.\n",
        "\n",
        "Now, let's see how a 2D convolution works. First, we understand a typical CNN architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wACRVHSnGWL1"
      },
      "source": [
        "### CNN Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOk9Z4MdGWL1"
      },
      "source": [
        "Typical CNN architectures stack a few convolutional layers (generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on. The image gets smaller and smaller as it progresses through the network, but it also gets deeper and deeper (i.e., with more feature maps), as shown in the figure below. At the top of the stack, a regular feedforward neural network is added, composed of a few fully connected layers (+ReLUs), and the final layer outputs the prediction.\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://i1.wp.com/thecleverprogrammer.com/wp-content/uploads/2020/11/1-cnnlayer.png?resize=1024%2C259&ssl=1\" width=600px/>\n",
        "</center>\n",
        "\n",
        "$\\hspace{11cm} \\text {CNN architecture}$\n",
        "<br><br>\n",
        "\n",
        "Here is how we can implement a simple CNN to tackle the Fashion MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-s7j23vGWL1"
      },
      "source": [
        "# Using Keras to load the dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eoG8ubBGWL2"
      },
      "source": [
        "# Visualize an image from data\n",
        "print(\"Label: \", y_train_full[0])\n",
        "plt.imshow(X_train_full[0], cmap='Greys')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ATde942GWL2"
      },
      "source": [
        "# Shape and datatype of X_train\n",
        "X_train_full.shape, X_train_full.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxHft9mdGWL2"
      },
      "source": [
        "# Split into training and validation data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=123)\n",
        "X_train.shape, X_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0v9OOixGWL2"
      },
      "source": [
        "# Reshape train, test, and validation data\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], 28, 28, 1)\n",
        "X_train.shape, X_val.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5tHXN4rGWL3"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/CNN_architecture_fmnist.png\" width=900px/>\n",
        "</center>\n",
        "$\\hspace{7.5cm} \\text {CNN architecture used for Fashion MNIST dataset}$\n",
        "<br><br>\n",
        "\n",
        "The CNN architecture used for FMNIST dataset is shown in the figure above and its corresponding sequential model is created using the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDS4NtKeGWL3"
      },
      "source": [
        "# Create model\n",
        "model = Sequential([\n",
        "                    Conv2D(64, 7, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]),\n",
        "                    MaxPooling2D(2),\n",
        "                    Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
        "                    Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
        "                    MaxPooling2D(2),\n",
        "                    Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
        "                    Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
        "                    MaxPooling2D(2),\n",
        "                    Flatten(),\n",
        "                    Dense(128, activation=\"relu\"),\n",
        "                    Dropout(0.5),\n",
        "                    Dense(64, activation=\"relu\"),\n",
        "                    Dropout(0.5),\n",
        "                    Dense(10, activation=\"softmax\")\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36rNsPuLGWL3"
      },
      "source": [
        "Let’s go through this model:\n",
        "\n",
        "* The first layer uses 64 fairly large filters (7 × 7) but no stride because the input images are not very large. It also sets input_shape=[28, 28, 1], because the images are 28 × 28 pixels, with a single color channel (i.e., grayscale).\n",
        "\n",
        "* Next, we have a max pooling layer which uses a pool size of 2, so it divides each spatial dimension by a factor of 2.\n",
        "\n",
        "* Then we repeat the same structure twice: two convolutional layers followed by a max pooling layer. For larger images, we could repeat this structure several more times (the number of repetitions is a hyperparameter we can tune).\n",
        "\n",
        "* Note that the number of filters grows as we climb up the CNN toward the output layer (it is initially 64, then 128, then 256): since the number of low-level features is often fairly low (e.g., small circles, horizontal lines), but there are many different ways to combine them into higher-level features.\n",
        "\n",
        "* Next is the fully connected network, composed of two hidden dense layers and a dense output layer. Note that we must flatten its inputs since a dense network expects a 1D array of features for each instance. We also add two dropout layers, to reduce overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLiFiuyeGWL3"
      },
      "source": [
        "# Compile model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyYKZOdiGWL3"
      },
      "source": [
        "Here we use `ImageDataGenerator` that generates batches of tensor image data with real-time data augmentation. The images output by the generator will have the same dimensions as the input images. It lets us augment the images in real-time while the model is still training. \n",
        "\n",
        "We can apply any random transformations on the training image as it is passed to the model. Few parameters are:\n",
        "\n",
        "* **rescale**: scale the image\n",
        "* **horizontal_flip**: randomly flip inputs horizontally\n",
        "* **width_shift_range**: shift the image to the left or right (horizontal shifts)\n",
        "* **height_shift_range**: shift the image vertically (up or down)\n",
        "* **rotation_range**: degree range for random rotations of the image\n",
        "\n",
        "To know more about ImageDataGenerator, click [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di7PG3XGGWL3"
      },
      "source": [
        "# Model Parameters\n",
        "batch_size = 256\n",
        "\n",
        "# Instantiate ImageDataGenerator\n",
        "gen = ImageDataGenerator(rescale = 1.0/255, \n",
        "                         width_shift_range = 0.005,\n",
        "                         height_shift_range = 0.005,\n",
        "                         rotation_range = 0,\n",
        "                         horizontal_flip = True)\n",
        "\n",
        "# Generate batches of tensor image data\n",
        "train_batches = gen.flow(X_train, y_train, batch_size = batch_size)\n",
        "val_batches = gen.flow(X_val, y_val, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9y4B3LQmF8M"
      },
      "source": [
        "**Note:** Use [`flow_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory) to load images from the directory. It takes the path to a directory & generates batches of augmented data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaIvMTWrmEXL"
      },
      "source": [
        "# Train the model\n",
        "history = model.fit(train_batches,\n",
        "                    steps_per_epoch = X_train.shape[0]//batch_size,\n",
        "                    epochs = 1,\n",
        "                    validation_data = val_batches,\n",
        "                    validation_steps = X_val.shape[0]//batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3R93T5IGWL4"
      },
      "source": [
        "# Instantiate ImageDataGenerator\n",
        "test_gen = ImageDataGenerator(rescale=1.0/255)\n",
        "# Generate batches of tensor image data\n",
        "test_batches = test_gen.flow(X_test, y_test, batch_size= 50)\n",
        "\n",
        "# Evaluate Model against test data and get the score\n",
        "score = model.evaluate(test_batches)\n",
        "\n",
        "# Print Metrics\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p48GcdeKGWL4"
      },
      "source": [
        "By increasing the number of epochs, the accuracy can be improved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmWA8_3AGWL4"
      },
      "source": [
        "# Predict class probabilities for first three instances of X_test\n",
        "X_new = X_test[-3:]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXaz6rgeGWL4"
      },
      "source": [
        "# List of labels\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Z_sUv8GWL4"
      },
      "source": [
        "# Predict class labels for first three instances of X_test\n",
        "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
        "print(\"Predicted labels: \\n\", y_pred)\n",
        "\n",
        "print(np.array(class_names)[y_pred])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsrtUE5DGWL4"
      },
      "source": [
        "# Actual labels\n",
        "fig, ax = plt.subplots(1,3)\n",
        "for axi, i in zip(ax.ravel(), np.arange(len(X_new))):\n",
        "    axi.imshow(X_new[i].reshape(28, 28), cmap='Greys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WjysELFx1D9"
      },
      "source": [
        "From the above results, we can see the performance of network on three images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG8yhDzAhSd4"
      },
      "source": [
        "Now, let's look at the AlexNet architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJx93dFgYUzS"
      },
      "source": [
        "### AlexNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97_jAo5AYby7"
      },
      "source": [
        "The AlexNet CNN architecture won the 2012 ImageNet ILSVRC challenge by a\n",
        "large margin. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It was the first to stack convolutional layers directly on top of one another, instead of stacking a pooling layer on top of each convolutional layer. The table given below presents this architecture.\n",
        "<br><br>\n",
        "\n",
        "|  Layer  |  Type  | Maps  |  Size  |  Kernel Size | Stride  |  Padding  |  Activation  \n",
        "|:--------------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|\n",
        "|  Out  | Fully connected | - |  1,000 | - | - | - | Softmax |\n",
        "| F10 | Fully connected | - | 4,096 | - | - | - | ReLU |\n",
        "| F9 | Fully connected | - | 4,096 | - | - | - | ReLU |\n",
        "| S8 | Max pooling | 256 | 6 x 6 | 3 x 3 | 2 | valid | - |\n",
        "| C7 | Convolution | 256 | 13 x 13 | 3 x 3 | 1 | same | ReLU |\n",
        "| C6 | Convolution | 384 | 13 x 13 | 3 x 3 | 1 | same | ReLU |\n",
        "| C5 | Convolution | 384 | 13 x 13 | 3 x 3 | 1 | same | ReLU |\n",
        "| S4 | Max pooling | 256 | 13 x 13 | 3 x 3 | 2 | valid | - |\n",
        "| C3 | Convolution | 256 | 27 x 27 | 5 x 5 | 1 | same | ReLU |\n",
        "| S2 | Max pooling | 96 | 27 x 27 | 3 x 3 | 2 | valid | - |\n",
        "| C1 | Convolution | 96 | 55 x 55 | 11 x 11 | 4 | valid | ReLU |\n",
        "| In | Input | 3 (RGB) | 227 x 227 | - | - | - | - |\n",
        "\n",
        "$\\hspace{11cm} \\text {AlexNet architecture}$\n",
        "<br><br>\n",
        "\n",
        "To reduce overfitting, the authors used two regularization techniques: \n",
        "\n",
        "* **dropout** with a 50% dropout rate during training to the outputs of layers F9 and F10\n",
        "\n",
        "* **data augmentation** by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions.\n",
        "\n",
        "Let's build the AlexNet architecture and train it on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. The dataset contains 60,000 colour images, each with dimensions 32x32px. The content of the images within the dataset is sampled from 10 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG8oPbxgfD9M"
      },
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO8nJXENc4UZ"
      },
      "source": [
        "# List to refer labels\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SicSsFDlg0q3"
      },
      "source": [
        "By default, the CIFAR dataset is partitioned into 50,000 training data and 10,000 test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hryFMs6yg5w1"
      },
      "source": [
        "# Shape of train and test images\n",
        "train_images.shape, test_images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8lSZGQX2k7i"
      },
      "source": [
        "Instead of taking all the 50000 instances for training, we will only use the first 5000 instances as validation set and next the 5000 instances as training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZbeEwoBdY1k"
      },
      "source": [
        "# Validation data\n",
        "val_images, val_labels = train_images[:5000], train_labels[:5000]\n",
        "# Training data\n",
        "train_images, train_labels = train_images[5000:10000], train_labels[5000:10000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp4JiR0U3E3l"
      },
      "source": [
        "To ease data manipulation and modification we transform the dataset into a TensorFlow dataset using `tf.data.Dataset.from_tensor_slices` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX3SelvVhlLr"
      },
      "source": [
        "# Convert to TensorFlow dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arkXw5fujoYC"
      },
      "source": [
        "# Visualize few images\n",
        "plt.figure(figsize=(10,10))\n",
        "for i, (image, label) in enumerate(train_ds.take(5)):\n",
        "    ax = plt.subplot(5,5,i+1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(class_names[label.numpy()[0]])\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeZ0jMuj4i7x"
      },
      "source": [
        "Now as per the network, we need to do few preprocessing steps:\n",
        "\n",
        "* Normalize images to have a mean of 0 and standard deviation of 1\n",
        "\n",
        "* Resizing of the images from 32 x 32 to 227 x 227. The AlexNet network input expects a 227x227 image.\n",
        "\n",
        "We create a function called `process_images` for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQNnlYXnjj7-"
      },
      "source": [
        "def process_images(image, label):\n",
        "    ''' Normalize images to have a mean of 0 and standard deviation of 1, \n",
        "        resize images from 32x32 to 227x227 '''\n",
        "    image = tf.image.per_image_standardization(image)\n",
        "    image = tf.image.resize(image, (227,227))\n",
        "    return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yrMgCW7jhoX"
      },
      "source": [
        "# Size of training, testing, validation set\n",
        "train_ds_size = len(train_ds)\n",
        "test_ds_size = len(test_ds)\n",
        "val_ds_size = len(val_ds)\n",
        "print(\"Training data size:\", train_ds_size)\n",
        "print(\"Test data size:\", test_ds_size)\n",
        "print(\"Validation data size:\", val_ds_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0MLwnvCjdq-"
      },
      "source": [
        "# Perform preprocessing with shuffle and batch data operations\n",
        "train_ds = (train_ds.map(process_images)\n",
        "                    .shuffle(buffer_size=train_ds_size)\n",
        "                    .batch(batch_size=32, drop_remainder=True))\n",
        "\n",
        "test_ds = (test_ds.map(process_images)\n",
        "                  .shuffle(buffer_size=test_ds_size)\n",
        "                  .batch(batch_size=32, drop_remainder=True))\n",
        "\n",
        "val_ds = (val_ds.map(process_images)\n",
        "                .shuffle(buffer_size=val_ds_size)\n",
        "                .batch(batch_size=32, drop_remainder=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvS4htQliMqx"
      },
      "source": [
        "Model implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYfX1HGIiOfd"
      },
      "source": [
        "# Create AlexNet CNN architecture\n",
        "model = Sequential([\n",
        "                    Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
        "                    BatchNormalization(),\n",
        "                    MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
        "                    Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "                    BatchNormalization(),\n",
        "                    MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
        "                    Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "                    BatchNormalization(),\n",
        "                    Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "                    BatchNormalization(),\n",
        "                    Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "                    BatchNormalization(),\n",
        "                    MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
        "                    Flatten(),\n",
        "                    Dense(4096, activation='relu'),\n",
        "                    Dropout(0.5),\n",
        "                    Dense(4096, activation='relu'),\n",
        "                    Dropout(0.5),\n",
        "                    Dense(10, activation='softmax')\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCGPud2bjFfR"
      },
      "source": [
        "# Compile model\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.optimizers.SGD(learning_rate = 0.001), metrics = ['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbbswBHpjMF3"
      },
      "source": [
        "# Train model on training set\n",
        "history = model.fit(train_ds, epochs = 1, validation_data = val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpwGIkPwjUiZ"
      },
      "source": [
        "# Model performance on test set\n",
        "model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT6B1L_QR9wz"
      },
      "source": [
        "By increasing the number of training instances and epochs, the model performance can be increased.\n",
        "\n",
        "Other than AlexNet we have few more CNN architectures such as ResNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfrR70pXyHjl"
      },
      "source": [
        "### ResNet (Residual Network) **(Optional)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHU6arE2GWL5"
      },
      "source": [
        "Kaiming He et al. won the ILSVRC 2015 challenge using a Residual Network (or\n",
        "ResNet). The winning variant used an extremely deep CNN composed of 152 layers (other variants had 34, 50, and 101 layers). \n",
        "\n",
        "It confirmed the general trend: models are getting deeper and deeper, with fewer and fewer parameters. The key to being able to train such a deep network is to use **skip connections** (also called shortcut connections): the signal feeding into a layer is also added to the output of a layer located a bit higher up the stack.\n",
        "\n",
        "When training a neural network, the goal is to make it model a target function $h(x)$. If we add the input $x$ to the output of the network (i.e., we add a skip connection), then the network will be forced to model $f(x) = h(x) – x$ rather than $h(x)$. This is called **residual learning** as shown below.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Residual_learning.png\" width=450px/>\n",
        "</center>\n",
        "\n",
        "$\\hspace{10cm} \\text {Residual learning}$\n",
        "<br><br>\n",
        "\n",
        "When we initialize a regular neural network, its weights are close to zero, so the network just outputs values close to zero. If we add a skip connection, the resulting network just outputs a copy of its inputs; in other words, it initially models the identity function. If the target function is fairly close to the identity function, this will speed up training considerably."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kk38--eGWL5"
      },
      "source": [
        "ResNet’s architecture can be seen in the figure below. It has in between very deep stack of simple residual units. Each residual unit is composed of two convolutional layers, with Batch Normalization (BN) and ReLU activation, using 3 × 3 kernels and preserving spatial dimensions (stride 1, \"same\" padding).\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1294/0*65jDuZ6VTgaLUSuk.png\" width=600px/>\n",
        "</center>\n",
        "\n",
        "$\\hspace{10cm} \\text {ResNet architecture}$\n",
        "<br><br>\n",
        "\n",
        "**Note that** the number of feature maps is doubled every few residual units, at the same time as their height and width are halved (using a convolutional layer with stride 2). When this happens, the inputs cannot be added directly to the outputs of the residual unit because they don’t have the same shape (for example, this problem affects the skip connection represented by the dashed arrow in the figure given above ResNet architecture). \n",
        "\n",
        "To solve this problem, the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right number of output feature maps (as shown below).\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Skip_connection.png\" width=500px/>\n",
        "</center>\n",
        "\n",
        "$\\hspace{8cm} \\text {Skip connection when changing feature map size and depth}$\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB5fz3JmGWL5"
      },
      "source": [
        "### Implementing a ResNet-50 CNN Using Pretrained Models from Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqftlc0hGWL5"
      },
      "source": [
        "We can load the ResNet-50 model, pretrained on ImageNet, from keras.applications package, with the following line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKlvu39uGWL5"
      },
      "source": [
        "# Load pretrained ResNet50 model\n",
        "model = keras.applications.resnet50.ResNet50(weights=\"imagenet\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk4HV3akGWL6"
      },
      "source": [
        "This will create a ResNet-50 model and download weights pretrained on the ImageNet dataset. To use it, we first need to ensure that the images have the right size. A ResNet-50 model expects 224 × 224-pixel images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgm4gL8rGWL6"
      },
      "source": [
        "# Images before reshape\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "ax[0].imshow(images[0])\n",
        "ax[1].imshow(images[1])\n",
        "print(\"Images before reshape\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqIjv0X_GWL6"
      },
      "source": [
        "# Crop range(positions) in percentages\n",
        "china_box = [0, 0.03, 1, 0.68]\n",
        "flower_box = [0.19, 0.26, 0.86, 0.7]\n",
        "\n",
        "# Reshape images\n",
        "images_resized = tf.image.crop_and_resize( images, boxes=[china_box, flower_box], box_indices=[0,1], crop_size=[224,224])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwT3NbvEGWL6"
      },
      "source": [
        "# Images after reshape\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "ax[0].imshow(images_resized[0])\n",
        "ax[1].imshow(images_resized[1])\n",
        "print(\"Images after reshape\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMll8y5OGWL6"
      },
      "source": [
        "The pretrained models assume that the images are preprocessed in a specific way. In some cases they may expect the inputs to be scaled from 0 to 1, or –1 to 1, and so on. Each model provides a `preprocess_input()` function. These functions assume that the pixel values range from 0 to 255, so we must multiply them by 255 (since earlier we scaled them to the 0–1 range):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdeXUJxEGWL6"
      },
      "source": [
        "# Preprocess images\n",
        "inputs = keras.applications.resnet50.preprocess_input(images_resized * 255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoOw15-OGWL6"
      },
      "source": [
        "Use the pretrained model to make predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7AuNnPzGWL7"
      },
      "source": [
        "# Make prediction\n",
        "Y_proba = model.predict(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZlJHKJGWL7"
      },
      "source": [
        "As usual, the output Y_proba is a matrix with one row per image and one column per class (in this case, there are 1,000 classes). To display the top K predictions, including the class name and the estimated probability of each predicted class, use the `decode_predictions()` function. For each image, it returns an array containing the top K predictions, where each prediction is represented as an array containing the class identifier, its name, and the corresponding confidence score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YX4oiXHGWL7"
      },
      "source": [
        "# Top k predictions\n",
        "top_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
        "for image_index in range(len(images)):\n",
        "    print(\"Image #{}\".format(image_index))\n",
        "    for class_id, name, y_proba in top_K[image_index]:\n",
        "      print(\" {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDTiFqaeGWL7"
      },
      "source": [
        "The correct classes (monastery and daisy) appear in the top three results for both images. That’s pretty good, considering that the model had to choose from among 1,000 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqEr5Jx-GWL7"
      },
      "source": [
        "### Theory Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dfmVta-GWL7"
      },
      "source": [
        "1. What are the advantages of a CNN over a fully connected DNN for image classification?\n",
        "\n",
        " These are the main advantages of a CNN over a fully connected DNN for image\n",
        "classification:\n",
        "\n",
        "  * Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has many fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data.\n",
        "  \n",
        "  * When a CNN has learned a kernel that can detect a particular feature, it can\n",
        "detect that feature anywhere in the image. In contrast, when a DNN learns a\n",
        "feature in one location, it can detect it only in that particular location. Since images typically have very repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks such as classification, using fewer training examples.\n",
        "\n",
        "  * Finally, a DNN has no prior knowledge of how pixels are organized; it does not know that nearby pixels are close. A CNN’s architecture embeds this prior\n",
        "knowledge. Lower layers typically identify features in small areas of the images, while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start compared to DNNs.\n",
        "\n",
        "2. Why would you want to add a max pooling layer rather than a convolutional\n",
        "layer with the same stride?\n",
        "\n",
        " A max pooling layer has no parameters at all, whereas a convolutional layer has\n",
        "quite a few."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title Select the FALSE statement: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"VALID padding will include all the rows and columns whereas SAME padding may ignore some rows and columns of the input image\", \"A higher stride means higher information loss but resultant less computation\", \"A max pooling layer does not have any parameter\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}